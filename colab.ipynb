{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert :-(.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe4xFsb3k1-F"
      },
      "source": [
        "To run *Method Exploration Relation Extraction Using BERT* repo in Colab:\n",
        "1. Upload the repo to your Google Drive.\n",
        "2. Open this notebook in Colab.\n",
        "3. Set Colab to use GPU.\n",
        "4. Run the following code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob1663Mylw67"
      },
      "source": [
        "## Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrB1ewfrwGvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8645033f-88dc-45a4-f632-8cd3a852a0b9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckVToE6MwWo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b383c3f-8c9f-4fd4-b5fd-ab7a194e7d2e"
      },
      "source": [
        "# Please modify the following to your repo folder in google drive\n",
        "%cd /content/drive/My\\ Drive/REPO-FOLDER"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/RD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeD6ZzpPw4HK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26681567-da53-4837-97e3-46e6f13781ac"
      },
      "source": [
        "# This prints the current location\n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/RD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeEkd-C-mCzr"
      },
      "source": [
        "## Install Needed Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNntNRB0xoz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36888b33-a3db-49b0-ff1a-07fdb5e23e14"
      },
      "source": [
        "!pip install transformers==3.5.1  #use this version or else problem in embedding getting\n",
        "#!pip install numpy==1.16.4 # might not needed\n",
        "#!pip install tensorflow==2.3.0 # might not needed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 28.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 34.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 38.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 29.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 32.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 34.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 25.8MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 23.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 24.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 22.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 22.6MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 22.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 22.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 22.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 22.6MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 22.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 22.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 22.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.8)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 49.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2019.12.20)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.1) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.1) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2020.11.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.1) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=18b0a3d450c4ad4c738c5b909dca60cc42dccd48f575dcd302089c22ac4e482a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n",
            "Collecting bert-serving-server\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/bd/cab677bbd0c5fb08b72e468371d2bca6ed9507785739b4656b0b5470d90b/bert_serving_server-1.10.0-py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.18.5)\n",
            "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (20.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.15.0)\n",
            "Collecting GPUtil>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=7dcb2e84314224f128994685bb2e79c75535bff054385feeb8699fe2faad88dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil, bert-serving-server\n",
            "Successfully installed GPUtil-1.4.0 bert-serving-server-1.10.0\n",
            "Collecting bert-serving-client\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/09/aae1405378a848b2e87769ad89a43d6d71978c4e15534ca48e82e723a72f/bert_serving_client-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (20.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (1.18.5)\n",
            "Installing collected packages: bert-serving-client\n",
            "Successfully installed bert-serving-client-1.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsIowi8YmQdS"
      },
      "source": [
        "## Training\n",
        "It will take hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0T46RwAwiyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c526bc7-8957-4fa5-fa8b-ad40d1dd67c8"
      },
      "source": [
        "!python train.py --bert --no-attn --save_epoch 10 --num_epoch 35 --batch_size 20 --special_token --lr 0.03\n",
        "#注意這裡有加special token!!!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-01 23:44:44.723592: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Loading data from dataset/tacred with batch size 20...\n",
            "100% 68124/68124 [00:01<00:00, 50229.39it/s]\n",
            "3407 batches created for dataset/tacred/train.json\n",
            "batch count 3407\n",
            "100% 22631/22631 [00:00<00:00, 25169.26it/s]\n",
            "1132 batches created for dataset/tacred/dev.json\n",
            "batch count 1132\n",
            "Config saved to file ./saved_models/00/config.json\n",
            "\n",
            "Running with the following configs:\n",
            "\tdata_dir : dataset/tacred\n",
            "\tvocab_dir : dataset/vocab\n",
            "\ttrain_file : train.json\n",
            "\tdev_file : dev.json\n",
            "\temb_dim : 768\n",
            "\tner_dim : 0\n",
            "\tpos_dim : 0\n",
            "\thidden_dim : 768\n",
            "\tmlp_dim : 300\n",
            "\tnum_layers : 1\n",
            "\tdropout : 0.5\n",
            "\tword_dropout : 0.04\n",
            "\ttopn : 10000000000.0\n",
            "\tlower : False\n",
            "\tattn : False\n",
            "\tattn_dim : 200\n",
            "\tpe_dim : 20\n",
            "\tlr : 0.03\n",
            "\tlr_decay : 0.9\n",
            "\toptim : sgd\n",
            "\tnum_epoch : 35\n",
            "\tbatch_size : 20\n",
            "\tmax_grad_norm : 5.0\n",
            "\tlog_step : 20\n",
            "\tlog : logs.txt\n",
            "\tsave_epoch : 10\n",
            "\tsave_dir : ./saved_models\n",
            "\tid : 00\n",
            "\tinfo : \n",
            "\tseed : 1234\n",
            "\tcuda : True\n",
            "\tcpu : False\n",
            "\tbert : True\n",
            "\tlife : False\n",
            "\tspecial_token : True\n",
            "\tnum_class : 42\n",
            "\tvocab_size : 28996\n",
            "\tmodel_save_dir : ./saved_models/00\n",
            "\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Finetune all embeddings.\n",
            "2020-12-01 23:45:10.545035: step 20/119245 (epoch 1/35), loss = 1.228846 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:45:21.104271: step 40/119245 (epoch 1/35), loss = 0.919734 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:45:29.782469: step 60/119245 (epoch 1/35), loss = 1.566440 (0.428 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:45:40.088525: step 80/119245 (epoch 1/35), loss = 0.550600 (0.484 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:45:48.903783: step 100/119245 (epoch 1/35), loss = 0.564679 (0.603 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:45:59.081790: step 120/119245 (epoch 1/35), loss = 1.175287 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:46:08.616142: step 140/119245 (epoch 1/35), loss = 0.677182 (0.543 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:46:16.998083: step 160/119245 (epoch 1/35), loss = 1.512534 (0.360 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:46:26.921024: step 180/119245 (epoch 1/35), loss = 0.602065 (1.011 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:46:36.606901: step 200/119245 (epoch 1/35), loss = 0.336516 (0.483 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:46:46.037274: step 220/119245 (epoch 1/35), loss = 1.456089 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:46:54.774807: step 240/119245 (epoch 1/35), loss = 1.108097 (0.340 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:47:04.659586: step 260/119245 (epoch 1/35), loss = 0.588407 (0.431 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:47:14.865885: step 280/119245 (epoch 1/35), loss = 1.122238 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:47:23.346049: step 300/119245 (epoch 1/35), loss = 0.737105 (0.431 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:47:33.465172: step 320/119245 (epoch 1/35), loss = 1.478998 (0.527 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:47:43.473405: step 340/119245 (epoch 1/35), loss = 0.767425 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:47:54.986591: step 360/119245 (epoch 1/35), loss = 1.020425 (0.430 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:48:03.928594: step 380/119245 (epoch 1/35), loss = 0.505961 (0.401 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:48:14.063795: step 400/119245 (epoch 1/35), loss = 0.905706 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:48:24.005446: step 420/119245 (epoch 1/35), loss = 1.057712 (0.299 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:48:33.363502: step 440/119245 (epoch 1/35), loss = 1.221985 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:48:45.864258: step 460/119245 (epoch 1/35), loss = 1.169326 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:48:55.190100: step 480/119245 (epoch 1/35), loss = 1.342517 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:49:06.518353: step 500/119245 (epoch 1/35), loss = 1.487619 (1.654 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:49:15.880191: step 520/119245 (epoch 1/35), loss = 0.907609 (0.448 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:49:26.829831: step 540/119245 (epoch 1/35), loss = 0.546745 (0.384 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:49:35.537371: step 560/119245 (epoch 1/35), loss = 1.753031 (0.391 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:49:45.422503: step 580/119245 (epoch 1/35), loss = 1.416556 (0.391 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:49:54.139992: step 600/119245 (epoch 1/35), loss = 0.731106 (0.410 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:50:03.525984: step 620/119245 (epoch 1/35), loss = 0.598117 (0.883 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:50:12.609237: step 640/119245 (epoch 1/35), loss = 0.832168 (0.481 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:50:22.450465: step 660/119245 (epoch 1/35), loss = 0.562372 (0.787 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:50:33.414507: step 680/119245 (epoch 1/35), loss = 0.552372 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:50:43.220788: step 700/119245 (epoch 1/35), loss = 0.643374 (0.339 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:50:53.255809: step 720/119245 (epoch 1/35), loss = 1.011643 (0.335 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:51:03.697995: step 740/119245 (epoch 1/35), loss = 0.616402 (0.553 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:51:13.119378: step 760/119245 (epoch 1/35), loss = 1.260540 (0.657 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:51:23.073526: step 780/119245 (epoch 1/35), loss = 0.558817 (0.554 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:51:32.764146: step 800/119245 (epoch 1/35), loss = 0.720902 (1.030 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:51:41.940130: step 820/119245 (epoch 1/35), loss = 0.927985 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:51:51.266052: step 840/119245 (epoch 1/35), loss = 0.749878 (0.341 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:52:00.608196: step 860/119245 (epoch 1/35), loss = 0.626070 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:52:12.292310: step 880/119245 (epoch 1/35), loss = 1.008691 (0.361 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:52:23.676163: step 900/119245 (epoch 1/35), loss = 1.386193 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:52:33.408498: step 920/119245 (epoch 1/35), loss = 0.387948 (0.402 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:52:42.399368: step 940/119245 (epoch 1/35), loss = 0.528314 (0.508 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:52:51.360806: step 960/119245 (epoch 1/35), loss = 0.971682 (0.523 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:52:59.875047: step 980/119245 (epoch 1/35), loss = 0.453999 (0.519 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:53:08.997577: step 1000/119245 (epoch 1/35), loss = 0.666706 (0.520 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:53:17.763683: step 1020/119245 (epoch 1/35), loss = 0.247427 (0.432 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:53:27.141591: step 1040/119245 (epoch 1/35), loss = 0.369823 (0.588 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:53:37.132486: step 1060/119245 (epoch 1/35), loss = 0.444344 (0.365 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:53:47.761376: step 1080/119245 (epoch 1/35), loss = 0.859357 (0.527 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:54:00.324542: step 1100/119245 (epoch 1/35), loss = 0.662479 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:54:09.292532: step 1120/119245 (epoch 1/35), loss = 0.558009 (0.485 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:54:18.958945: step 1140/119245 (epoch 1/35), loss = 0.334712 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:54:29.863572: step 1160/119245 (epoch 1/35), loss = 0.501502 (0.367 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:54:39.529751: step 1180/119245 (epoch 1/35), loss = 0.660486 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:54:49.484149: step 1200/119245 (epoch 1/35), loss = 0.294412 (0.524 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:54:58.540213: step 1220/119245 (epoch 1/35), loss = 1.200723 (0.371 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:55:07.774392: step 1240/119245 (epoch 1/35), loss = 0.899846 (0.430 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:55:17.346255: step 1260/119245 (epoch 1/35), loss = 0.193031 (0.631 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:55:28.510652: step 1280/119245 (epoch 1/35), loss = 0.355692 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:55:42.040600: step 1300/119245 (epoch 1/35), loss = 1.375744 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:55:51.930136: step 1320/119245 (epoch 1/35), loss = 0.430665 (0.440 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:56:02.169630: step 1340/119245 (epoch 1/35), loss = 0.486434 (0.551 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:56:11.450150: step 1360/119245 (epoch 1/35), loss = 0.684364 (0.391 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:56:21.362996: step 1380/119245 (epoch 1/35), loss = 0.488764 (0.653 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:56:30.253445: step 1400/119245 (epoch 1/35), loss = 0.501635 (0.455 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:56:40.179181: step 1420/119245 (epoch 1/35), loss = 1.768715 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:56:50.103722: step 1440/119245 (epoch 1/35), loss = 0.617820 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:57:00.304303: step 1460/119245 (epoch 1/35), loss = 0.402371 (0.460 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:57:09.856103: step 1480/119245 (epoch 1/35), loss = 0.646563 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:57:18.706641: step 1500/119245 (epoch 1/35), loss = 0.833114 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:57:28.340283: step 1520/119245 (epoch 1/35), loss = 0.608792 (0.435 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:57:39.422680: step 1540/119245 (epoch 1/35), loss = 0.590196 (0.299 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:57:48.759188: step 1560/119245 (epoch 1/35), loss = 0.501655 (0.502 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:57:58.920486: step 1580/119245 (epoch 1/35), loss = 0.586556 (0.534 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:58:08.336249: step 1600/119245 (epoch 1/35), loss = 0.563184 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:58:18.486045: step 1620/119245 (epoch 1/35), loss = 1.408849 (0.369 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:58:28.372229: step 1640/119245 (epoch 1/35), loss = 0.931141 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:58:38.304099: step 1660/119245 (epoch 1/35), loss = 0.541997 (0.327 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:58:48.271293: step 1680/119245 (epoch 1/35), loss = 0.541254 (0.424 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:58:57.411644: step 1700/119245 (epoch 1/35), loss = 0.853969 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:59:09.067956: step 1720/119245 (epoch 1/35), loss = 0.491793 (0.626 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:59:19.208189: step 1740/119245 (epoch 1/35), loss = 0.280168 (0.328 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:59:29.187865: step 1760/119245 (epoch 1/35), loss = 0.135756 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:59:38.287683: step 1780/119245 (epoch 1/35), loss = 0.533764 (0.420 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:59:48.863126: step 1800/119245 (epoch 1/35), loss = 1.035527 (0.534 sec/batch), lr: 0.030000\n",
            "2020-12-01 23:59:58.256583: step 1820/119245 (epoch 1/35), loss = 0.774285 (0.624 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:00:07.631027: step 1840/119245 (epoch 1/35), loss = 0.134187 (0.408 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:00:19.526041: step 1860/119245 (epoch 1/35), loss = 0.226567 (0.544 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:00:28.998356: step 1880/119245 (epoch 1/35), loss = 0.319620 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:00:40.260501: step 1900/119245 (epoch 1/35), loss = 0.158147 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:00:49.203661: step 1920/119245 (epoch 1/35), loss = 0.641907 (0.320 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:01:00.241983: step 1940/119245 (epoch 1/35), loss = 0.210967 (0.490 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:01:10.818032: step 1960/119245 (epoch 1/35), loss = 0.493675 (0.321 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:01:21.828540: step 1980/119245 (epoch 1/35), loss = 0.687957 (0.506 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:01:31.942017: step 2000/119245 (epoch 1/35), loss = 1.284552 (0.371 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:01:42.087551: step 2020/119245 (epoch 1/35), loss = 0.942496 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:01:52.828941: step 2040/119245 (epoch 1/35), loss = 0.788204 (0.771 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:02:01.296709: step 2060/119245 (epoch 1/35), loss = 0.483226 (0.299 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:02:11.847163: step 2080/119245 (epoch 1/35), loss = 0.457213 (0.464 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:02:21.311890: step 2100/119245 (epoch 1/35), loss = 0.231852 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:02:31.868834: step 2120/119245 (epoch 1/35), loss = 0.662979 (0.542 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:02:40.938340: step 2140/119245 (epoch 1/35), loss = 0.885510 (0.418 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:02:51.591151: step 2160/119245 (epoch 1/35), loss = 0.300805 (0.975 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:03:01.073799: step 2180/119245 (epoch 1/35), loss = 0.570094 (0.470 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:03:11.899572: step 2200/119245 (epoch 1/35), loss = 0.580644 (0.951 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:03:22.119838: step 2220/119245 (epoch 1/35), loss = 0.804853 (0.494 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:03:31.705724: step 2240/119245 (epoch 1/35), loss = 0.520698 (0.296 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:03:41.553227: step 2260/119245 (epoch 1/35), loss = 0.361956 (1.010 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:03:50.650399: step 2280/119245 (epoch 1/35), loss = 0.311886 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:04:00.567269: step 2300/119245 (epoch 1/35), loss = 0.502484 (0.412 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:04:10.021787: step 2320/119245 (epoch 1/35), loss = 0.781561 (0.631 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:04:20.828443: step 2340/119245 (epoch 1/35), loss = 0.336675 (0.454 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:04:32.385630: step 2360/119245 (epoch 1/35), loss = 0.153780 (0.424 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:04:42.911007: step 2380/119245 (epoch 1/35), loss = 0.463500 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:04:52.406735: step 2400/119245 (epoch 1/35), loss = 0.489491 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:05:01.673307: step 2420/119245 (epoch 1/35), loss = 0.235851 (0.558 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:05:12.470235: step 2440/119245 (epoch 1/35), loss = 0.698674 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:05:22.326798: step 2460/119245 (epoch 1/35), loss = 0.304179 (0.463 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:05:32.697072: step 2480/119245 (epoch 1/35), loss = 0.573522 (0.527 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:05:42.619739: step 2500/119245 (epoch 1/35), loss = 0.551353 (0.491 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:05:52.039410: step 2520/119245 (epoch 1/35), loss = 0.444445 (0.370 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:06:01.285648: step 2540/119245 (epoch 1/35), loss = 0.315354 (0.578 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:06:11.155494: step 2560/119245 (epoch 1/35), loss = 0.462192 (0.623 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:06:21.105295: step 2580/119245 (epoch 1/35), loss = 0.106651 (0.315 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:06:29.678786: step 2600/119245 (epoch 1/35), loss = 0.819175 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:06:40.103541: step 2620/119245 (epoch 1/35), loss = 0.259333 (0.822 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:06:49.992405: step 2640/119245 (epoch 1/35), loss = 0.049078 (0.517 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:06:59.048467: step 2660/119245 (epoch 1/35), loss = 0.580217 (0.343 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:07:07.782712: step 2680/119245 (epoch 1/35), loss = 0.462360 (0.554 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:07:17.418155: step 2700/119245 (epoch 1/35), loss = 0.375535 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:07:29.232565: step 2720/119245 (epoch 1/35), loss = 0.457308 (1.520 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:07:38.590788: step 2740/119245 (epoch 1/35), loss = 0.065690 (1.003 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:07:48.490489: step 2760/119245 (epoch 1/35), loss = 0.440581 (0.344 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:07:57.734549: step 2780/119245 (epoch 1/35), loss = 0.297357 (0.414 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:08:07.533062: step 2800/119245 (epoch 1/35), loss = 0.233429 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:08:17.279000: step 2820/119245 (epoch 1/35), loss = 0.455487 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:08:26.864627: step 2840/119245 (epoch 1/35), loss = 0.222022 (0.524 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:08:36.966383: step 2860/119245 (epoch 1/35), loss = 0.404293 (0.305 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:08:46.419314: step 2880/119245 (epoch 1/35), loss = 0.528696 (0.326 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:08:55.768996: step 2900/119245 (epoch 1/35), loss = 0.476665 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:09:05.138572: step 2920/119245 (epoch 1/35), loss = 0.386227 (0.294 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:09:13.901361: step 2940/119245 (epoch 1/35), loss = 0.497735 (0.301 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:09:25.693828: step 2960/119245 (epoch 1/35), loss = 0.635980 (0.620 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:09:35.046691: step 2980/119245 (epoch 1/35), loss = 0.152399 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:09:44.536587: step 3000/119245 (epoch 1/35), loss = 0.266869 (0.325 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:09:55.260906: step 3020/119245 (epoch 1/35), loss = 0.564953 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:10:04.743981: step 3040/119245 (epoch 1/35), loss = 0.421172 (0.335 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:10:14.190206: step 3060/119245 (epoch 1/35), loss = 0.165778 (0.467 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:10:25.155599: step 3080/119245 (epoch 1/35), loss = 0.423091 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:10:35.153382: step 3100/119245 (epoch 1/35), loss = 0.142399 (0.615 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:10:45.321668: step 3120/119245 (epoch 1/35), loss = 0.679206 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:10:54.789830: step 3140/119245 (epoch 1/35), loss = 0.240453 (0.414 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:11:04.448874: step 3160/119245 (epoch 1/35), loss = 0.113459 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:11:12.540335: step 3180/119245 (epoch 1/35), loss = 0.303157 (0.367 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:11:23.487997: step 3200/119245 (epoch 1/35), loss = 0.098314 (0.326 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:11:33.036550: step 3220/119245 (epoch 1/35), loss = 0.686083 (0.621 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:11:42.011028: step 3240/119245 (epoch 1/35), loss = 0.351126 (0.448 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:11:50.547233: step 3260/119245 (epoch 1/35), loss = 0.523426 (0.526 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:12:00.662555: step 3280/119245 (epoch 1/35), loss = 0.275438 (0.672 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:12:10.200603: step 3300/119245 (epoch 1/35), loss = 0.424840 (0.373 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:12:21.260197: step 3320/119245 (epoch 1/35), loss = 0.249027 (0.977 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:12:31.225061: step 3340/119245 (epoch 1/35), loss = 0.523906 (0.489 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:12:40.598199: step 3360/119245 (epoch 1/35), loss = 0.693078 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:12:50.633972: step 3380/119245 (epoch 1/35), loss = 0.275897 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:13:01.964201: step 3400/119245 (epoch 1/35), loss = 0.141973 (0.474 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  81.46%  R:  79.29%  F1:  80.36%  #: 338\n",
            "org:city_of_headquarters             P:  65.22%  R:  13.76%  F1:  22.73%  #: 109\n",
            "org:country_of_headquarters          P:  62.50%  R:   8.47%  F1:  14.93%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  93.33%  R:  36.84%  F1:  52.83%  #: 38\n",
            "org:founded_by                       P: 100.00%  R:   2.63%  F1:   5.13%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:   0.00%  R:   0.00%  F1:   0.00%  #: 85\n",
            "org:number_of_employees/members      P:   0.00%  R:   0.00%  F1:   0.00%  #: 27\n",
            "org:parents                          P:  26.67%  R:   4.17%  F1:   7.21%  #: 96\n",
            "org:political/religious_affiliation  P:   0.00%  R:   0.00%  F1:   0.00%  #: 10\n",
            "org:shareholders                     P:   0.00%  R:   0.00%  F1:   0.00%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  63.79%  R:  52.86%  F1:  57.81%  #: 70\n",
            "org:subsidiaries                     P:   0.00%  R:   0.00%  F1:   0.00%  #: 113\n",
            "org:top_members/employees            P:  67.01%  R:  84.46%  F1:  74.73%  #: 534\n",
            "org:website                          P:  93.98%  R:  90.70%  F1:  92.31%  #: 86\n",
            "per:age                              P:  94.58%  R:  64.61%  F1:  76.77%  #: 243\n",
            "per:alternate_names                  P:  88.89%  R:  21.05%  F1:  34.04%  #: 38\n",
            "per:cause_of_death                   P:  94.17%  R:  57.74%  F1:  71.59%  #: 168\n",
            "per:charges                          P:  85.71%  R:  11.43%  F1:  20.17%  #: 105\n",
            "per:children                         P:  65.00%  R:  26.26%  F1:  37.41%  #: 99\n",
            "per:cities_of_residence              P:  44.44%  R:  22.35%  F1:  29.74%  #: 179\n",
            "per:city_of_birth                    P:  62.50%  R:  15.15%  F1:  24.39%  #: 33\n",
            "per:city_of_death                    P:  79.73%  R:  50.00%  F1:  61.46%  #: 118\n",
            "per:countries_of_residence           P:  30.87%  R:  20.35%  F1:  24.53%  #: 226\n",
            "per:country_of_birth                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 20\n",
            "per:country_of_death                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 46\n",
            "per:date_of_birth                    P:  92.59%  R:  80.65%  F1:  86.21%  #: 31\n",
            "per:date_of_death                    P:  87.50%  R:  27.18%  F1:  41.48%  #: 206\n",
            "per:employee_of                      P:  64.77%  R:  48.53%  F1:  55.49%  #: 375\n",
            "per:origin                           P:  62.00%  R:  29.52%  F1:  40.00%  #: 210\n",
            "per:other_family                     P:   0.00%  R:   0.00%  F1:   0.00%  #: 80\n",
            "per:parents                          P:  30.56%  R:  39.29%  F1:  34.38%  #: 56\n",
            "per:religion                         P:   0.00%  R:   0.00%  F1:   0.00%  #: 53\n",
            "per:schools_attended                 P:  88.89%  R:  32.00%  F1:  47.06%  #: 50\n",
            "per:siblings                         P:  26.83%  R:  36.67%  F1:  30.99%  #: 30\n",
            "per:spouse                           P:  79.41%  R:  50.94%  F1:  62.07%  #: 159\n",
            "per:stateorprovince_of_birth         P: 100.00%  R:   3.85%  F1:   7.41%  #: 26\n",
            "per:stateorprovince_of_death         P:   0.00%  R:   0.00%  F1:   0.00%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  29.57%  R:  47.22%  F1:  36.36%  #: 72\n",
            "per:title                            P:  81.92%  R:  76.93%  F1:  79.35%  #: 919\n",
            "\n",
            "Final Score:\n",
            "19071 Guess as no relation\n",
            "3560 guess\n",
            "2531 correct\n",
            "5436 gold\n",
            "Precision (micro): 71.096%\n",
            "   Recall (micro): 46.560%\n",
            "       F1 (micro): 56.269%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 1: train_loss = 0.633748, dev_loss = 0.519477, dev_f1 = 0.5627\n",
            "model saved to ./saved_models/00/checkpoint_epoch_1.pt\n",
            "new best model saved.\n",
            "\n",
            "2020-12-02 00:16:03.239064: step 3420/119245 (epoch 2/35), loss = 0.048300 (0.625 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:16:14.472275: step 3440/119245 (epoch 2/35), loss = 0.042760 (0.424 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:16:24.430005: step 3460/119245 (epoch 2/35), loss = 0.206670 (0.331 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:16:33.678295: step 3480/119245 (epoch 2/35), loss = 0.457293 (0.359 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:16:43.831292: step 3500/119245 (epoch 2/35), loss = 0.198751 (0.492 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:16:53.595664: step 3520/119245 (epoch 2/35), loss = 0.288957 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:17:03.732354: step 3540/119245 (epoch 2/35), loss = 0.467141 (0.373 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:17:12.574667: step 3560/119245 (epoch 2/35), loss = 0.226843 (0.380 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:17:21.531566: step 3580/119245 (epoch 2/35), loss = 0.215062 (0.455 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:17:31.975094: step 3600/119245 (epoch 2/35), loss = 0.177867 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:17:41.780161: step 3620/119245 (epoch 2/35), loss = 0.091945 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:17:50.995268: step 3640/119245 (epoch 2/35), loss = 0.158280 (0.448 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:18:00.711716: step 3660/119245 (epoch 2/35), loss = 0.291140 (0.433 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:18:11.292250: step 3680/119245 (epoch 2/35), loss = 0.514221 (0.372 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:18:19.445670: step 3700/119245 (epoch 2/35), loss = 0.588892 (0.365 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:18:28.930659: step 3720/119245 (epoch 2/35), loss = 0.398647 (0.412 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:18:39.839628: step 3740/119245 (epoch 2/35), loss = 0.859895 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:18:49.911255: step 3760/119245 (epoch 2/35), loss = 0.654276 (0.906 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:19:00.779826: step 3780/119245 (epoch 2/35), loss = 0.671829 (0.439 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:19:09.852335: step 3800/119245 (epoch 2/35), loss = 0.658096 (0.623 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:19:20.777983: step 3820/119245 (epoch 2/35), loss = 0.418965 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:19:30.011305: step 3840/119245 (epoch 2/35), loss = 0.623873 (0.557 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:19:42.796804: step 3860/119245 (epoch 2/35), loss = 0.191967 (0.904 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:19:52.106716: step 3880/119245 (epoch 2/35), loss = 0.488068 (0.495 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:20:02.297729: step 3900/119245 (epoch 2/35), loss = 0.431879 (0.629 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:20:12.864763: step 3920/119245 (epoch 2/35), loss = 0.064016 (0.652 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:20:22.107017: step 3940/119245 (epoch 2/35), loss = 0.171288 (0.344 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:20:32.856614: step 3960/119245 (epoch 2/35), loss = 0.332841 (0.507 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:20:42.214332: step 3980/119245 (epoch 2/35), loss = 0.156654 (0.531 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:20:51.187399: step 4000/119245 (epoch 2/35), loss = 0.233610 (0.488 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:20:59.960980: step 4020/119245 (epoch 2/35), loss = 0.228021 (0.452 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:21:09.469382: step 4040/119245 (epoch 2/35), loss = 0.296917 (0.353 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:21:19.509149: step 4060/119245 (epoch 2/35), loss = 0.512910 (0.490 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:21:30.525265: step 4080/119245 (epoch 2/35), loss = 0.174852 (0.353 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:21:40.625805: step 4100/119245 (epoch 2/35), loss = 0.567056 (0.560 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:21:50.507205: step 4120/119245 (epoch 2/35), loss = 0.491382 (0.368 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:22:00.723804: step 4140/119245 (epoch 2/35), loss = 0.375772 (0.584 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:22:10.175227: step 4160/119245 (epoch 2/35), loss = 0.589077 (0.945 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:22:19.424713: step 4180/119245 (epoch 2/35), loss = 0.219561 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:22:28.960845: step 4200/119245 (epoch 2/35), loss = 0.114875 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:22:39.084451: step 4220/119245 (epoch 2/35), loss = 0.111062 (0.544 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:22:48.325696: step 4240/119245 (epoch 2/35), loss = 0.402851 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:22:57.606649: step 4260/119245 (epoch 2/35), loss = 0.424424 (0.610 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:23:08.868585: step 4280/119245 (epoch 2/35), loss = 0.654078 (0.468 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:23:19.646924: step 4300/119245 (epoch 2/35), loss = 0.203299 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:23:30.156259: step 4320/119245 (epoch 2/35), loss = 0.093214 (0.478 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:23:39.049454: step 4340/119245 (epoch 2/35), loss = 0.131852 (0.432 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:23:48.166884: step 4360/119245 (epoch 2/35), loss = 0.676649 (0.359 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:23:56.900022: step 4380/119245 (epoch 2/35), loss = 0.180885 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:24:05.412361: step 4400/119245 (epoch 2/35), loss = 0.490499 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:24:14.074758: step 4420/119245 (epoch 2/35), loss = 0.912348 (0.446 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:24:23.413886: step 4440/119245 (epoch 2/35), loss = 0.048035 (0.582 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:24:33.981443: step 4460/119245 (epoch 2/35), loss = 0.697796 (0.341 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:24:44.286622: step 4480/119245 (epoch 2/35), loss = 0.373681 (0.619 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:24:56.726342: step 4500/119245 (epoch 2/35), loss = 0.340932 (0.508 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:25:05.827796: step 4520/119245 (epoch 2/35), loss = 0.816533 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:25:15.092608: step 4540/119245 (epoch 2/35), loss = 0.888906 (0.608 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:25:26.085424: step 4560/119245 (epoch 2/35), loss = 0.754473 (0.471 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:25:36.125368: step 4580/119245 (epoch 2/35), loss = 0.673557 (1.028 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:25:45.447480: step 4600/119245 (epoch 2/35), loss = 0.255792 (0.611 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:25:55.403538: step 4620/119245 (epoch 2/35), loss = 0.097012 (0.688 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:26:04.586872: step 4640/119245 (epoch 2/35), loss = 0.073529 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:26:13.637936: step 4660/119245 (epoch 2/35), loss = 0.507753 (0.344 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:26:24.033641: step 4680/119245 (epoch 2/35), loss = 0.734150 (0.580 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:26:37.669807: step 4700/119245 (epoch 2/35), loss = 0.369313 (0.541 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:26:47.847784: step 4720/119245 (epoch 2/35), loss = 0.406718 (0.625 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:26:58.466548: step 4740/119245 (epoch 2/35), loss = 0.498792 (0.652 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:27:08.077339: step 4760/119245 (epoch 2/35), loss = 0.438973 (0.594 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:27:17.437809: step 4780/119245 (epoch 2/35), loss = 0.276217 (0.654 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:27:26.679583: step 4800/119245 (epoch 2/35), loss = 0.531343 (0.470 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:27:36.635482: step 4820/119245 (epoch 2/35), loss = 0.078022 (0.662 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:27:46.507526: step 4840/119245 (epoch 2/35), loss = 0.298132 (0.637 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:27:55.956738: step 4860/119245 (epoch 2/35), loss = 0.600152 (0.507 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:28:05.991579: step 4880/119245 (epoch 2/35), loss = 0.196870 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:28:15.151412: step 4900/119245 (epoch 2/35), loss = 0.369108 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:28:24.841482: step 4920/119245 (epoch 2/35), loss = 0.728522 (0.302 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:28:35.140725: step 4940/119245 (epoch 2/35), loss = 0.084538 (0.949 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:28:44.769200: step 4960/119245 (epoch 2/35), loss = 0.730271 (0.342 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:28:54.775769: step 4980/119245 (epoch 2/35), loss = 0.312084 (0.630 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:29:04.472667: step 5000/119245 (epoch 2/35), loss = 0.424766 (0.340 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:29:14.366647: step 5020/119245 (epoch 2/35), loss = 0.291263 (0.591 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:29:24.640473: step 5040/119245 (epoch 2/35), loss = 0.169701 (0.691 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:29:34.397432: step 5060/119245 (epoch 2/35), loss = 0.131234 (0.474 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:29:44.386012: step 5080/119245 (epoch 2/35), loss = 0.260397 (0.627 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:29:53.397336: step 5100/119245 (epoch 2/35), loss = 0.180083 (0.361 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:30:03.370136: step 5120/119245 (epoch 2/35), loss = 0.395416 (0.592 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:30:14.533294: step 5140/119245 (epoch 2/35), loss = 0.604191 (0.497 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:30:24.617993: step 5160/119245 (epoch 2/35), loss = 0.526729 (0.445 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:30:34.233910: step 5180/119245 (epoch 2/35), loss = 0.589176 (0.546 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:30:44.567718: step 5200/119245 (epoch 2/35), loss = 0.192344 (0.570 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:30:53.564597: step 5220/119245 (epoch 2/35), loss = 0.439499 (0.482 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:31:03.456749: step 5240/119245 (epoch 2/35), loss = 0.343835 (0.434 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:31:14.167351: step 5260/119245 (epoch 2/35), loss = 0.342628 (1.648 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:31:24.987399: step 5280/119245 (epoch 2/35), loss = 0.305089 (0.510 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:31:36.085261: step 5300/119245 (epoch 2/35), loss = 0.624142 (0.350 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:31:45.355837: step 5320/119245 (epoch 2/35), loss = 0.597134 (0.333 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:31:55.304835: step 5340/119245 (epoch 2/35), loss = 0.224079 (0.551 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:32:06.182954: step 5360/119245 (epoch 2/35), loss = 0.514393 (0.456 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:32:17.354743: step 5380/119245 (epoch 2/35), loss = 0.506196 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:32:27.165304: step 5400/119245 (epoch 2/35), loss = 0.141630 (0.456 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:32:37.475288: step 5420/119245 (epoch 2/35), loss = 0.183365 (0.455 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:32:46.860314: step 5440/119245 (epoch 2/35), loss = 0.023885 (0.465 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:32:57.289834: step 5460/119245 (epoch 2/35), loss = 0.606159 (0.493 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:33:07.491467: step 5480/119245 (epoch 2/35), loss = 0.289123 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:33:17.148262: step 5500/119245 (epoch 2/35), loss = 0.148274 (0.547 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:33:27.233142: step 5520/119245 (epoch 2/35), loss = 0.535733 (0.463 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:33:36.311999: step 5540/119245 (epoch 2/35), loss = 1.062941 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:33:47.005946: step 5560/119245 (epoch 2/35), loss = 0.401469 (0.973 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:33:57.108339: step 5580/119245 (epoch 2/35), loss = 0.353264 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:34:06.633661: step 5600/119245 (epoch 2/35), loss = 0.350403 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:34:17.129253: step 5620/119245 (epoch 2/35), loss = 0.231181 (0.564 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:34:27.558005: step 5640/119245 (epoch 2/35), loss = 0.218039 (0.444 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:34:36.460247: step 5660/119245 (epoch 2/35), loss = 0.290186 (0.518 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:34:46.414075: step 5680/119245 (epoch 2/35), loss = 0.563129 (0.551 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:34:55.701257: step 5700/119245 (epoch 2/35), loss = 0.308144 (0.549 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:35:05.583889: step 5720/119245 (epoch 2/35), loss = 0.299368 (0.451 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:35:16.379692: step 5740/119245 (epoch 2/35), loss = 0.353931 (0.455 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:35:27.325051: step 5760/119245 (epoch 2/35), loss = 0.218940 (0.620 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:35:37.897191: step 5780/119245 (epoch 2/35), loss = 0.221511 (0.584 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:35:47.812064: step 5800/119245 (epoch 2/35), loss = 0.788156 (0.488 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:35:57.117200: step 5820/119245 (epoch 2/35), loss = 0.474314 (0.373 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:36:07.057013: step 5840/119245 (epoch 2/35), loss = 0.097497 (0.352 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:36:18.038211: step 5860/119245 (epoch 2/35), loss = 0.483620 (0.589 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:36:27.726946: step 5880/119245 (epoch 2/35), loss = 0.440716 (0.479 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:36:37.282038: step 5900/119245 (epoch 2/35), loss = 0.216043 (0.732 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:36:47.567179: step 5920/119245 (epoch 2/35), loss = 0.538725 (0.475 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:36:56.822454: step 5940/119245 (epoch 2/35), loss = 0.344169 (0.418 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:37:06.105297: step 5960/119245 (epoch 2/35), loss = 0.392615 (0.386 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:37:16.722068: step 5980/119245 (epoch 2/35), loss = 0.305342 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:37:25.219326: step 6000/119245 (epoch 2/35), loss = 0.453844 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:37:34.882615: step 6020/119245 (epoch 2/35), loss = 0.235920 (0.505 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:37:45.331892: step 6040/119245 (epoch 2/35), loss = 0.144683 (0.534 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:37:53.824056: step 6060/119245 (epoch 2/35), loss = 0.711322 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:38:03.039306: step 6080/119245 (epoch 2/35), loss = 0.084469 (0.460 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:38:12.546160: step 6100/119245 (epoch 2/35), loss = 0.227625 (0.360 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:38:22.925725: step 6120/119245 (epoch 2/35), loss = 0.192674 (0.395 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:38:33.104536: step 6140/119245 (epoch 2/35), loss = 0.251997 (0.490 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:38:43.607449: step 6160/119245 (epoch 2/35), loss = 0.584087 (0.521 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:38:52.406153: step 6180/119245 (epoch 2/35), loss = 0.107997 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:39:02.096942: step 6200/119245 (epoch 2/35), loss = 0.662585 (0.520 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:39:11.843640: step 6220/119245 (epoch 2/35), loss = 0.656029 (0.407 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:39:21.485983: step 6240/119245 (epoch 2/35), loss = 0.646035 (0.404 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:39:31.976601: step 6260/119245 (epoch 2/35), loss = 0.648240 (0.561 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:39:41.592853: step 6280/119245 (epoch 2/35), loss = 0.325334 (1.008 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:39:50.684801: step 6300/119245 (epoch 2/35), loss = 0.288614 (0.349 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:39:59.546165: step 6320/119245 (epoch 2/35), loss = 0.136766 (0.532 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:40:08.934182: step 6340/119245 (epoch 2/35), loss = 0.598486 (0.372 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:40:19.702061: step 6360/119245 (epoch 2/35), loss = 0.147726 (1.565 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:40:29.781598: step 6380/119245 (epoch 2/35), loss = 0.302600 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:40:39.215660: step 6400/119245 (epoch 2/35), loss = 0.181006 (0.649 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:40:50.010866: step 6420/119245 (epoch 2/35), loss = 0.169748 (0.428 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:40:59.614865: step 6440/119245 (epoch 2/35), loss = 0.586876 (0.593 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:41:09.117888: step 6460/119245 (epoch 2/35), loss = 0.141765 (0.441 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:41:19.934674: step 6480/119245 (epoch 2/35), loss = 0.101120 (0.333 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:41:29.190956: step 6500/119245 (epoch 2/35), loss = 0.219645 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:41:39.845156: step 6520/119245 (epoch 2/35), loss = 0.159530 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:41:49.563385: step 6540/119245 (epoch 2/35), loss = 0.227540 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:41:59.190772: step 6560/119245 (epoch 2/35), loss = 0.936148 (0.975 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:42:07.650293: step 6580/119245 (epoch 2/35), loss = 0.073403 (0.466 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:42:16.801269: step 6600/119245 (epoch 2/35), loss = 0.452929 (0.456 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:42:27.552170: step 6620/119245 (epoch 2/35), loss = 0.394010 (0.479 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:42:36.353404: step 6640/119245 (epoch 2/35), loss = 0.159465 (0.432 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:42:45.168273: step 6660/119245 (epoch 2/35), loss = 0.273869 (0.562 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:42:54.240755: step 6680/119245 (epoch 2/35), loss = 0.324326 (0.549 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:43:04.684257: step 6700/119245 (epoch 2/35), loss = 0.172070 (0.480 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:43:14.872211: step 6720/119245 (epoch 2/35), loss = 0.700770 (0.371 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:43:24.647029: step 6740/119245 (epoch 2/35), loss = 0.126605 (0.616 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:43:34.166597: step 6760/119245 (epoch 2/35), loss = 0.394048 (0.373 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:43:43.998755: step 6780/119245 (epoch 2/35), loss = 0.309582 (0.519 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:43:55.833255: step 6800/119245 (epoch 2/35), loss = 0.303528 (0.499 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  82.42%  R:  80.47%  F1:  81.44%  #: 338\n",
            "org:city_of_headquarters             P:  72.55%  R:  33.94%  F1:  46.25%  #: 109\n",
            "org:country_of_headquarters          P:  56.91%  R:  39.55%  F1:  46.67%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  81.82%  R:  71.05%  F1:  76.06%  #: 38\n",
            "org:founded_by                       P:  79.41%  R:  35.53%  F1:  49.09%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P: 100.00%  R:   2.35%  F1:   4.60%  #: 85\n",
            "org:number_of_employees/members      P: 100.00%  R:  18.52%  F1:  31.25%  #: 27\n",
            "org:parents                          P:  48.15%  R:  27.08%  F1:  34.67%  #: 96\n",
            "org:political/religious_affiliation  P:  22.73%  R:  50.00%  F1:  31.25%  #: 10\n",
            "org:shareholders                     P:   0.00%  R:   0.00%  F1:   0.00%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  47.75%  R:  75.71%  F1:  58.56%  #: 70\n",
            "org:subsidiaries                     P:   0.00%  R:   0.00%  F1:   0.00%  #: 113\n",
            "org:top_members/employees            P:  64.99%  R:  84.83%  F1:  73.60%  #: 534\n",
            "org:website                          P:  86.46%  R:  96.51%  F1:  91.21%  #: 86\n",
            "per:age                              P:  93.75%  R:  80.25%  F1:  86.47%  #: 243\n",
            "per:alternate_names                  P:  80.00%  R:  31.58%  F1:  45.28%  #: 38\n",
            "per:cause_of_death                   P:  84.17%  R:  69.64%  F1:  76.22%  #: 168\n",
            "per:charges                          P:  82.19%  R:  57.14%  F1:  67.42%  #: 105\n",
            "per:children                         P:  69.35%  R:  43.43%  F1:  53.42%  #: 99\n",
            "per:cities_of_residence              P:  50.00%  R:  30.17%  F1:  37.63%  #: 179\n",
            "per:city_of_birth                    P:  60.00%  R:  81.82%  F1:  69.23%  #: 33\n",
            "per:city_of_death                    P:  78.16%  R:  57.63%  F1:  66.34%  #: 118\n",
            "per:countries_of_residence           P:  31.01%  R:  47.35%  F1:  37.48%  #: 226\n",
            "per:country_of_birth                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 20\n",
            "per:country_of_death                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 46\n",
            "per:date_of_birth                    P:  90.32%  R:  90.32%  F1:  90.32%  #: 31\n",
            "per:date_of_death                    P:  86.84%  R:  32.04%  F1:  46.81%  #: 206\n",
            "per:employee_of                      P:  69.66%  R:  53.87%  F1:  60.75%  #: 375\n",
            "per:origin                           P:  79.35%  R:  34.76%  F1:  48.34%  #: 210\n",
            "per:other_family                     P:   0.00%  R:   0.00%  F1:   0.00%  #: 80\n",
            "per:parents                          P:  42.25%  R:  53.57%  F1:  47.24%  #: 56\n",
            "per:religion                         P:  80.00%  R:  15.09%  F1:  25.40%  #: 53\n",
            "per:schools_attended                 P:  80.49%  R:  66.00%  F1:  72.53%  #: 50\n",
            "per:siblings                         P:  45.24%  R:  63.33%  F1:  52.78%  #: 30\n",
            "per:spouse                           P:  73.64%  R:  59.75%  F1:  65.97%  #: 159\n",
            "per:stateorprovince_of_birth         P:  72.00%  R:  69.23%  F1:  70.59%  #: 26\n",
            "per:stateorprovince_of_death         P:  80.00%  R:   9.76%  F1:  17.39%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  36.84%  R:  48.61%  F1:  41.92%  #: 72\n",
            "per:title                            P:  79.06%  R:  84.22%  F1:  81.56%  #: 919\n",
            "\n",
            "Final Score:\n",
            "18101 Guess as no relation\n",
            "4530 guess\n",
            "3128 correct\n",
            "5436 gold\n",
            "Precision (micro): 69.051%\n",
            "   Recall (micro): 57.542%\n",
            "       F1 (micro): 62.773%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 2: train_loss = 0.382966, dev_loss = 0.473135, dev_f1 = 0.6277\n",
            "model saved to ./saved_models/00/checkpoint_epoch_2.pt\n",
            "new best model saved.\n",
            "\n",
            "2020-12-02 00:46:57.034903: step 6820/119245 (epoch 3/35), loss = 0.526961 (0.370 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:47:07.466757: step 6840/119245 (epoch 3/35), loss = 0.143336 (1.657 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:47:18.402123: step 6860/119245 (epoch 3/35), loss = 0.207104 (0.365 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:47:27.572965: step 6880/119245 (epoch 3/35), loss = 0.174680 (1.051 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:47:37.838861: step 6900/119245 (epoch 3/35), loss = 0.353574 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:47:47.139676: step 6920/119245 (epoch 3/35), loss = 0.460996 (0.468 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:47:56.979901: step 6940/119245 (epoch 3/35), loss = 0.201468 (0.553 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:48:06.322504: step 6960/119245 (epoch 3/35), loss = 0.403484 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:48:14.974858: step 6980/119245 (epoch 3/35), loss = 0.191509 (0.610 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:48:25.258420: step 7000/119245 (epoch 3/35), loss = 0.127346 (1.024 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:48:35.231578: step 7020/119245 (epoch 3/35), loss = 0.365242 (0.448 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:48:44.445571: step 7040/119245 (epoch 3/35), loss = 0.122712 (0.612 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:48:53.958843: step 7060/119245 (epoch 3/35), loss = 0.253916 (0.658 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:49:04.806022: step 7080/119245 (epoch 3/35), loss = 0.098263 (0.595 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:49:13.096116: step 7100/119245 (epoch 3/35), loss = 0.254847 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:49:22.119368: step 7120/119245 (epoch 3/35), loss = 0.151025 (0.539 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:49:32.879907: step 7140/119245 (epoch 3/35), loss = 0.619923 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:49:42.676573: step 7160/119245 (epoch 3/35), loss = 0.276245 (0.552 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:49:53.917033: step 7180/119245 (epoch 3/35), loss = 0.304291 (0.520 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:50:03.495998: step 7200/119245 (epoch 3/35), loss = 0.353271 (0.527 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:50:14.028905: step 7220/119245 (epoch 3/35), loss = 0.433054 (0.454 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:50:23.119814: step 7240/119245 (epoch 3/35), loss = 0.343163 (0.428 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:50:35.126892: step 7260/119245 (epoch 3/35), loss = 0.087378 (1.615 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:50:44.862317: step 7280/119245 (epoch 3/35), loss = 0.166016 (0.428 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:50:54.947994: step 7300/119245 (epoch 3/35), loss = 0.272918 (0.625 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:51:05.782587: step 7320/119245 (epoch 3/35), loss = 0.662114 (0.603 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:51:15.032305: step 7340/119245 (epoch 3/35), loss = 0.151739 (0.580 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:51:26.180331: step 7360/119245 (epoch 3/35), loss = 0.219598 (0.343 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:51:34.996087: step 7380/119245 (epoch 3/35), loss = 0.310714 (0.468 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:51:44.318552: step 7400/119245 (epoch 3/35), loss = 0.724036 (0.511 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:51:53.262297: step 7420/119245 (epoch 3/35), loss = 0.325399 (0.332 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:52:02.458936: step 7440/119245 (epoch 3/35), loss = 0.061155 (0.435 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:52:11.831346: step 7460/119245 (epoch 3/35), loss = 0.529369 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:52:22.166761: step 7480/119245 (epoch 3/35), loss = 0.358565 (0.536 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:52:33.417613: step 7500/119245 (epoch 3/35), loss = 0.153796 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:52:42.217992: step 7520/119245 (epoch 3/35), loss = 0.295745 (0.620 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:52:52.613452: step 7540/119245 (epoch 3/35), loss = 0.230431 (0.576 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:53:02.381291: step 7560/119245 (epoch 3/35), loss = 0.379875 (0.348 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:53:12.105808: step 7580/119245 (epoch 3/35), loss = 0.205275 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:53:21.728053: step 7600/119245 (epoch 3/35), loss = 0.281604 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:53:31.656166: step 7620/119245 (epoch 3/35), loss = 0.312276 (0.339 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:53:40.853455: step 7640/119245 (epoch 3/35), loss = 0.483013 (0.635 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:53:50.320166: step 7660/119245 (epoch 3/35), loss = 0.555202 (0.351 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:54:00.680771: step 7680/119245 (epoch 3/35), loss = 0.044675 (0.579 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:54:10.825800: step 7700/119245 (epoch 3/35), loss = 0.064513 (0.473 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:54:22.416676: step 7720/119245 (epoch 3/35), loss = 0.356511 (0.365 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:54:31.883741: step 7740/119245 (epoch 3/35), loss = 0.271864 (0.446 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:54:40.809309: step 7760/119245 (epoch 3/35), loss = 0.488718 (0.521 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:54:49.469399: step 7780/119245 (epoch 3/35), loss = 0.130971 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:54:58.159901: step 7800/119245 (epoch 3/35), loss = 0.076801 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:55:06.991702: step 7820/119245 (epoch 3/35), loss = 0.047676 (0.460 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:55:15.860792: step 7840/119245 (epoch 3/35), loss = 0.507104 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:55:26.400735: step 7860/119245 (epoch 3/35), loss = 0.608147 (0.537 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:55:35.519817: step 7880/119245 (epoch 3/35), loss = 0.503588 (0.686 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:55:46.818028: step 7900/119245 (epoch 3/35), loss = 0.534783 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:55:58.351660: step 7920/119245 (epoch 3/35), loss = 0.314571 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:56:07.306114: step 7940/119245 (epoch 3/35), loss = 0.518445 (0.558 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:56:17.690476: step 7960/119245 (epoch 3/35), loss = 0.269199 (0.406 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:56:28.152459: step 7980/119245 (epoch 3/35), loss = 0.737144 (0.529 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:56:37.250530: step 8000/119245 (epoch 3/35), loss = 0.375782 (0.454 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:56:47.427517: step 8020/119245 (epoch 3/35), loss = 0.366511 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:56:56.786472: step 8040/119245 (epoch 3/35), loss = 0.399381 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:57:06.040136: step 8060/119245 (epoch 3/35), loss = 0.137199 (0.532 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:57:15.637537: step 8080/119245 (epoch 3/35), loss = 0.094537 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:57:27.929850: step 8100/119245 (epoch 3/35), loss = 0.096538 (0.488 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:57:39.704669: step 8120/119245 (epoch 3/35), loss = 0.246408 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:57:49.968578: step 8140/119245 (epoch 3/35), loss = 1.275410 (0.558 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:57:59.841749: step 8160/119245 (epoch 3/35), loss = 0.545798 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:58:09.481682: step 8180/119245 (epoch 3/35), loss = 0.287321 (0.449 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:58:18.680508: step 8200/119245 (epoch 3/35), loss = 0.132570 (0.463 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:58:28.353590: step 8220/119245 (epoch 3/35), loss = 0.239128 (0.362 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:58:37.969551: step 8240/119245 (epoch 3/35), loss = 0.039624 (0.431 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:58:47.594172: step 8260/119245 (epoch 3/35), loss = 0.234537 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:58:57.919997: step 8280/119245 (epoch 3/35), loss = 0.075631 (0.512 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:59:07.373872: step 8300/119245 (epoch 3/35), loss = 0.385118 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:59:16.508906: step 8320/119245 (epoch 3/35), loss = 0.390920 (0.616 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:59:26.372810: step 8340/119245 (epoch 3/35), loss = 0.510153 (0.533 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:59:36.503442: step 8360/119245 (epoch 3/35), loss = 0.498198 (0.543 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:59:46.232828: step 8380/119245 (epoch 3/35), loss = 0.086887 (0.329 sec/batch), lr: 0.030000\n",
            "2020-12-02 00:59:56.348731: step 8400/119245 (epoch 3/35), loss = 0.265762 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:00:06.436200: step 8420/119245 (epoch 3/35), loss = 0.528158 (0.480 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:00:15.608032: step 8440/119245 (epoch 3/35), loss = 0.074767 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:00:26.317830: step 8460/119245 (epoch 3/35), loss = 0.370165 (0.374 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:00:35.644281: step 8480/119245 (epoch 3/35), loss = 0.412682 (0.493 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:00:45.441637: step 8500/119245 (epoch 3/35), loss = 0.455140 (0.437 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:00:54.723036: step 8520/119245 (epoch 3/35), loss = 0.545272 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:01:06.011691: step 8540/119245 (epoch 3/35), loss = 0.045211 (0.401 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:01:16.023733: step 8560/119245 (epoch 3/35), loss = 0.563641 (0.408 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:01:25.980902: step 8580/119245 (epoch 3/35), loss = 0.081448 (0.525 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:01:36.102523: step 8600/119245 (epoch 3/35), loss = 0.058771 (0.605 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:01:45.656779: step 8620/119245 (epoch 3/35), loss = 0.061748 (0.576 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:01:54.968978: step 8640/119245 (epoch 3/35), loss = 0.394110 (0.454 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:02:04.624785: step 8660/119245 (epoch 3/35), loss = 0.582599 (0.715 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:02:16.566853: step 8680/119245 (epoch 3/35), loss = 0.534862 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:02:28.028346: step 8700/119245 (epoch 3/35), loss = 0.191589 (0.767 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:02:36.813909: step 8720/119245 (epoch 3/35), loss = 0.311987 (0.329 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:02:46.646140: step 8740/119245 (epoch 3/35), loss = 0.540255 (0.488 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:02:57.075221: step 8760/119245 (epoch 3/35), loss = 0.214977 (0.374 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:03:08.748131: step 8780/119245 (epoch 3/35), loss = 0.500016 (0.842 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:03:18.761476: step 8800/119245 (epoch 3/35), loss = 0.141724 (0.533 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:03:28.956856: step 8820/119245 (epoch 3/35), loss = 0.109564 (0.610 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:03:38.876480: step 8840/119245 (epoch 3/35), loss = 0.517635 (0.515 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:03:48.882588: step 8860/119245 (epoch 3/35), loss = 0.536731 (0.422 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:03:58.958314: step 8880/119245 (epoch 3/35), loss = 0.143601 (1.664 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:04:08.219516: step 8900/119245 (epoch 3/35), loss = 0.339471 (0.338 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:04:18.079505: step 8920/119245 (epoch 3/35), loss = 0.536901 (0.471 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:04:28.236771: step 8940/119245 (epoch 3/35), loss = 0.061960 (0.440 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:04:37.529490: step 8960/119245 (epoch 3/35), loss = 0.341581 (0.396 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:04:47.563036: step 8980/119245 (epoch 3/35), loss = 0.404507 (0.470 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:04:57.736017: step 9000/119245 (epoch 3/35), loss = 0.055201 (0.594 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:05:08.160836: step 9020/119245 (epoch 3/35), loss = 0.079883 (0.636 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:05:19.168288: step 9040/119245 (epoch 3/35), loss = 0.151036 (0.675 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:05:28.060048: step 9060/119245 (epoch 3/35), loss = 0.129056 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:05:37.999762: step 9080/119245 (epoch 3/35), loss = 0.341444 (0.557 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:05:46.853213: step 9100/119245 (epoch 3/35), loss = 0.344174 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:05:56.678107: step 9120/119245 (epoch 3/35), loss = 0.127944 (0.543 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:06:07.260460: step 9140/119245 (epoch 3/35), loss = 0.203454 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:06:17.153794: step 9160/119245 (epoch 3/35), loss = 0.732357 (0.686 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:06:28.851340: step 9180/119245 (epoch 3/35), loss = 0.635047 (0.539 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:06:39.072961: step 9200/119245 (epoch 3/35), loss = 0.840394 (0.595 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:06:48.530767: step 9220/119245 (epoch 3/35), loss = 0.145855 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:06:57.817840: step 9240/119245 (epoch 3/35), loss = 0.449115 (0.435 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:07:09.044451: step 9260/119245 (epoch 3/35), loss = 0.195614 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:07:18.893879: step 9280/119245 (epoch 3/35), loss = 0.504821 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:07:28.656043: step 9300/119245 (epoch 3/35), loss = 0.124296 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:07:39.544375: step 9320/119245 (epoch 3/35), loss = 0.445345 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:07:48.416652: step 9340/119245 (epoch 3/35), loss = 0.347822 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:07:57.753237: step 9360/119245 (epoch 3/35), loss = 0.151434 (0.827 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:08:07.477748: step 9380/119245 (epoch 3/35), loss = 0.395283 (0.524 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:08:16.968796: step 9400/119245 (epoch 3/35), loss = 0.030627 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:08:26.030350: step 9420/119245 (epoch 3/35), loss = 0.244600 (0.613 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:08:36.830030: step 9440/119245 (epoch 3/35), loss = 0.290745 (0.558 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:08:45.659970: step 9460/119245 (epoch 3/35), loss = 0.095754 (0.351 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:08:54.800335: step 9480/119245 (epoch 3/35), loss = 0.191969 (0.382 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:09:04.379039: step 9500/119245 (epoch 3/35), loss = 0.410240 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:09:14.920221: step 9520/119245 (epoch 3/35), loss = 0.054056 (0.606 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:09:24.794869: step 9540/119245 (epoch 3/35), loss = 0.055901 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:09:34.513137: step 9560/119245 (epoch 3/35), loss = 0.308882 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:09:44.131235: step 9580/119245 (epoch 3/35), loss = 0.584118 (0.401 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:09:53.646296: step 9600/119245 (epoch 3/35), loss = 0.426680 (0.424 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:10:03.522934: step 9620/119245 (epoch 3/35), loss = 0.272153 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:10:13.306507: step 9640/119245 (epoch 3/35), loss = 0.619561 (0.369 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:10:23.234959: step 9660/119245 (epoch 3/35), loss = 0.040482 (0.612 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:10:32.622932: step 9680/119245 (epoch 3/35), loss = 0.678371 (0.517 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:10:41.932253: step 9700/119245 (epoch 3/35), loss = 0.055535 (0.307 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:10:51.676424: step 9720/119245 (epoch 3/35), loss = 0.112910 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:11:00.648177: step 9740/119245 (epoch 3/35), loss = 0.490292 (0.464 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:11:10.075656: step 9760/119245 (epoch 3/35), loss = 0.205740 (0.373 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:11:21.882295: step 9780/119245 (epoch 3/35), loss = 0.408571 (0.336 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:11:30.553983: step 9800/119245 (epoch 3/35), loss = 0.599867 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:11:40.408536: step 9820/119245 (epoch 3/35), loss = 0.522438 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:11:51.163102: step 9840/119245 (epoch 3/35), loss = 0.292739 (0.465 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:12:00.937945: step 9860/119245 (epoch 3/35), loss = 0.289005 (0.623 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:12:10.853820: step 9880/119245 (epoch 3/35), loss = 0.086031 (0.511 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:12:21.001147: step 9900/119245 (epoch 3/35), loss = 0.299650 (0.377 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:12:30.780314: step 9920/119245 (epoch 3/35), loss = 0.655295 (0.316 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:12:41.059469: step 9940/119245 (epoch 3/35), loss = 0.192616 (0.537 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:12:50.467305: step 9960/119245 (epoch 3/35), loss = 0.184836 (0.506 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:12:59.947272: step 9980/119245 (epoch 3/35), loss = 0.179425 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:13:08.577122: step 10000/119245 (epoch 3/35), loss = 0.511125 (0.336 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:13:19.271114: step 10020/119245 (epoch 3/35), loss = 0.325632 (0.393 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:13:28.606199: step 10040/119245 (epoch 3/35), loss = 0.297534 (0.530 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:13:37.449462: step 10060/119245 (epoch 3/35), loss = 0.465984 (0.465 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:13:46.328610: step 10080/119245 (epoch 3/35), loss = 0.140767 (0.434 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:13:57.079951: step 10100/119245 (epoch 3/35), loss = 0.078191 (0.935 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:14:06.037991: step 10120/119245 (epoch 3/35), loss = 0.314299 (0.612 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:14:16.870576: step 10140/119245 (epoch 3/35), loss = 0.265756 (0.336 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:14:26.516439: step 10160/119245 (epoch 3/35), loss = 0.165129 (0.391 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:14:36.495563: step 10180/119245 (epoch 3/35), loss = 0.152339 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:14:46.520501: step 10200/119245 (epoch 3/35), loss = 0.351011 (0.617 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:14:57.336718: step 10220/119245 (epoch 3/35), loss = 0.022969 (0.425 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  80.00%  R:  75.74%  F1:  77.81%  #: 338\n",
            "org:city_of_headquarters             P:  67.86%  R:  34.86%  F1:  46.06%  #: 109\n",
            "org:country_of_headquarters          P:  66.04%  R:  39.55%  F1:  49.47%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  89.66%  R:  68.42%  F1:  77.61%  #: 38\n",
            "org:founded_by                       P:  80.49%  R:  43.42%  F1:  56.41%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  92.31%  R:  28.24%  F1:  43.24%  #: 85\n",
            "org:number_of_employees/members      P: 100.00%  R:  25.93%  F1:  41.18%  #: 27\n",
            "org:parents                          P:  32.26%  R:  31.25%  F1:  31.75%  #: 96\n",
            "org:political/religious_affiliation  P:  23.08%  R:  30.00%  F1:  26.09%  #: 10\n",
            "org:shareholders                     P: 100.00%  R:   7.27%  F1:  13.56%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  58.23%  R:  65.71%  F1:  61.74%  #: 70\n",
            "org:subsidiaries                     P:  40.00%  R:   8.85%  F1:  14.49%  #: 113\n",
            "org:top_members/employees            P:  72.73%  R:  82.40%  F1:  77.26%  #: 534\n",
            "org:website                          P:  89.25%  R:  96.51%  F1:  92.74%  #: 86\n",
            "per:age                              P:  93.95%  R:  83.13%  F1:  88.21%  #: 243\n",
            "per:alternate_names                  P:  73.68%  R:  36.84%  F1:  49.12%  #: 38\n",
            "per:cause_of_death                   P:  92.45%  R:  58.33%  F1:  71.53%  #: 168\n",
            "per:charges                          P:  83.93%  R:  44.76%  F1:  58.39%  #: 105\n",
            "per:children                         P:  57.63%  R:  34.34%  F1:  43.04%  #: 99\n",
            "per:cities_of_residence              P:  41.46%  R:  37.99%  F1:  39.65%  #: 179\n",
            "per:city_of_birth                    P:  76.92%  R:  60.61%  F1:  67.80%  #: 33\n",
            "per:city_of_death                    P:  74.12%  R:  53.39%  F1:  62.07%  #: 118\n",
            "per:countries_of_residence           P:  38.30%  R:  39.82%  F1:  39.05%  #: 226\n",
            "per:country_of_birth                 P:  60.00%  R:  15.00%  F1:  24.00%  #: 20\n",
            "per:country_of_death                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 46\n",
            "per:date_of_birth                    P:  96.55%  R:  90.32%  F1:  93.33%  #: 31\n",
            "per:date_of_death                    P:  77.01%  R:  65.05%  F1:  70.53%  #: 206\n",
            "per:employee_of                      P:  67.00%  R:  53.60%  F1:  59.56%  #: 375\n",
            "per:origin                           P:  72.19%  R:  51.90%  F1:  60.39%  #: 210\n",
            "per:other_family                     P:   0.00%  R:   0.00%  F1:   0.00%  #: 80\n",
            "per:parents                          P:  31.68%  R:  57.14%  F1:  40.76%  #: 56\n",
            "per:religion                         P:  70.00%  R:  52.83%  F1:  60.22%  #: 53\n",
            "per:schools_attended                 P:  90.00%  R:  54.00%  F1:  67.50%  #: 50\n",
            "per:siblings                         P:  56.25%  R:  60.00%  F1:  58.06%  #: 30\n",
            "per:spouse                           P:  70.81%  R:  71.70%  F1:  71.25%  #: 159\n",
            "per:stateorprovince_of_birth         P:  66.67%  R:  61.54%  F1:  64.00%  #: 26\n",
            "per:stateorprovince_of_death         P: 100.00%  R:   9.76%  F1:  17.78%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  36.70%  R:  55.56%  F1:  44.20%  #: 72\n",
            "per:title                            P:  77.80%  R:  85.42%  F1:  81.43%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17998 Guess as no relation\n",
            "4633 guess\n",
            "3245 correct\n",
            "5436 gold\n",
            "Precision (micro): 70.041%\n",
            "   Recall (micro): 59.695%\n",
            "       F1 (micro): 64.455%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 3: train_loss = 0.319256, dev_loss = 0.468560, dev_f1 = 0.6446\n",
            "model saved to ./saved_models/00/checkpoint_epoch_3.pt\n",
            "new best model saved.\n",
            "\n",
            "2020-12-02 01:17:58.403888: step 10240/119245 (epoch 4/35), loss = 0.531266 (0.384 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:18:09.805582: step 10260/119245 (epoch 4/35), loss = 0.426813 (0.492 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:18:18.951542: step 10280/119245 (epoch 4/35), loss = 0.896571 (0.365 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:18:29.712955: step 10300/119245 (epoch 4/35), loss = 0.107574 (0.336 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:18:38.699615: step 10320/119245 (epoch 4/35), loss = 0.073629 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:18:49.134186: step 10340/119245 (epoch 4/35), loss = 0.509181 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:18:58.620070: step 10360/119245 (epoch 4/35), loss = 0.068755 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:19:07.141753: step 10380/119245 (epoch 4/35), loss = 0.045722 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:19:16.433396: step 10400/119245 (epoch 4/35), loss = 0.138009 (0.337 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:19:26.698252: step 10420/119245 (epoch 4/35), loss = 0.191830 (0.545 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:19:36.264576: step 10440/119245 (epoch 4/35), loss = 0.051493 (0.425 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:19:45.203666: step 10460/119245 (epoch 4/35), loss = 0.162921 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:19:55.101503: step 10480/119245 (epoch 4/35), loss = 0.451192 (0.396 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:20:05.468853: step 10500/119245 (epoch 4/35), loss = 0.178976 (0.317 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:20:14.062571: step 10520/119245 (epoch 4/35), loss = 0.281181 (0.516 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:20:24.169521: step 10540/119245 (epoch 4/35), loss = 0.013572 (0.520 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:20:34.514980: step 10560/119245 (epoch 4/35), loss = 0.290866 (0.461 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:20:46.054881: step 10580/119245 (epoch 4/35), loss = 0.829667 (0.391 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:20:55.094509: step 10600/119245 (epoch 4/35), loss = 0.065742 (0.337 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:21:05.181583: step 10620/119245 (epoch 4/35), loss = 0.134522 (1.332 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:21:15.365715: step 10640/119245 (epoch 4/35), loss = 0.155323 (0.338 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:21:24.602643: step 10660/119245 (epoch 4/35), loss = 0.163426 (0.470 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:21:37.192769: step 10680/119245 (epoch 4/35), loss = 0.062709 (0.509 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:21:46.440062: step 10700/119245 (epoch 4/35), loss = 0.257514 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:21:56.548944: step 10720/119245 (epoch 4/35), loss = 0.302669 (0.370 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:22:07.086841: step 10740/119245 (epoch 4/35), loss = 0.520867 (0.519 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:22:18.093225: step 10760/119245 (epoch 4/35), loss = 0.227014 (1.004 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:22:26.782660: step 10780/119245 (epoch 4/35), loss = 0.597315 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:22:36.707346: step 10800/119245 (epoch 4/35), loss = 0.336825 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:22:45.382370: step 10820/119245 (epoch 4/35), loss = 0.177711 (0.320 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:22:54.304995: step 10840/119245 (epoch 4/35), loss = 0.598855 (0.378 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:23:03.790056: step 10860/119245 (epoch 4/35), loss = 0.378099 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:23:13.361244: step 10880/119245 (epoch 4/35), loss = 0.078731 (0.499 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:23:24.699425: step 10900/119245 (epoch 4/35), loss = 0.365817 (0.519 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:23:34.575256: step 10920/119245 (epoch 4/35), loss = 0.308787 (0.355 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:23:44.621570: step 10940/119245 (epoch 4/35), loss = 0.736356 (0.466 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:23:54.864292: step 10960/119245 (epoch 4/35), loss = 0.220252 (0.621 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:24:04.150879: step 10980/119245 (epoch 4/35), loss = 0.307735 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:24:14.196130: step 11000/119245 (epoch 4/35), loss = 0.467911 (0.519 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:24:23.376420: step 11020/119245 (epoch 4/35), loss = 0.473199 (0.588 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:24:33.250978: step 11040/119245 (epoch 4/35), loss = 0.080026 (0.482 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:24:42.569857: step 11060/119245 (epoch 4/35), loss = 0.037649 (0.395 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:24:51.749471: step 11080/119245 (epoch 4/35), loss = 0.153164 (0.361 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:25:03.302082: step 11100/119245 (epoch 4/35), loss = 0.251178 (0.428 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:25:14.587862: step 11120/119245 (epoch 4/35), loss = 0.680973 (0.549 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:25:24.267793: step 11140/119245 (epoch 4/35), loss = 0.178633 (0.429 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:25:33.115430: step 11160/119245 (epoch 4/35), loss = 0.029247 (0.377 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:25:41.994764: step 11180/119245 (epoch 4/35), loss = 0.391800 (0.408 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:25:50.443492: step 11200/119245 (epoch 4/35), loss = 0.110903 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:25:59.534847: step 11220/119245 (epoch 4/35), loss = 0.289286 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:26:08.368837: step 11240/119245 (epoch 4/35), loss = 0.340876 (0.628 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:26:17.533126: step 11260/119245 (epoch 4/35), loss = 0.037004 (0.423 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:26:27.693329: step 11280/119245 (epoch 4/35), loss = 0.023056 (0.418 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:26:38.161120: step 11300/119245 (epoch 4/35), loss = 0.169466 (0.611 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:26:50.855768: step 11320/119245 (epoch 4/35), loss = 0.532054 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:26:59.714513: step 11340/119245 (epoch 4/35), loss = 0.523203 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:27:09.403550: step 11360/119245 (epoch 4/35), loss = 0.258928 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:27:20.365311: step 11380/119245 (epoch 4/35), loss = 0.272230 (0.531 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:27:30.033852: step 11400/119245 (epoch 4/35), loss = 0.695679 (0.625 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:27:39.790255: step 11420/119245 (epoch 4/35), loss = 0.143445 (0.512 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:27:48.982403: step 11440/119245 (epoch 4/35), loss = 0.039675 (0.624 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:27:58.152998: step 11460/119245 (epoch 4/35), loss = 0.025242 (0.382 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:28:07.517180: step 11480/119245 (epoch 4/35), loss = 0.429580 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:28:18.869500: step 11500/119245 (epoch 4/35), loss = 0.426727 (1.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:28:32.377222: step 11520/119245 (epoch 4/35), loss = 0.301087 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:28:42.185747: step 11540/119245 (epoch 4/35), loss = 0.406170 (0.464 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:28:52.277591: step 11560/119245 (epoch 4/35), loss = 0.371780 (0.544 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:29:01.681461: step 11580/119245 (epoch 4/35), loss = 0.672038 (0.393 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:29:11.273075: step 11600/119245 (epoch 4/35), loss = 0.387161 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:29:20.395459: step 11620/119245 (epoch 4/35), loss = 0.432829 (0.440 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:29:30.255754: step 11640/119245 (epoch 4/35), loss = 0.163266 (0.341 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:29:40.164777: step 11660/119245 (epoch 4/35), loss = 0.248713 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:29:50.313080: step 11680/119245 (epoch 4/35), loss = 0.260752 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:29:59.934086: step 11700/119245 (epoch 4/35), loss = 0.349819 (0.406 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:30:08.679997: step 11720/119245 (epoch 4/35), loss = 0.341615 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:30:18.248781: step 11740/119245 (epoch 4/35), loss = 0.092017 (0.624 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:30:29.451927: step 11760/119245 (epoch 4/35), loss = 0.161334 (0.550 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:30:38.552789: step 11780/119245 (epoch 4/35), loss = 0.353883 (0.625 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:30:48.635380: step 11800/119245 (epoch 4/35), loss = 0.464820 (0.503 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:30:58.098025: step 11820/119245 (epoch 4/35), loss = 0.428666 (0.410 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:31:08.258161: step 11840/119245 (epoch 4/35), loss = 0.158475 (0.460 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:31:17.986793: step 11860/119245 (epoch 4/35), loss = 0.083758 (0.338 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:31:27.917350: step 11880/119245 (epoch 4/35), loss = 0.488870 (0.439 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:31:37.719170: step 11900/119245 (epoch 4/35), loss = 0.058660 (0.496 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:31:46.852414: step 11920/119245 (epoch 4/35), loss = 0.095618 (0.466 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:31:58.262497: step 11940/119245 (epoch 4/35), loss = 0.355194 (0.758 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:32:08.630361: step 11960/119245 (epoch 4/35), loss = 0.277226 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:32:18.557963: step 11980/119245 (epoch 4/35), loss = 0.022487 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:32:27.564042: step 12000/119245 (epoch 4/35), loss = 0.305558 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:32:38.039662: step 12020/119245 (epoch 4/35), loss = 0.337593 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:32:47.320150: step 12040/119245 (epoch 4/35), loss = 0.345015 (0.478 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:32:56.904518: step 12060/119245 (epoch 4/35), loss = 0.581358 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:33:08.619455: step 12080/119245 (epoch 4/35), loss = 0.176911 (0.349 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:33:18.288001: step 12100/119245 (epoch 4/35), loss = 0.055304 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:33:29.480190: step 12120/119245 (epoch 4/35), loss = 0.751281 (0.650 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:33:38.466251: step 12140/119245 (epoch 4/35), loss = 0.209180 (0.420 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:33:49.337534: step 12160/119245 (epoch 4/35), loss = 0.182693 (0.540 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:34:00.065715: step 12180/119245 (epoch 4/35), loss = 0.016625 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:34:10.911515: step 12200/119245 (epoch 4/35), loss = 0.287810 (0.327 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:34:21.109147: step 12220/119245 (epoch 4/35), loss = 0.644101 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:34:31.176015: step 12240/119245 (epoch 4/35), loss = 0.254066 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:34:41.529151: step 12260/119245 (epoch 4/35), loss = 0.569453 (1.657 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:34:50.421469: step 12280/119245 (epoch 4/35), loss = 0.349303 (0.544 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:35:00.707291: step 12300/119245 (epoch 4/35), loss = 0.427738 (0.638 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:35:10.216651: step 12320/119245 (epoch 4/35), loss = 0.573185 (0.546 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:35:20.597085: step 12340/119245 (epoch 4/35), loss = 0.484931 (0.486 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:35:29.759383: step 12360/119245 (epoch 4/35), loss = 0.262312 (0.444 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:35:39.815583: step 12380/119245 (epoch 4/35), loss = 0.071664 (0.404 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:35:49.788019: step 12400/119245 (epoch 4/35), loss = 0.027980 (0.478 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:36:00.109502: step 12420/119245 (epoch 4/35), loss = 0.149036 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:36:10.802754: step 12440/119245 (epoch 4/35), loss = 0.159650 (0.608 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:36:20.577177: step 12460/119245 (epoch 4/35), loss = 0.175359 (0.596 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:36:29.669335: step 12480/119245 (epoch 4/35), loss = 0.232451 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:36:39.279135: step 12500/119245 (epoch 4/35), loss = 0.081957 (0.379 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:36:49.162334: step 12520/119245 (epoch 4/35), loss = 0.430079 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:36:58.409763: step 12540/119245 (epoch 4/35), loss = 0.117588 (0.531 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:37:09.366226: step 12560/119245 (epoch 4/35), loss = 0.122614 (0.608 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:37:20.969014: step 12580/119245 (epoch 4/35), loss = 0.244895 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:37:31.478153: step 12600/119245 (epoch 4/35), loss = 0.183889 (0.606 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:37:40.881485: step 12620/119245 (epoch 4/35), loss = 0.087105 (0.374 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:37:49.936008: step 12640/119245 (epoch 4/35), loss = 0.299350 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:38:00.803785: step 12660/119245 (epoch 4/35), loss = 0.522600 (0.486 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:38:10.553735: step 12680/119245 (epoch 4/35), loss = 0.248917 (0.378 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:38:20.858838: step 12700/119245 (epoch 4/35), loss = 0.212286 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:38:30.783239: step 12720/119245 (epoch 4/35), loss = 0.085562 (0.526 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:38:40.322057: step 12740/119245 (epoch 4/35), loss = 1.015804 (0.422 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:38:49.337868: step 12760/119245 (epoch 4/35), loss = 0.358421 (0.376 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:38:59.103055: step 12780/119245 (epoch 4/35), loss = 0.207567 (0.485 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:39:09.302893: step 12800/119245 (epoch 4/35), loss = 0.182057 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:39:17.736376: step 12820/119245 (epoch 4/35), loss = 0.079772 (0.386 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:39:27.731083: step 12840/119245 (epoch 4/35), loss = 0.306990 (0.596 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:39:37.801939: step 12860/119245 (epoch 4/35), loss = 0.106432 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:39:46.964814: step 12880/119245 (epoch 4/35), loss = 0.063424 (0.616 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:39:55.425248: step 12900/119245 (epoch 4/35), loss = 0.287936 (0.473 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:40:05.149029: step 12920/119245 (epoch 4/35), loss = 0.335165 (0.368 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:40:15.793839: step 12940/119245 (epoch 4/35), loss = 0.572990 (0.475 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:40:25.658634: step 12960/119245 (epoch 4/35), loss = 0.287006 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:40:36.188607: step 12980/119245 (epoch 4/35), loss = 0.061635 (0.508 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:40:45.392419: step 13000/119245 (epoch 4/35), loss = 0.044306 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:40:55.220890: step 13020/119245 (epoch 4/35), loss = 0.524714 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:41:04.856965: step 13040/119245 (epoch 4/35), loss = 0.759596 (0.624 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:41:14.379653: step 13060/119245 (epoch 4/35), loss = 0.145713 (0.557 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:41:24.652334: step 13080/119245 (epoch 4/35), loss = 0.340940 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:41:34.077379: step 13100/119245 (epoch 4/35), loss = 0.027796 (0.485 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:41:43.405475: step 13120/119245 (epoch 4/35), loss = 0.056130 (0.508 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:41:52.771550: step 13140/119245 (epoch 4/35), loss = 0.129825 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:42:01.465125: step 13160/119245 (epoch 4/35), loss = 0.136404 (0.446 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:42:12.951375: step 13180/119245 (epoch 4/35), loss = 0.138393 (0.493 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:42:22.428069: step 13200/119245 (epoch 4/35), loss = 0.277853 (0.541 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:42:32.054056: step 13220/119245 (epoch 4/35), loss = 0.078999 (0.338 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:42:42.644363: step 13240/119245 (epoch 4/35), loss = 0.223391 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:42:52.180395: step 13260/119245 (epoch 4/35), loss = 0.121094 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:43:01.478814: step 13280/119245 (epoch 4/35), loss = 0.580516 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:43:12.534012: step 13300/119245 (epoch 4/35), loss = 0.333459 (0.623 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:43:22.185502: step 13320/119245 (epoch 4/35), loss = 0.341943 (0.508 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:43:32.473474: step 13340/119245 (epoch 4/35), loss = 0.151189 (0.430 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:43:41.983309: step 13360/119245 (epoch 4/35), loss = 0.167744 (0.502 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:43:51.588121: step 13380/119245 (epoch 4/35), loss = 0.300091 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:43:59.700520: step 13400/119245 (epoch 4/35), loss = 0.411369 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:44:10.599011: step 13420/119245 (epoch 4/35), loss = 0.066289 (1.534 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:44:19.794155: step 13440/119245 (epoch 4/35), loss = 0.840023 (0.410 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:44:28.920020: step 13460/119245 (epoch 4/35), loss = 0.249290 (0.355 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:44:37.359792: step 13480/119245 (epoch 4/35), loss = 0.143109 (0.473 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:44:47.314404: step 13500/119245 (epoch 4/35), loss = 0.461334 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:44:57.101638: step 13520/119245 (epoch 4/35), loss = 0.138433 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:45:07.454257: step 13540/119245 (epoch 4/35), loss = 0.064779 (0.349 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:45:17.858301: step 13560/119245 (epoch 4/35), loss = 0.452641 (1.013 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:45:27.271767: step 13580/119245 (epoch 4/35), loss = 0.310428 (0.470 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:45:37.174589: step 13600/119245 (epoch 4/35), loss = 0.181978 (0.371 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:45:48.450128: step 13620/119245 (epoch 4/35), loss = 0.058835 (0.582 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  80.12%  R:  79.88%  F1:  80.00%  #: 338\n",
            "org:city_of_headquarters             P:  64.71%  R:  50.46%  F1:  56.70%  #: 109\n",
            "org:country_of_headquarters          P:  50.84%  R:  68.36%  F1:  58.31%  #: 177\n",
            "org:dissolved                        P:  10.00%  R:  12.50%  F1:  11.11%  #: 8\n",
            "org:founded                          P:  85.29%  R:  76.32%  F1:  80.56%  #: 38\n",
            "org:founded_by                       P:  76.60%  R:  47.37%  F1:  58.54%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  81.97%  R:  58.82%  F1:  68.49%  #: 85\n",
            "org:number_of_employees/members      P:  81.25%  R:  48.15%  F1:  60.47%  #: 27\n",
            "org:parents                          P:  48.78%  R:  20.83%  F1:  29.20%  #: 96\n",
            "org:political/religious_affiliation  P:  25.00%  R:  40.00%  F1:  30.77%  #: 10\n",
            "org:shareholders                     P:  32.61%  R:  27.27%  F1:  29.70%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  47.27%  R:  74.29%  F1:  57.78%  #: 70\n",
            "org:subsidiaries                     P:  49.06%  R:  23.01%  F1:  31.33%  #: 113\n",
            "org:top_members/employees            P:  72.84%  R:  77.34%  F1:  75.02%  #: 534\n",
            "org:website                          P:  92.05%  R:  94.19%  F1:  93.10%  #: 86\n",
            "per:age                              P:  90.21%  R:  87.24%  F1:  88.70%  #: 243\n",
            "per:alternate_names                  P:  80.00%  R:  42.11%  F1:  55.17%  #: 38\n",
            "per:cause_of_death                   P:  82.50%  R:  78.57%  F1:  80.49%  #: 168\n",
            "per:charges                          P:  78.22%  R:  75.24%  F1:  76.70%  #: 105\n",
            "per:children                         P:  63.04%  R:  29.29%  F1:  40.00%  #: 99\n",
            "per:cities_of_residence              P:  41.35%  R:  54.75%  F1:  47.12%  #: 179\n",
            "per:city_of_birth                    P:  70.97%  R:  66.67%  F1:  68.75%  #: 33\n",
            "per:city_of_death                    P:  66.93%  R:  72.03%  F1:  69.39%  #: 118\n",
            "per:countries_of_residence           P:  33.76%  R:  58.85%  F1:  42.90%  #: 226\n",
            "per:country_of_birth                 P:  75.00%  R:  30.00%  F1:  42.86%  #: 20\n",
            "per:country_of_death                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 46\n",
            "per:date_of_birth                    P:  87.50%  R:  90.32%  F1:  88.89%  #: 31\n",
            "per:date_of_death                    P:  73.85%  R:  78.16%  F1:  75.94%  #: 206\n",
            "per:employee_of                      P:  58.99%  R:  62.13%  F1:  60.52%  #: 375\n",
            "per:origin                           P:  67.18%  R:  41.90%  F1:  51.61%  #: 210\n",
            "per:other_family                     P:  20.00%  R:   1.25%  F1:   2.35%  #: 80\n",
            "per:parents                          P:  24.20%  R:  67.86%  F1:  35.68%  #: 56\n",
            "per:religion                         P:  60.38%  R:  60.38%  F1:  60.38%  #: 53\n",
            "per:schools_attended                 P:  80.95%  R:  68.00%  F1:  73.91%  #: 50\n",
            "per:siblings                         P:  51.43%  R:  60.00%  F1:  55.38%  #: 30\n",
            "per:spouse                           P:  68.48%  R:  71.07%  F1:  69.75%  #: 159\n",
            "per:stateorprovince_of_birth         P:  70.37%  R:  73.08%  F1:  71.70%  #: 26\n",
            "per:stateorprovince_of_death         P:  71.43%  R:  12.20%  F1:  20.83%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  36.64%  R:  66.67%  F1:  47.29%  #: 72\n",
            "per:title                            P:  74.51%  R:  87.49%  F1:  80.48%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17046 Guess as no relation\n",
            "5585 guess\n",
            "3620 correct\n",
            "5436 gold\n",
            "Precision (micro): 64.816%\n",
            "   Recall (micro): 66.593%\n",
            "       F1 (micro): 65.693%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 4: train_loss = 0.267761, dev_loss = 0.482281, dev_f1 = 0.6569\n",
            "model saved to ./saved_models/00/checkpoint_epoch_4.pt\n",
            "new best model saved.\n",
            "\n",
            "2020-12-02 01:48:48.900867: step 13640/119245 (epoch 5/35), loss = 0.372879 (0.448 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:49:00.490223: step 13660/119245 (epoch 5/35), loss = 0.313434 (0.496 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:49:10.424352: step 13680/119245 (epoch 5/35), loss = 0.153253 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:49:19.576610: step 13700/119245 (epoch 5/35), loss = 0.144984 (0.523 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:49:29.506750: step 13720/119245 (epoch 5/35), loss = 0.031755 (0.572 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:49:39.345752: step 13740/119245 (epoch 5/35), loss = 0.179190 (0.466 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:49:49.471718: step 13760/119245 (epoch 5/35), loss = 0.059934 (0.573 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:49:58.357490: step 13780/119245 (epoch 5/35), loss = 0.387724 (0.512 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:50:07.229790: step 13800/119245 (epoch 5/35), loss = 0.065524 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:50:17.575490: step 13820/119245 (epoch 5/35), loss = 0.208181 (0.622 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:50:27.312903: step 13840/119245 (epoch 5/35), loss = 0.224052 (0.342 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:50:36.492128: step 13860/119245 (epoch 5/35), loss = 0.208420 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:50:46.214393: step 13880/119245 (epoch 5/35), loss = 0.288063 (0.412 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:50:56.830875: step 13900/119245 (epoch 5/35), loss = 0.837832 (0.382 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:51:04.951513: step 13920/119245 (epoch 5/35), loss = 0.029472 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:51:14.370994: step 13940/119245 (epoch 5/35), loss = 0.291476 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:51:25.320835: step 13960/119245 (epoch 5/35), loss = 0.286548 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:51:34.817165: step 13980/119245 (epoch 5/35), loss = 0.228694 (0.456 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:51:46.117207: step 14000/119245 (epoch 5/35), loss = 0.149088 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:51:55.038557: step 14020/119245 (epoch 5/35), loss = 0.611732 (0.343 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:52:06.142851: step 14040/119245 (epoch 5/35), loss = 0.243185 (0.650 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:52:15.194414: step 14060/119245 (epoch 5/35), loss = 0.117572 (0.658 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:52:27.549323: step 14080/119245 (epoch 5/35), loss = 0.239398 (0.344 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:52:37.237135: step 14100/119245 (epoch 5/35), loss = 0.028583 (0.618 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:52:47.284083: step 14120/119245 (epoch 5/35), loss = 0.114347 (0.474 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:52:57.770714: step 14140/119245 (epoch 5/35), loss = 0.040179 (0.473 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:53:07.225691: step 14160/119245 (epoch 5/35), loss = 0.054304 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:53:17.804221: step 14180/119245 (epoch 5/35), loss = 0.133137 (0.327 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:53:27.096509: step 14200/119245 (epoch 5/35), loss = 0.088172 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:53:36.120307: step 14220/119245 (epoch 5/35), loss = 0.244645 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:53:44.915810: step 14240/119245 (epoch 5/35), loss = 0.092647 (0.312 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:53:54.492000: step 14260/119245 (epoch 5/35), loss = 0.050775 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:54:04.348401: step 14280/119245 (epoch 5/35), loss = 0.742934 (0.516 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:54:15.445122: step 14300/119245 (epoch 5/35), loss = 0.220803 (0.460 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:54:25.249111: step 14320/119245 (epoch 5/35), loss = 0.017199 (0.401 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:54:35.279694: step 14340/119245 (epoch 5/35), loss = 0.821751 (0.406 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:54:45.183844: step 14360/119245 (epoch 5/35), loss = 0.019268 (0.463 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:54:54.187833: step 14380/119245 (epoch 5/35), loss = 0.469020 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:55:03.958885: step 14400/119245 (epoch 5/35), loss = 0.240879 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:55:13.397782: step 14420/119245 (epoch 5/35), loss = 0.128985 (0.370 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:55:23.414298: step 14440/119245 (epoch 5/35), loss = 0.188358 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:55:32.783500: step 14460/119245 (epoch 5/35), loss = 0.395733 (0.545 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:55:41.891864: step 14480/119245 (epoch 5/35), loss = 0.049391 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:55:53.247424: step 14500/119245 (epoch 5/35), loss = 0.294014 (0.691 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:56:04.098121: step 14520/119245 (epoch 5/35), loss = 0.103077 (0.396 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:56:14.497568: step 14540/119245 (epoch 5/35), loss = 0.095953 (0.402 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:56:23.383604: step 14560/119245 (epoch 5/35), loss = 0.314953 (0.448 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:56:32.536783: step 14580/119245 (epoch 5/35), loss = 0.037619 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:56:41.221201: step 14600/119245 (epoch 5/35), loss = 0.794528 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:56:49.751161: step 14620/119245 (epoch 5/35), loss = 0.594278 (0.343 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:56:58.321000: step 14640/119245 (epoch 5/35), loss = 0.429794 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:57:07.546564: step 14660/119245 (epoch 5/35), loss = 0.231829 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:57:18.342469: step 14680/119245 (epoch 5/35), loss = 0.042668 (0.449 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:57:28.293467: step 14700/119245 (epoch 5/35), loss = 0.032939 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:57:40.833249: step 14720/119245 (epoch 5/35), loss = 0.176394 (0.510 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:57:50.053278: step 14740/119245 (epoch 5/35), loss = 0.365265 (0.360 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:57:59.142432: step 14760/119245 (epoch 5/35), loss = 0.106438 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:58:10.246974: step 14780/119245 (epoch 5/35), loss = 0.079190 (0.544 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:58:19.694718: step 14800/119245 (epoch 5/35), loss = 0.234449 (0.532 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:58:29.408307: step 14820/119245 (epoch 5/35), loss = 0.531800 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:58:39.307601: step 14840/119245 (epoch 5/35), loss = 0.415526 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:58:48.770383: step 14860/119245 (epoch 5/35), loss = 0.145737 (0.438 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:58:57.935753: step 14880/119245 (epoch 5/35), loss = 0.025502 (0.418 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:59:08.051627: step 14900/119245 (epoch 5/35), loss = 0.230028 (0.348 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:59:21.757226: step 14920/119245 (epoch 5/35), loss = 0.132981 (1.562 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:59:31.876805: step 14940/119245 (epoch 5/35), loss = 0.470939 (0.401 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:59:42.396890: step 14960/119245 (epoch 5/35), loss = 0.468583 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 01:59:52.031599: step 14980/119245 (epoch 5/35), loss = 0.071008 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:00:01.329716: step 15000/119245 (epoch 5/35), loss = 0.372033 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:00:10.675755: step 15020/119245 (epoch 5/35), loss = 0.437056 (0.464 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:00:20.320090: step 15040/119245 (epoch 5/35), loss = 0.220132 (0.516 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:00:30.149019: step 15060/119245 (epoch 5/35), loss = 0.095816 (0.610 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:00:39.665142: step 15080/119245 (epoch 5/35), loss = 0.031136 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:00:49.763727: step 15100/119245 (epoch 5/35), loss = 0.231380 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:00:58.897701: step 15120/119245 (epoch 5/35), loss = 0.599984 (0.348 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:01:08.618196: step 15140/119245 (epoch 5/35), loss = 0.356565 (0.446 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:01:18.252377: step 15160/119245 (epoch 5/35), loss = 0.705314 (0.410 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:01:28.432033: step 15180/119245 (epoch 5/35), loss = 0.332675 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:01:38.113522: step 15200/119245 (epoch 5/35), loss = 0.416605 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:01:48.043474: step 15220/119245 (epoch 5/35), loss = 0.057717 (0.578 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:01:57.662736: step 15240/119245 (epoch 5/35), loss = 0.033385 (0.440 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:02:07.792718: step 15260/119245 (epoch 5/35), loss = 0.116083 (0.480 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:02:17.733502: step 15280/119245 (epoch 5/35), loss = 0.202814 (0.429 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:02:27.511644: step 15300/119245 (epoch 5/35), loss = 0.037932 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:02:36.716171: step 15320/119245 (epoch 5/35), loss = 0.511733 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:02:46.406281: step 15340/119245 (epoch 5/35), loss = 0.212672 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:02:57.588810: step 15360/119245 (epoch 5/35), loss = 0.336348 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:03:07.683452: step 15380/119245 (epoch 5/35), loss = 0.217915 (0.448 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:03:17.119535: step 15400/119245 (epoch 5/35), loss = 0.430418 (0.425 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:03:27.372339: step 15420/119245 (epoch 5/35), loss = 0.092504 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:03:36.375856: step 15440/119245 (epoch 5/35), loss = 0.131262 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:03:46.279975: step 15460/119245 (epoch 5/35), loss = 0.283076 (0.504 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:03:55.686335: step 15480/119245 (epoch 5/35), loss = 0.033190 (0.449 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:04:07.584137: step 15500/119245 (epoch 5/35), loss = 0.059592 (0.615 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:04:18.829882: step 15520/119245 (epoch 5/35), loss = 0.141763 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:04:28.114850: step 15540/119245 (epoch 5/35), loss = 0.340006 (0.407 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:04:37.834108: step 15560/119245 (epoch 5/35), loss = 0.520811 (0.374 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:04:48.748434: step 15580/119245 (epoch 5/35), loss = 0.299385 (0.548 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:04:59.928360: step 15600/119245 (epoch 5/35), loss = 0.057510 (0.391 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:05:09.523441: step 15620/119245 (epoch 5/35), loss = 0.308025 (0.438 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:05:19.800836: step 15640/119245 (epoch 5/35), loss = 0.189541 (0.484 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:05:29.076559: step 15660/119245 (epoch 5/35), loss = 0.074749 (0.341 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:05:39.409267: step 15680/119245 (epoch 5/35), loss = 0.282598 (0.524 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:05:49.641068: step 15700/119245 (epoch 5/35), loss = 0.215129 (0.661 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:05:59.050404: step 15720/119245 (epoch 5/35), loss = 0.525844 (0.631 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:06:09.178199: step 15740/119245 (epoch 5/35), loss = 0.360513 (0.521 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:06:18.243176: step 15760/119245 (epoch 5/35), loss = 0.503959 (0.509 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:06:28.335824: step 15780/119245 (epoch 5/35), loss = 0.335369 (1.029 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:06:38.949724: step 15800/119245 (epoch 5/35), loss = 0.409502 (0.425 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:06:48.432452: step 15820/119245 (epoch 5/35), loss = 0.339761 (0.480 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:06:58.740220: step 15840/119245 (epoch 5/35), loss = 0.408299 (0.444 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:07:09.267030: step 15860/119245 (epoch 5/35), loss = 0.008035 (0.329 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:07:18.066474: step 15880/119245 (epoch 5/35), loss = 0.414229 (0.496 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:07:27.962336: step 15900/119245 (epoch 5/35), loss = 0.193095 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:07:37.230128: step 15920/119245 (epoch 5/35), loss = 0.019775 (0.642 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:07:47.231909: step 15940/119245 (epoch 5/35), loss = 0.024000 (0.393 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:07:57.973561: step 15960/119245 (epoch 5/35), loss = 0.263338 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:08:08.754042: step 15980/119245 (epoch 5/35), loss = 0.020796 (0.331 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:08:19.308256: step 16000/119245 (epoch 5/35), loss = 0.115853 (0.954 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:08:29.281488: step 16020/119245 (epoch 5/35), loss = 0.083830 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:08:38.655909: step 16040/119245 (epoch 5/35), loss = 0.153690 (0.493 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:08:48.613651: step 16060/119245 (epoch 5/35), loss = 0.285308 (0.791 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:08:59.325500: step 16080/119245 (epoch 5/35), loss = 0.088577 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:09:09.107490: step 16100/119245 (epoch 5/35), loss = 0.058096 (0.441 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:09:18.372218: step 16120/119245 (epoch 5/35), loss = 0.282327 (0.509 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:09:28.897756: step 16140/119245 (epoch 5/35), loss = 0.191457 (0.486 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:09:38.144865: step 16160/119245 (epoch 5/35), loss = 0.012013 (0.470 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:09:47.372159: step 16180/119245 (epoch 5/35), loss = 0.041726 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:09:57.971714: step 16200/119245 (epoch 5/35), loss = 0.093560 (0.533 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:10:06.405636: step 16220/119245 (epoch 5/35), loss = 0.030327 (0.503 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:10:15.913847: step 16240/119245 (epoch 5/35), loss = 0.028236 (1.023 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:10:26.217260: step 16260/119245 (epoch 5/35), loss = 0.016133 (0.545 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:10:34.693863: step 16280/119245 (epoch 5/35), loss = 0.136635 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:10:44.043240: step 16300/119245 (epoch 5/35), loss = 0.151226 (0.371 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:10:53.689812: step 16320/119245 (epoch 5/35), loss = 0.318194 (0.445 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:11:04.055475: step 16340/119245 (epoch 5/35), loss = 0.211154 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:11:14.199055: step 16360/119245 (epoch 5/35), loss = 0.398651 (0.524 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:11:24.676411: step 16380/119245 (epoch 5/35), loss = 0.063223 (0.320 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:11:33.534534: step 16400/119245 (epoch 5/35), loss = 1.163590 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:11:43.144235: step 16420/119245 (epoch 5/35), loss = 0.014365 (0.343 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:11:52.960868: step 16440/119245 (epoch 5/35), loss = 0.159838 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:12:02.582531: step 16460/119245 (epoch 5/35), loss = 0.063690 (0.452 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:12:12.834280: step 16480/119245 (epoch 5/35), loss = 0.043746 (0.434 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:12:21.993960: step 16500/119245 (epoch 5/35), loss = 0.417471 (0.432 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:12:31.686883: step 16520/119245 (epoch 5/35), loss = 0.138398 (0.788 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:12:40.296311: step 16540/119245 (epoch 5/35), loss = 0.057887 (0.470 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:12:49.811735: step 16560/119245 (epoch 5/35), loss = 0.172818 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:12:59.337274: step 16580/119245 (epoch 5/35), loss = 0.225179 (0.544 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:13:10.591865: step 16600/119245 (epoch 5/35), loss = 0.142296 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:13:19.706463: step 16620/119245 (epoch 5/35), loss = 0.092590 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:13:30.675580: step 16640/119245 (epoch 5/35), loss = 0.171585 (0.495 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:13:40.117621: step 16660/119245 (epoch 5/35), loss = 0.313361 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:13:49.748995: step 16680/119245 (epoch 5/35), loss = 0.066740 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:14:00.648661: step 16700/119245 (epoch 5/35), loss = 0.076766 (0.467 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:14:09.792671: step 16720/119245 (epoch 5/35), loss = 0.146235 (0.810 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:14:20.384129: step 16740/119245 (epoch 5/35), loss = 0.127911 (0.907 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:14:30.139184: step 16760/119245 (epoch 5/35), loss = 0.176841 (1.037 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:14:39.181303: step 16780/119245 (epoch 5/35), loss = 0.395034 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:14:48.100556: step 16800/119245 (epoch 5/35), loss = 0.008094 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:14:57.188819: step 16820/119245 (epoch 5/35), loss = 0.313021 (0.590 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:15:07.895974: step 16840/119245 (epoch 5/35), loss = 0.196361 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:15:16.761880: step 16860/119245 (epoch 5/35), loss = 0.517818 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:15:25.403107: step 16880/119245 (epoch 5/35), loss = 0.073471 (0.439 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:15:34.447792: step 16900/119245 (epoch 5/35), loss = 0.063747 (0.365 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:15:45.010693: step 16920/119245 (epoch 5/35), loss = 0.787109 (0.350 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:15:55.320313: step 16940/119245 (epoch 5/35), loss = 0.033374 (0.512 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:16:04.827410: step 16960/119245 (epoch 5/35), loss = 0.171114 (0.361 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:16:14.604296: step 16980/119245 (epoch 5/35), loss = 0.171003 (0.475 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:16:24.287784: step 17000/119245 (epoch 5/35), loss = 0.163343 (0.333 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:16:36.102755: step 17020/119245 (epoch 5/35), loss = 0.838498 (0.492 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  79.82%  R:  79.59%  F1:  79.70%  #: 338\n",
            "org:city_of_headquarters             P:  64.04%  R:  52.29%  F1:  57.58%  #: 109\n",
            "org:country_of_headquarters          P:  62.76%  R:  51.41%  F1:  56.52%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  86.21%  R:  65.79%  F1:  74.63%  #: 38\n",
            "org:founded_by                       P:  77.27%  R:  44.74%  F1:  56.67%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  80.60%  R:  63.53%  F1:  71.05%  #: 85\n",
            "org:number_of_employees/members      P:  78.95%  R:  55.56%  F1:  65.22%  #: 27\n",
            "org:parents                          P:  30.14%  R:  45.83%  F1:  36.36%  #: 96\n",
            "org:political/religious_affiliation  P:  15.38%  R:  40.00%  F1:  22.22%  #: 10\n",
            "org:shareholders                     P:  39.53%  R:  30.91%  F1:  34.69%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  64.38%  R:  67.14%  F1:  65.73%  #: 70\n",
            "org:subsidiaries                     P:  53.57%  R:  13.27%  F1:  21.28%  #: 113\n",
            "org:top_members/employees            P:  74.68%  R:  76.78%  F1:  75.72%  #: 534\n",
            "org:website                          P:  87.50%  R:  97.67%  F1:  92.31%  #: 86\n",
            "per:age                              P:  82.72%  R:  92.59%  F1:  87.38%  #: 243\n",
            "per:alternate_names                  P:  66.67%  R:  42.11%  F1:  51.61%  #: 38\n",
            "per:cause_of_death                   P:  85.51%  R:  70.24%  F1:  77.12%  #: 168\n",
            "per:charges                          P:  68.91%  R:  78.10%  F1:  73.21%  #: 105\n",
            "per:children                         P:  62.92%  R:  56.57%  F1:  59.57%  #: 99\n",
            "per:cities_of_residence              P:  50.86%  R:  32.96%  F1:  40.00%  #: 179\n",
            "per:city_of_birth                    P:  65.91%  R:  87.88%  F1:  75.32%  #: 33\n",
            "per:city_of_death                    P:  70.19%  R:  61.86%  F1:  65.77%  #: 118\n",
            "per:countries_of_residence           P:  34.66%  R:  53.98%  F1:  42.21%  #: 226\n",
            "per:country_of_birth                 P:  75.00%  R:  30.00%  F1:  42.86%  #: 20\n",
            "per:country_of_death                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 46\n",
            "per:date_of_birth                    P:  84.85%  R:  90.32%  F1:  87.50%  #: 31\n",
            "per:date_of_death                    P:  77.19%  R:  42.72%  F1:  55.00%  #: 206\n",
            "per:employee_of                      P:  65.99%  R:  52.27%  F1:  58.33%  #: 375\n",
            "per:origin                           P:  68.92%  R:  48.57%  F1:  56.98%  #: 210\n",
            "per:other_family                     P:  60.00%  R:   7.50%  F1:  13.33%  #: 80\n",
            "per:parents                          P:  46.03%  R:  51.79%  F1:  48.74%  #: 56\n",
            "per:religion                         P:  68.18%  R:  56.60%  F1:  61.86%  #: 53\n",
            "per:schools_attended                 P:  92.31%  R:  48.00%  F1:  63.16%  #: 50\n",
            "per:siblings                         P:  64.29%  R:  60.00%  F1:  62.07%  #: 30\n",
            "per:spouse                           P:  69.29%  R:  61.01%  F1:  64.88%  #: 159\n",
            "per:stateorprovince_of_birth         P:  64.29%  R:  69.23%  F1:  66.67%  #: 26\n",
            "per:stateorprovince_of_death         P:  33.33%  R:   4.88%  F1:   8.51%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  33.02%  R:  48.61%  F1:  39.33%  #: 72\n",
            "per:title                            P:  75.40%  R:  86.72%  F1:  80.67%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17571 Guess as no relation\n",
            "5060 guess\n",
            "3422 correct\n",
            "5436 gold\n",
            "Precision (micro): 67.628%\n",
            "   Recall (micro): 62.951%\n",
            "       F1 (micro): 65.206%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 5: train_loss = 0.225646, dev_loss = 0.523057, dev_f1 = 0.6521\n",
            "model saved to ./saved_models/00/checkpoint_epoch_5.pt\n",
            "\n",
            "2020-12-02 02:19:33.678541: step 17040/119245 (epoch 6/35), loss = 0.400320 (1.002 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:19:42.572965: step 17060/119245 (epoch 6/35), loss = 0.031696 (0.422 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:19:54.480425: step 17080/119245 (epoch 6/35), loss = 0.303198 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:20:02.806487: step 17100/119245 (epoch 6/35), loss = 0.212851 (0.352 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:20:13.600830: step 17120/119245 (epoch 6/35), loss = 0.078775 (0.340 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:20:22.842956: step 17140/119245 (epoch 6/35), loss = 0.207423 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:20:32.574037: step 17160/119245 (epoch 6/35), loss = 0.329862 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:20:42.075875: step 17180/119245 (epoch 6/35), loss = 0.039465 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:20:50.550374: step 17200/119245 (epoch 6/35), loss = 0.097508 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:21:00.383708: step 17220/119245 (epoch 6/35), loss = 0.077604 (0.486 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:21:10.928308: step 17240/119245 (epoch 6/35), loss = 0.049312 (0.477 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:21:19.946890: step 17260/119245 (epoch 6/35), loss = 0.015660 (0.444 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:21:29.330189: step 17280/119245 (epoch 6/35), loss = 0.030928 (0.632 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:21:40.171568: step 17300/119245 (epoch 6/35), loss = 0.007617 (0.518 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:21:48.609327: step 17320/119245 (epoch 6/35), loss = 0.025896 (0.402 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:21:57.475170: step 17340/119245 (epoch 6/35), loss = 0.125138 (0.650 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:22:08.313262: step 17360/119245 (epoch 6/35), loss = 0.313725 (0.452 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:22:17.947596: step 17380/119245 (epoch 6/35), loss = 0.237670 (0.467 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:22:29.205484: step 17400/119245 (epoch 6/35), loss = 0.324767 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:22:38.734013: step 17420/119245 (epoch 6/35), loss = 0.057142 (0.539 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:22:49.311251: step 17440/119245 (epoch 6/35), loss = 0.014438 (0.540 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:22:58.368189: step 17460/119245 (epoch 6/35), loss = 0.315370 (0.396 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:23:09.108982: step 17480/119245 (epoch 6/35), loss = 0.120604 (0.343 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:23:19.958097: step 17500/119245 (epoch 6/35), loss = 0.415993 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:23:29.844404: step 17520/119245 (epoch 6/35), loss = 0.049087 (0.391 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:23:40.640569: step 17540/119245 (epoch 6/35), loss = 0.067700 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:23:49.909114: step 17560/119245 (epoch 6/35), loss = 0.093604 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:24:01.235116: step 17580/119245 (epoch 6/35), loss = 0.201515 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:24:09.878532: step 17600/119245 (epoch 6/35), loss = 0.023911 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:24:19.157742: step 17620/119245 (epoch 6/35), loss = 0.365173 (0.348 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:24:28.284267: step 17640/119245 (epoch 6/35), loss = 0.203116 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:24:37.317572: step 17660/119245 (epoch 6/35), loss = 0.020767 (0.371 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:24:46.740965: step 17680/119245 (epoch 6/35), loss = 0.203663 (0.376 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:24:56.973231: step 17700/119245 (epoch 6/35), loss = 0.542744 (0.491 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:25:08.331657: step 17720/119245 (epoch 6/35), loss = 0.014344 (0.435 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:25:16.872270: step 17740/119245 (epoch 6/35), loss = 0.397303 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:25:27.236950: step 17760/119245 (epoch 6/35), loss = 0.133038 (0.509 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:25:37.222269: step 17780/119245 (epoch 6/35), loss = 0.369801 (0.381 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:25:46.772093: step 17800/119245 (epoch 6/35), loss = 0.155374 (0.423 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:25:56.589240: step 17820/119245 (epoch 6/35), loss = 0.252064 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:26:06.523439: step 17840/119245 (epoch 6/35), loss = 0.092224 (0.368 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:26:15.378120: step 17860/119245 (epoch 6/35), loss = 0.023024 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:26:25.050167: step 17880/119245 (epoch 6/35), loss = 0.542507 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:26:35.158492: step 17900/119245 (epoch 6/35), loss = 0.023233 (0.473 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:26:45.374891: step 17920/119245 (epoch 6/35), loss = 0.442318 (0.418 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:26:57.042237: step 17940/119245 (epoch 6/35), loss = 0.130043 (0.624 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:27:06.343999: step 17960/119245 (epoch 6/35), loss = 0.041992 (0.378 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:27:15.202640: step 17980/119245 (epoch 6/35), loss = 0.228599 (0.423 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:27:23.957630: step 18000/119245 (epoch 6/35), loss = 0.026249 (0.342 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:27:32.633105: step 18020/119245 (epoch 6/35), loss = 0.132608 (0.555 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:27:41.341659: step 18040/119245 (epoch 6/35), loss = 0.358847 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:27:50.242353: step 18060/119245 (epoch 6/35), loss = 0.014500 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:28:00.609496: step 18080/119245 (epoch 6/35), loss = 0.040101 (0.370 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:28:09.533553: step 18100/119245 (epoch 6/35), loss = 0.513553 (0.617 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:28:20.982992: step 18120/119245 (epoch 6/35), loss = 0.050931 (0.530 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:28:32.440784: step 18140/119245 (epoch 6/35), loss = 0.208052 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:28:41.256676: step 18160/119245 (epoch 6/35), loss = 0.010673 (0.412 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:28:51.763848: step 18180/119245 (epoch 6/35), loss = 0.163337 (0.758 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:29:02.109828: step 18200/119245 (epoch 6/35), loss = 0.040035 (0.435 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:29:11.232478: step 18220/119245 (epoch 6/35), loss = 0.365048 (0.614 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:29:21.358111: step 18240/119245 (epoch 6/35), loss = 0.243074 (0.666 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:29:30.707320: step 18260/119245 (epoch 6/35), loss = 0.082224 (0.440 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:29:39.834593: step 18280/119245 (epoch 6/35), loss = 0.029718 (0.374 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:29:49.518512: step 18300/119245 (epoch 6/35), loss = 0.051833 (0.360 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:30:01.691032: step 18320/119245 (epoch 6/35), loss = 0.306857 (0.902 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:30:13.486887: step 18340/119245 (epoch 6/35), loss = 0.038931 (0.607 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:30:23.555461: step 18360/119245 (epoch 6/35), loss = 0.151209 (0.514 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:30:33.562877: step 18380/119245 (epoch 6/35), loss = 0.046710 (0.434 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:30:43.148583: step 18400/119245 (epoch 6/35), loss = 0.104087 (0.408 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:30:52.351322: step 18420/119245 (epoch 6/35), loss = 0.265712 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:31:02.065535: step 18440/119245 (epoch 6/35), loss = 0.054700 (1.272 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:31:11.613981: step 18460/119245 (epoch 6/35), loss = 0.299504 (0.418 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:31:21.237448: step 18480/119245 (epoch 6/35), loss = 0.016454 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:31:31.355805: step 18500/119245 (epoch 6/35), loss = 0.195050 (0.561 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:31:40.836523: step 18520/119245 (epoch 6/35), loss = 0.166445 (0.582 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:31:49.739161: step 18540/119245 (epoch 6/35), loss = 0.248284 (0.743 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:31:59.671687: step 18560/119245 (epoch 6/35), loss = 0.132766 (0.619 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:32:09.775110: step 18580/119245 (epoch 6/35), loss = 0.142895 (0.420 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:32:19.734280: step 18600/119245 (epoch 6/35), loss = 0.065563 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:32:29.618104: step 18620/119245 (epoch 6/35), loss = 0.132458 (0.477 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:32:39.729691: step 18640/119245 (epoch 6/35), loss = 0.362905 (1.349 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:32:48.951493: step 18660/119245 (epoch 6/35), loss = 0.038650 (0.707 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:32:59.663026: step 18680/119245 (epoch 6/35), loss = 0.026458 (0.473 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:33:08.804142: step 18700/119245 (epoch 6/35), loss = 0.079791 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:33:18.599810: step 18720/119245 (epoch 6/35), loss = 0.180488 (0.679 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:33:27.923266: step 18740/119245 (epoch 6/35), loss = 0.426136 (0.372 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:33:39.153511: step 18760/119245 (epoch 6/35), loss = 0.063998 (0.600 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:33:49.085639: step 18780/119245 (epoch 6/35), loss = 0.093054 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:33:58.902951: step 18800/119245 (epoch 6/35), loss = 0.130839 (0.351 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:34:08.930736: step 18820/119245 (epoch 6/35), loss = 0.295404 (0.590 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:34:18.475523: step 18840/119245 (epoch 6/35), loss = 0.472610 (0.543 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:34:27.830491: step 18860/119245 (epoch 6/35), loss = 0.051527 (0.439 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:34:37.178714: step 18880/119245 (epoch 6/35), loss = 0.637922 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:34:49.429059: step 18900/119245 (epoch 6/35), loss = 0.398202 (0.354 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:35:00.494103: step 18920/119245 (epoch 6/35), loss = 0.268171 (1.307 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:35:09.667912: step 18940/119245 (epoch 6/35), loss = 0.222790 (0.349 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:35:19.286538: step 18960/119245 (epoch 6/35), loss = 0.030676 (1.503 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:35:29.768678: step 18980/119245 (epoch 6/35), loss = 0.048045 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:35:40.905874: step 19000/119245 (epoch 6/35), loss = 0.112331 (0.614 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:35:51.082476: step 19020/119245 (epoch 6/35), loss = 0.161591 (0.595 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:36:01.155673: step 19040/119245 (epoch 6/35), loss = 0.121865 (0.467 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:36:11.153894: step 19060/119245 (epoch 6/35), loss = 0.231589 (0.865 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:36:21.199987: step 19080/119245 (epoch 6/35), loss = 0.178417 (0.369 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:36:29.993626: step 19100/119245 (epoch 6/35), loss = 0.548355 (0.483 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:36:40.514991: step 19120/119245 (epoch 6/35), loss = 0.014740 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:36:50.183388: step 19140/119245 (epoch 6/35), loss = 0.050286 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:37:00.373018: step 19160/119245 (epoch 6/35), loss = 0.262210 (0.434 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:37:09.691793: step 19180/119245 (epoch 6/35), loss = 0.334236 (0.422 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:37:19.652883: step 19200/119245 (epoch 6/35), loss = 0.185745 (0.393 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:37:29.687102: step 19220/119245 (epoch 6/35), loss = 0.053018 (0.480 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:37:40.042528: step 19240/119245 (epoch 6/35), loss = 0.726898 (0.365 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:37:50.997147: step 19260/119245 (epoch 6/35), loss = 0.057823 (0.972 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:38:00.054103: step 19280/119245 (epoch 6/35), loss = 0.272287 (0.542 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:38:09.838302: step 19300/119245 (epoch 6/35), loss = 0.069879 (0.555 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:38:18.822795: step 19320/119245 (epoch 6/35), loss = 0.058910 (0.698 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:38:28.479878: step 19340/119245 (epoch 6/35), loss = 0.015529 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:38:39.203027: step 19360/119245 (epoch 6/35), loss = 0.358997 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:38:48.792108: step 19380/119245 (epoch 6/35), loss = 0.085687 (0.544 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:39:00.532249: step 19400/119245 (epoch 6/35), loss = 0.338453 (0.599 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:39:10.619539: step 19420/119245 (epoch 6/35), loss = 0.291424 (0.379 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:39:20.269220: step 19440/119245 (epoch 6/35), loss = 0.056278 (0.524 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:39:29.358609: step 19460/119245 (epoch 6/35), loss = 0.477378 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:39:40.542578: step 19480/119245 (epoch 6/35), loss = 0.328070 (0.815 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:39:50.172324: step 19500/119245 (epoch 6/35), loss = 0.226553 (0.325 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:39:59.987903: step 19520/119245 (epoch 6/35), loss = 0.067167 (0.452 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:40:10.758713: step 19540/119245 (epoch 6/35), loss = 0.112455 (1.020 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:40:19.544416: step 19560/119245 (epoch 6/35), loss = 0.325042 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:40:28.415649: step 19580/119245 (epoch 6/35), loss = 0.079752 (0.435 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:40:38.366039: step 19600/119245 (epoch 6/35), loss = 0.140717 (0.622 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:40:47.958476: step 19620/119245 (epoch 6/35), loss = 0.006248 (0.372 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:40:56.780876: step 19640/119245 (epoch 6/35), loss = 0.054792 (0.408 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:41:07.587240: step 19660/119245 (epoch 6/35), loss = 0.034586 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:41:16.558853: step 19680/119245 (epoch 6/35), loss = 0.073106 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:41:25.658772: step 19700/119245 (epoch 6/35), loss = 0.153085 (0.374 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:41:35.153042: step 19720/119245 (epoch 6/35), loss = 0.120870 (0.746 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:41:45.514934: step 19740/119245 (epoch 6/35), loss = 0.066127 (1.011 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:41:55.571370: step 19760/119245 (epoch 6/35), loss = 0.072600 (0.560 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:42:05.242827: step 19780/119245 (epoch 6/35), loss = 0.041361 (0.380 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:42:14.838577: step 19800/119245 (epoch 6/35), loss = 0.031824 (0.381 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:42:24.279709: step 19820/119245 (epoch 6/35), loss = 0.242309 (0.373 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:42:34.145995: step 19840/119245 (epoch 6/35), loss = 0.496473 (0.594 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:42:43.845802: step 19860/119245 (epoch 6/35), loss = 0.058210 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:42:53.552895: step 19880/119245 (epoch 6/35), loss = 0.306754 (0.825 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:43:03.040216: step 19900/119245 (epoch 6/35), loss = 0.264777 (0.318 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:43:12.557132: step 19920/119245 (epoch 6/35), loss = 0.153063 (0.493 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:43:22.096671: step 19940/119245 (epoch 6/35), loss = 0.049624 (0.554 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:43:30.971772: step 19960/119245 (epoch 6/35), loss = 0.195712 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:43:40.370631: step 19980/119245 (epoch 6/35), loss = 0.045928 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:43:52.151909: step 20000/119245 (epoch 6/35), loss = 0.128141 (0.549 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:44:00.689834: step 20020/119245 (epoch 6/35), loss = 0.127750 (0.326 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:44:10.579885: step 20040/119245 (epoch 6/35), loss = 0.008308 (0.631 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:44:21.186431: step 20060/119245 (epoch 6/35), loss = 0.250275 (0.424 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:44:30.783587: step 20080/119245 (epoch 6/35), loss = 0.126529 (0.519 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:44:40.726886: step 20100/119245 (epoch 6/35), loss = 0.009286 (0.509 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:44:50.910206: step 20120/119245 (epoch 6/35), loss = 0.063030 (0.616 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:45:00.662756: step 20140/119245 (epoch 6/35), loss = 0.004914 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:45:10.630456: step 20160/119245 (epoch 6/35), loss = 0.057308 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:45:20.004761: step 20180/119245 (epoch 6/35), loss = 0.067597 (0.511 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:45:29.605730: step 20200/119245 (epoch 6/35), loss = 0.352064 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:45:38.243961: step 20220/119245 (epoch 6/35), loss = 0.062320 (0.508 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:45:48.850645: step 20240/119245 (epoch 6/35), loss = 0.230160 (0.321 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:45:58.035069: step 20260/119245 (epoch 6/35), loss = 0.113431 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:46:06.857657: step 20280/119245 (epoch 6/35), loss = 0.231737 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:46:15.717268: step 20300/119245 (epoch 6/35), loss = 0.240954 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:46:25.854738: step 20320/119245 (epoch 6/35), loss = 0.013876 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:46:35.044649: step 20340/119245 (epoch 6/35), loss = 0.030000 (0.532 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:46:46.120536: step 20360/119245 (epoch 6/35), loss = 0.450811 (0.549 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:46:55.653399: step 20380/119245 (epoch 6/35), loss = 0.114893 (0.396 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:47:05.504900: step 20400/119245 (epoch 6/35), loss = 0.242303 (0.608 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:47:15.283512: step 20420/119245 (epoch 6/35), loss = 0.026990 (0.617 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:47:26.253135: step 20440/119245 (epoch 6/35), loss = 0.059824 (0.437 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  80.24%  R:  78.11%  F1:  79.16%  #: 338\n",
            "org:city_of_headquarters             P:  65.17%  R:  53.21%  F1:  58.59%  #: 109\n",
            "org:country_of_headquarters          P:  52.60%  R:  57.06%  F1:  54.74%  #: 177\n",
            "org:dissolved                        P:   7.14%  R:  12.50%  F1:   9.09%  #: 8\n",
            "org:founded                          P:  77.50%  R:  81.58%  F1:  79.49%  #: 38\n",
            "org:founded_by                       P:  68.85%  R:  55.26%  F1:  61.31%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  76.39%  R:  64.71%  F1:  70.06%  #: 85\n",
            "org:number_of_employees/members      P:  72.73%  R:  59.26%  F1:  65.31%  #: 27\n",
            "org:parents                          P:  29.49%  R:  47.92%  F1:  36.51%  #: 96\n",
            "org:political/religious_affiliation  P:  14.29%  R:  20.00%  F1:  16.67%  #: 10\n",
            "org:shareholders                     P:  23.53%  R:  29.09%  F1:  26.02%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  60.26%  R:  67.14%  F1:  63.51%  #: 70\n",
            "org:subsidiaries                     P:  48.28%  R:  24.78%  F1:  32.75%  #: 113\n",
            "org:top_members/employees            P:  73.22%  R:  73.22%  F1:  73.22%  #: 534\n",
            "org:website                          P:  89.36%  R:  97.67%  F1:  93.33%  #: 86\n",
            "per:age                              P:  86.85%  R:  89.71%  F1:  88.26%  #: 243\n",
            "per:alternate_names                  P:  73.68%  R:  36.84%  F1:  49.12%  #: 38\n",
            "per:cause_of_death                   P:  89.39%  R:  70.24%  F1:  78.67%  #: 168\n",
            "per:charges                          P:  73.87%  R:  78.10%  F1:  75.93%  #: 105\n",
            "per:children                         P:  75.00%  R:  48.48%  F1:  58.90%  #: 99\n",
            "per:cities_of_residence              P:  45.95%  R:  47.49%  F1:  46.70%  #: 179\n",
            "per:city_of_birth                    P:  74.36%  R:  87.88%  F1:  80.56%  #: 33\n",
            "per:city_of_death                    P:  75.58%  R:  55.08%  F1:  63.73%  #: 118\n",
            "per:countries_of_residence           P:  36.41%  R:  62.83%  F1:  46.10%  #: 226\n",
            "per:country_of_birth                 P:  75.00%  R:  30.00%  F1:  42.86%  #: 20\n",
            "per:country_of_death                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 46\n",
            "per:date_of_birth                    P:  82.35%  R:  90.32%  F1:  86.15%  #: 31\n",
            "per:date_of_death                    P:  78.83%  R:  52.43%  F1:  62.97%  #: 206\n",
            "per:employee_of                      P:  61.64%  R:  62.13%  F1:  61.89%  #: 375\n",
            "per:origin                           P:  73.43%  R:  50.00%  F1:  59.49%  #: 210\n",
            "per:other_family                     P:  63.64%  R:  17.50%  F1:  27.45%  #: 80\n",
            "per:parents                          P:  40.26%  R:  55.36%  F1:  46.62%  #: 56\n",
            "per:religion                         P:  66.67%  R:  60.38%  F1:  63.37%  #: 53\n",
            "per:schools_attended                 P:  86.11%  R:  62.00%  F1:  72.09%  #: 50\n",
            "per:siblings                         P:  66.67%  R:  66.67%  F1:  66.67%  #: 30\n",
            "per:spouse                           P:  64.04%  R:  71.70%  F1:  67.66%  #: 159\n",
            "per:stateorprovince_of_birth         P:  70.83%  R:  65.38%  F1:  68.00%  #: 26\n",
            "per:stateorprovince_of_death         P:  50.00%  R:   4.88%  F1:   8.89%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  35.94%  R:  63.89%  F1:  46.00%  #: 72\n",
            "per:title                            P:  79.35%  R:  82.37%  F1:  80.83%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17332 Guess as no relation\n",
            "5299 guess\n",
            "3527 correct\n",
            "5436 gold\n",
            "Precision (micro): 66.560%\n",
            "   Recall (micro): 64.882%\n",
            "       F1 (micro): 65.710%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 6: train_loss = 0.184336, dev_loss = 0.581159, dev_f1 = 0.6571\n",
            "model saved to ./saved_models/00/checkpoint_epoch_6.pt\n",
            "new best model saved.\n",
            "\n",
            "2020-12-02 02:50:27.223640: step 20460/119245 (epoch 7/35), loss = 0.178417 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:50:38.670874: step 20480/119245 (epoch 7/35), loss = 0.155825 (0.846 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:50:47.935393: step 20500/119245 (epoch 7/35), loss = 0.305146 (0.473 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:50:58.723833: step 20520/119245 (epoch 7/35), loss = 0.237770 (0.406 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:51:07.581528: step 20540/119245 (epoch 7/35), loss = 0.022861 (0.481 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:51:17.920426: step 20560/119245 (epoch 7/35), loss = 0.215870 (0.393 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:51:27.438827: step 20580/119245 (epoch 7/35), loss = 0.123089 (0.621 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:51:36.009554: step 20600/119245 (epoch 7/35), loss = 0.371301 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:51:45.325200: step 20620/119245 (epoch 7/35), loss = 0.163392 (0.515 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:51:55.355143: step 20640/119245 (epoch 7/35), loss = 0.102325 (0.370 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:52:04.987836: step 20660/119245 (epoch 7/35), loss = 0.036307 (0.519 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:52:13.908235: step 20680/119245 (epoch 7/35), loss = 0.044709 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:52:23.846224: step 20700/119245 (epoch 7/35), loss = 0.035293 (0.308 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:52:34.268890: step 20720/119245 (epoch 7/35), loss = 0.357712 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:52:42.625365: step 20740/119245 (epoch 7/35), loss = 0.025165 (0.461 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:52:52.669045: step 20760/119245 (epoch 7/35), loss = 0.141452 (1.007 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:53:03.038629: step 20780/119245 (epoch 7/35), loss = 0.069165 (0.377 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:53:14.572986: step 20800/119245 (epoch 7/35), loss = 0.294169 (0.636 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:53:23.601566: step 20820/119245 (epoch 7/35), loss = 0.189279 (0.401 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:53:32.676605: step 20840/119245 (epoch 7/35), loss = 0.142931 (0.516 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:53:43.807012: step 20860/119245 (epoch 7/35), loss = 0.297644 (0.549 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:53:52.908252: step 20880/119245 (epoch 7/35), loss = 0.094927 (0.429 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:54:05.409358: step 20900/119245 (epoch 7/35), loss = 0.345235 (0.412 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:54:14.698708: step 20920/119245 (epoch 7/35), loss = 0.024070 (0.491 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:54:24.823378: step 20940/119245 (epoch 7/35), loss = 0.207060 (0.364 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:54:35.204214: step 20960/119245 (epoch 7/35), loss = 0.071597 (0.491 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:54:45.696943: step 20980/119245 (epoch 7/35), loss = 0.015816 (1.037 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:54:54.973964: step 21000/119245 (epoch 7/35), loss = 0.064625 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:55:04.839011: step 21020/119245 (epoch 7/35), loss = 0.087911 (0.547 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:55:13.578269: step 21040/119245 (epoch 7/35), loss = 0.298283 (0.614 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:55:22.363131: step 21060/119245 (epoch 7/35), loss = 0.114741 (0.420 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:55:31.840239: step 21080/119245 (epoch 7/35), loss = 0.045743 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:55:41.177112: step 21100/119245 (epoch 7/35), loss = 0.142816 (0.321 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:55:52.431800: step 21120/119245 (epoch 7/35), loss = 0.007923 (0.438 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:56:02.458072: step 21140/119245 (epoch 7/35), loss = 0.020105 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:56:12.300989: step 21160/119245 (epoch 7/35), loss = 0.148492 (0.446 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:56:22.318067: step 21180/119245 (epoch 7/35), loss = 0.173667 (0.407 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:56:31.757190: step 21200/119245 (epoch 7/35), loss = 0.213524 (0.406 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:56:41.604965: step 21220/119245 (epoch 7/35), loss = 0.009552 (0.451 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:56:50.676104: step 21240/119245 (epoch 7/35), loss = 0.004412 (0.597 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:57:00.598968: step 21260/119245 (epoch 7/35), loss = 0.053493 (0.402 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:57:09.915461: step 21280/119245 (epoch 7/35), loss = 0.426266 (0.586 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:57:19.088805: step 21300/119245 (epoch 7/35), loss = 0.022765 (0.491 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:57:30.559005: step 21320/119245 (epoch 7/35), loss = 0.069733 (0.965 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:57:41.679329: step 21340/119245 (epoch 7/35), loss = 0.551355 (1.516 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:57:51.429942: step 21360/119245 (epoch 7/35), loss = 0.019843 (0.570 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:58:00.287855: step 21380/119245 (epoch 7/35), loss = 0.087716 (0.349 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:58:09.150395: step 21400/119245 (epoch 7/35), loss = 0.069391 (0.554 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:58:17.653693: step 21420/119245 (epoch 7/35), loss = 0.089110 (0.497 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:58:26.696529: step 21440/119245 (epoch 7/35), loss = 0.462166 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:58:35.230842: step 21460/119245 (epoch 7/35), loss = 0.013957 (0.535 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:58:44.582039: step 21480/119245 (epoch 7/35), loss = 0.041665 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:58:54.698830: step 21500/119245 (epoch 7/35), loss = 0.003030 (0.372 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:59:04.899767: step 21520/119245 (epoch 7/35), loss = 0.126246 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:59:17.601962: step 21540/119245 (epoch 7/35), loss = 0.078507 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:59:26.468044: step 21560/119245 (epoch 7/35), loss = 0.162549 (0.422 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:59:36.090151: step 21580/119245 (epoch 7/35), loss = 0.006045 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:59:46.863173: step 21600/119245 (epoch 7/35), loss = 0.157463 (0.802 sec/batch), lr: 0.030000\n",
            "2020-12-02 02:59:56.397404: step 21620/119245 (epoch 7/35), loss = 0.219598 (0.402 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:00:06.276566: step 21640/119245 (epoch 7/35), loss = 0.105770 (0.613 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:00:15.313697: step 21660/119245 (epoch 7/35), loss = 0.140869 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:00:24.657387: step 21680/119245 (epoch 7/35), loss = 0.414859 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:00:33.924896: step 21700/119245 (epoch 7/35), loss = 0.027610 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:00:44.350336: step 21720/119245 (epoch 7/35), loss = 0.174965 (0.554 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:00:58.824583: step 21740/119245 (epoch 7/35), loss = 0.033239 (1.551 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:01:08.466054: step 21760/119245 (epoch 7/35), loss = 0.365806 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:01:18.446547: step 21780/119245 (epoch 7/35), loss = 0.308005 (0.301 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:01:27.980878: step 21800/119245 (epoch 7/35), loss = 0.045223 (0.380 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:01:37.508031: step 21820/119245 (epoch 7/35), loss = 0.008818 (0.600 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:01:46.488765: step 21840/119245 (epoch 7/35), loss = 0.061916 (0.287 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:01:56.418298: step 21860/119245 (epoch 7/35), loss = 0.225553 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:02:06.209419: step 21880/119245 (epoch 7/35), loss = 0.261504 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:02:16.262086: step 21900/119245 (epoch 7/35), loss = 0.040673 (0.895 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:02:25.781910: step 21920/119245 (epoch 7/35), loss = 0.376170 (0.339 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:02:34.505431: step 21940/119245 (epoch 7/35), loss = 0.177900 (0.350 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:02:43.681383: step 21960/119245 (epoch 7/35), loss = 0.194730 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:02:54.901761: step 21980/119245 (epoch 7/35), loss = 0.148086 (0.476 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:03:03.898821: step 22000/119245 (epoch 7/35), loss = 0.150755 (0.551 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:03:14.071610: step 22020/119245 (epoch 7/35), loss = 0.066357 (0.532 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:03:23.553252: step 22040/119245 (epoch 7/35), loss = 0.008945 (0.477 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:03:33.621789: step 22060/119245 (epoch 7/35), loss = 0.070067 (0.478 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:03:43.478891: step 22080/119245 (epoch 7/35), loss = 0.096163 (0.316 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:03:53.318831: step 22100/119245 (epoch 7/35), loss = 0.046912 (0.480 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:04:03.024744: step 22120/119245 (epoch 7/35), loss = 0.056300 (0.490 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:04:12.129730: step 22140/119245 (epoch 7/35), loss = 0.120375 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:04:23.131083: step 22160/119245 (epoch 7/35), loss = 0.060128 (0.614 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:04:33.737401: step 22180/119245 (epoch 7/35), loss = 0.310556 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:04:43.611182: step 22200/119245 (epoch 7/35), loss = 0.496439 (0.471 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:04:52.530570: step 22220/119245 (epoch 7/35), loss = 0.413130 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:05:02.886407: step 22240/119245 (epoch 7/35), loss = 0.008594 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:05:12.105164: step 22260/119245 (epoch 7/35), loss = 0.230355 (0.295 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:05:21.750416: step 22280/119245 (epoch 7/35), loss = 0.174339 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:05:33.498669: step 22300/119245 (epoch 7/35), loss = 0.004468 (0.414 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:05:43.081830: step 22320/119245 (epoch 7/35), loss = 0.263630 (0.702 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:05:53.997138: step 22340/119245 (epoch 7/35), loss = 0.019707 (0.450 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:06:03.219074: step 22360/119245 (epoch 7/35), loss = 0.059735 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:06:13.899330: step 22380/119245 (epoch 7/35), loss = 0.065781 (0.715 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:06:24.682745: step 22400/119245 (epoch 7/35), loss = 0.608908 (0.485 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:06:35.553777: step 22420/119245 (epoch 7/35), loss = 0.064499 (0.519 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:06:45.618191: step 22440/119245 (epoch 7/35), loss = 0.028436 (0.467 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:06:55.647107: step 22460/119245 (epoch 7/35), loss = 0.006921 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:07:04.664724: step 22480/119245 (epoch 7/35), loss = 0.026222 (0.374 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:07:14.607264: step 22500/119245 (epoch 7/35), loss = 0.039254 (0.321 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:07:24.752682: step 22520/119245 (epoch 7/35), loss = 0.034478 (0.355 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:07:34.314726: step 22540/119245 (epoch 7/35), loss = 0.243285 (0.315 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:07:44.757222: step 22560/119245 (epoch 7/35), loss = 0.046853 (0.683 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:07:53.998483: step 22580/119245 (epoch 7/35), loss = 0.137529 (0.539 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:08:04.100668: step 22600/119245 (epoch 7/35), loss = 0.261184 (0.369 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:08:13.963520: step 22620/119245 (epoch 7/35), loss = 0.213719 (0.386 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:08:24.313380: step 22640/119245 (epoch 7/35), loss = 0.137608 (0.779 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:08:34.695156: step 22660/119245 (epoch 7/35), loss = 0.059031 (0.615 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:08:44.411533: step 22680/119245 (epoch 7/35), loss = 0.052126 (0.337 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:08:53.627982: step 22700/119245 (epoch 7/35), loss = 0.069534 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:09:03.221184: step 22720/119245 (epoch 7/35), loss = 0.055483 (0.318 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:09:12.996470: step 22740/119245 (epoch 7/35), loss = 0.384974 (0.420 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:09:22.099158: step 22760/119245 (epoch 7/35), loss = 0.475717 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:09:32.878664: step 22780/119245 (epoch 7/35), loss = 0.036400 (0.488 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:09:44.616177: step 22800/119245 (epoch 7/35), loss = 0.020776 (0.514 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:09:54.888242: step 22820/119245 (epoch 7/35), loss = 0.378386 (0.863 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:10:04.519769: step 22840/119245 (epoch 7/35), loss = 0.068263 (0.452 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:10:13.529824: step 22860/119245 (epoch 7/35), loss = 0.006997 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:10:24.330953: step 22880/119245 (epoch 7/35), loss = 0.045315 (0.456 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:10:34.185330: step 22900/119245 (epoch 7/35), loss = 0.069817 (0.354 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:10:44.404440: step 22920/119245 (epoch 7/35), loss = 0.075241 (0.374 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:10:54.103254: step 22940/119245 (epoch 7/35), loss = 0.185909 (0.621 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:11:03.730256: step 22960/119245 (epoch 7/35), loss = 0.201291 (0.467 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:11:12.725507: step 22980/119245 (epoch 7/35), loss = 0.374306 (0.422 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:11:22.360206: step 23000/119245 (epoch 7/35), loss = 0.229568 (0.598 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:11:32.598544: step 23020/119245 (epoch 7/35), loss = 0.043580 (0.482 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:11:40.988644: step 23040/119245 (epoch 7/35), loss = 0.083178 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:11:50.762809: step 23060/119245 (epoch 7/35), loss = 0.681673 (0.449 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:12:01.076659: step 23080/119245 (epoch 7/35), loss = 0.140803 (0.372 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:12:10.003573: step 23100/119245 (epoch 7/35), loss = 0.004530 (0.573 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:12:18.611049: step 23120/119245 (epoch 7/35), loss = 0.079301 (0.331 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:12:28.397411: step 23140/119245 (epoch 7/35), loss = 0.960671 (0.620 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:12:38.884141: step 23160/119245 (epoch 7/35), loss = 0.035854 (0.599 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:12:48.808317: step 23180/119245 (epoch 7/35), loss = 0.276269 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:12:59.188501: step 23200/119245 (epoch 7/35), loss = 0.024316 (0.382 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:13:08.484010: step 23220/119245 (epoch 7/35), loss = 0.308698 (0.577 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:13:18.270191: step 23240/119245 (epoch 7/35), loss = 0.006299 (0.690 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:13:27.638760: step 23260/119245 (epoch 7/35), loss = 0.271267 (0.766 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:13:37.166930: step 23280/119245 (epoch 7/35), loss = 0.048648 (0.364 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:13:47.575677: step 23300/119245 (epoch 7/35), loss = 0.360822 (0.372 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:13:56.850503: step 23320/119245 (epoch 7/35), loss = 0.053831 (0.581 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:14:06.064680: step 23340/119245 (epoch 7/35), loss = 0.117389 (0.315 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:14:15.423687: step 23360/119245 (epoch 7/35), loss = 0.010108 (0.635 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:14:24.086070: step 23380/119245 (epoch 7/35), loss = 0.033518 (0.493 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:14:35.425679: step 23400/119245 (epoch 7/35), loss = 0.190420 (0.514 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:14:44.759429: step 23420/119245 (epoch 7/35), loss = 0.084476 (0.460 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:14:54.502330: step 23440/119245 (epoch 7/35), loss = 0.044486 (0.527 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:15:05.039197: step 23460/119245 (epoch 7/35), loss = 0.074164 (0.369 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:15:14.524854: step 23480/119245 (epoch 7/35), loss = 0.579654 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:15:23.836463: step 23500/119245 (epoch 7/35), loss = 0.228249 (0.502 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:15:34.589983: step 23520/119245 (epoch 7/35), loss = 0.267051 (0.445 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:15:44.300504: step 23540/119245 (epoch 7/35), loss = 0.005735 (0.396 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:15:54.675038: step 23560/119245 (epoch 7/35), loss = 0.180847 (0.592 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:16:04.101595: step 23580/119245 (epoch 7/35), loss = 0.339867 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:16:13.772049: step 23600/119245 (epoch 7/35), loss = 0.006776 (0.352 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:16:21.825103: step 23620/119245 (epoch 7/35), loss = 0.049007 (0.437 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:16:31.636829: step 23640/119245 (epoch 7/35), loss = 0.026588 (0.996 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:16:41.949990: step 23660/119245 (epoch 7/35), loss = 0.064415 (0.551 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:16:51.095172: step 23680/119245 (epoch 7/35), loss = 0.181255 (0.582 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:16:59.333617: step 23700/119245 (epoch 7/35), loss = 0.301228 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:17:09.260162: step 23720/119245 (epoch 7/35), loss = 0.002471 (1.081 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:17:19.076838: step 23740/119245 (epoch 7/35), loss = 0.001717 (0.859 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:17:29.431415: step 23760/119245 (epoch 7/35), loss = 0.367310 (0.509 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:17:39.177866: step 23780/119245 (epoch 7/35), loss = 0.364001 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:17:49.071764: step 23800/119245 (epoch 7/35), loss = 0.073982 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:17:59.070109: step 23820/119245 (epoch 7/35), loss = 0.078125 (1.009 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:18:10.128496: step 23840/119245 (epoch 7/35), loss = 0.213568 (0.621 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  79.70%  R:  78.99%  F1:  79.35%  #: 338\n",
            "org:city_of_headquarters             P:  69.23%  R:  49.54%  F1:  57.75%  #: 109\n",
            "org:country_of_headquarters          P:  58.91%  R:  42.94%  F1:  49.67%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  85.71%  R:  78.95%  F1:  82.19%  #: 38\n",
            "org:founded_by                       P:  70.31%  R:  59.21%  F1:  64.29%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  83.67%  R:  48.24%  F1:  61.19%  #: 85\n",
            "org:number_of_employees/members      P:  80.95%  R:  62.96%  F1:  70.83%  #: 27\n",
            "org:parents                          P:  38.46%  R:  31.25%  F1:  34.48%  #: 96\n",
            "org:political/religious_affiliation  P:  16.13%  R:  50.00%  F1:  24.39%  #: 10\n",
            "org:shareholders                     P:  40.74%  R:  20.00%  F1:  26.83%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  64.79%  R:  65.71%  F1:  65.25%  #: 70\n",
            "org:subsidiaries                     P:  56.41%  R:  19.47%  F1:  28.95%  #: 113\n",
            "org:top_members/employees            P:  78.64%  R:  71.72%  F1:  75.02%  #: 534\n",
            "org:website                          P:  88.30%  R:  96.51%  F1:  92.22%  #: 86\n",
            "per:age                              P:  81.00%  R:  93.00%  F1:  86.59%  #: 243\n",
            "per:alternate_names                  P:  66.67%  R:  42.11%  F1:  51.61%  #: 38\n",
            "per:cause_of_death                   P:  90.83%  R:  58.93%  F1:  71.48%  #: 168\n",
            "per:charges                          P:  76.92%  R:  76.19%  F1:  76.56%  #: 105\n",
            "per:children                         P:  69.57%  R:  64.65%  F1:  67.02%  #: 99\n",
            "per:cities_of_residence              P:  49.04%  R:  43.02%  F1:  45.83%  #: 179\n",
            "per:city_of_birth                    P:  74.29%  R:  78.79%  F1:  76.47%  #: 33\n",
            "per:city_of_death                    P:  73.26%  R:  53.39%  F1:  61.76%  #: 118\n",
            "per:countries_of_residence           P:  35.61%  R:  41.59%  F1:  38.37%  #: 226\n",
            "per:country_of_birth                 P:  77.78%  R:  35.00%  F1:  48.28%  #: 20\n",
            "per:country_of_death                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 46\n",
            "per:date_of_birth                    P:  87.50%  R:  90.32%  F1:  88.89%  #: 31\n",
            "per:date_of_death                    P:  86.61%  R:  47.09%  F1:  61.01%  #: 206\n",
            "per:employee_of                      P:  59.85%  R:  63.20%  F1:  61.48%  #: 375\n",
            "per:origin                           P:  61.65%  R:  60.48%  F1:  61.06%  #: 210\n",
            "per:other_family                     P:  47.62%  R:  12.50%  F1:  19.80%  #: 80\n",
            "per:parents                          P:  39.29%  R:  58.93%  F1:  47.14%  #: 56\n",
            "per:religion                         P:  59.18%  R:  54.72%  F1:  56.86%  #: 53\n",
            "per:schools_attended                 P:  82.86%  R:  58.00%  F1:  68.24%  #: 50\n",
            "per:siblings                         P:  81.82%  R:  60.00%  F1:  69.23%  #: 30\n",
            "per:spouse                           P:  63.54%  R:  72.33%  F1:  67.65%  #: 159\n",
            "per:stateorprovince_of_birth         P:  72.73%  R:  61.54%  F1:  66.67%  #: 26\n",
            "per:stateorprovince_of_death         P:  57.14%  R:   9.76%  F1:  16.67%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  35.66%  R:  63.89%  F1:  45.77%  #: 72\n",
            "per:title                            P:  76.69%  R:  83.79%  F1:  80.08%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17624 Guess as no relation\n",
            "5007 guess\n",
            "3421 correct\n",
            "5436 gold\n",
            "Precision (micro): 68.324%\n",
            "   Recall (micro): 62.932%\n",
            "       F1 (micro): 65.518%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 7: train_loss = 0.157115, dev_loss = 0.619556, dev_f1 = 0.6552\n",
            "model saved to ./saved_models/00/checkpoint_epoch_7.pt\n",
            "\n",
            "2020-12-02 03:21:07.493486: step 23860/119245 (epoch 8/35), loss = 0.124617 (0.623 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:21:18.386220: step 23880/119245 (epoch 8/35), loss = 0.287344 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:21:28.275160: step 23900/119245 (epoch 8/35), loss = 0.169909 (0.391 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:21:37.225138: step 23920/119245 (epoch 8/35), loss = 0.020888 (0.352 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:21:47.102206: step 23940/119245 (epoch 8/35), loss = 0.005755 (0.332 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:21:57.012252: step 23960/119245 (epoch 8/35), loss = 0.056174 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:22:07.062791: step 23980/119245 (epoch 8/35), loss = 0.006437 (0.484 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:22:15.967528: step 24000/119245 (epoch 8/35), loss = 0.172513 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:22:24.864184: step 24020/119245 (epoch 8/35), loss = 0.421821 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:22:35.008486: step 24040/119245 (epoch 8/35), loss = 0.432637 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:22:45.005234: step 24060/119245 (epoch 8/35), loss = 0.013023 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:22:54.050151: step 24080/119245 (epoch 8/35), loss = 0.172930 (0.619 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:23:03.731254: step 24100/119245 (epoch 8/35), loss = 0.537615 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:23:14.309396: step 24120/119245 (epoch 8/35), loss = 0.352461 (0.586 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:23:22.386380: step 24140/119245 (epoch 8/35), loss = 0.030053 (0.423 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:23:31.796905: step 24160/119245 (epoch 8/35), loss = 0.232990 (0.715 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:23:42.769435: step 24180/119245 (epoch 8/35), loss = 0.144536 (0.477 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:23:52.178378: step 24200/119245 (epoch 8/35), loss = 0.014505 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:24:03.487267: step 24220/119245 (epoch 8/35), loss = 0.224510 (0.617 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:24:12.412828: step 24240/119245 (epoch 8/35), loss = 0.039690 (0.337 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:24:23.162462: step 24260/119245 (epoch 8/35), loss = 0.036938 (0.355 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:24:32.119050: step 24280/119245 (epoch 8/35), loss = 0.013907 (0.491 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:24:44.735903: step 24300/119245 (epoch 8/35), loss = 0.302091 (1.026 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:24:54.132056: step 24320/119245 (epoch 8/35), loss = 0.322677 (0.569 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:25:04.217193: step 24340/119245 (epoch 8/35), loss = 0.349167 (0.354 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:25:14.662561: step 24360/119245 (epoch 8/35), loss = 0.113737 (0.452 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:25:24.194294: step 24380/119245 (epoch 8/35), loss = 0.077992 (0.620 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:25:34.815824: step 24400/119245 (epoch 8/35), loss = 0.136140 (0.364 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:25:43.971729: step 24420/119245 (epoch 8/35), loss = 0.145716 (0.410 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:25:53.034200: step 24440/119245 (epoch 8/35), loss = 0.167316 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:26:01.872451: step 24460/119245 (epoch 8/35), loss = 0.739807 (0.404 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:26:11.333027: step 24480/119245 (epoch 8/35), loss = 0.234323 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:26:21.068522: step 24500/119245 (epoch 8/35), loss = 0.082282 (0.956 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:26:32.188653: step 24520/119245 (epoch 8/35), loss = 0.137920 (0.362 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:26:42.047432: step 24540/119245 (epoch 8/35), loss = 0.231927 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:26:52.003702: step 24560/119245 (epoch 8/35), loss = 0.171954 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:27:01.855310: step 24580/119245 (epoch 8/35), loss = 0.007044 (0.441 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:27:10.941005: step 24600/119245 (epoch 8/35), loss = 0.486961 (0.461 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:27:20.675378: step 24620/119245 (epoch 8/35), loss = 0.072590 (0.369 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:27:30.082109: step 24640/119245 (epoch 8/35), loss = 0.036357 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:27:39.970404: step 24660/119245 (epoch 8/35), loss = 0.253482 (0.468 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:27:49.089445: step 24680/119245 (epoch 8/35), loss = 0.022540 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:27:58.308154: step 24700/119245 (epoch 8/35), loss = 0.295984 (0.424 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:28:09.341391: step 24720/119245 (epoch 8/35), loss = 0.011988 (1.022 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:28:20.466234: step 24740/119245 (epoch 8/35), loss = 0.058208 (0.454 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:28:30.828352: step 24760/119245 (epoch 8/35), loss = 0.008537 (0.474 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:28:39.707219: step 24780/119245 (epoch 8/35), loss = 0.013482 (0.318 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:28:48.836021: step 24800/119245 (epoch 8/35), loss = 0.182634 (0.439 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:28:57.565124: step 24820/119245 (epoch 8/35), loss = 0.221118 (0.444 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:29:06.087070: step 24840/119245 (epoch 8/35), loss = 0.047395 (0.464 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:29:14.632006: step 24860/119245 (epoch 8/35), loss = 0.122072 (0.372 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:29:23.733904: step 24880/119245 (epoch 8/35), loss = 0.064654 (0.461 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:29:34.416053: step 24900/119245 (epoch 8/35), loss = 0.210535 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:29:44.306329: step 24920/119245 (epoch 8/35), loss = 0.011165 (0.573 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:29:56.680211: step 24940/119245 (epoch 8/35), loss = 0.374175 (0.479 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:30:05.979969: step 24960/119245 (epoch 8/35), loss = 0.315338 (0.530 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:30:14.907317: step 24980/119245 (epoch 8/35), loss = 0.181975 (0.501 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:30:25.806176: step 25000/119245 (epoch 8/35), loss = 0.048308 (0.466 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:30:35.193058: step 25020/119245 (epoch 8/35), loss = 0.058027 (0.494 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:30:44.890983: step 25040/119245 (epoch 8/35), loss = 0.305402 (0.670 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:30:54.847783: step 25060/119245 (epoch 8/35), loss = 0.319317 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:31:04.242800: step 25080/119245 (epoch 8/35), loss = 0.484346 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:31:13.373382: step 25100/119245 (epoch 8/35), loss = 0.084488 (0.520 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:31:23.544010: step 25120/119245 (epoch 8/35), loss = 0.282840 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:31:35.932336: step 25140/119245 (epoch 8/35), loss = 0.207502 (0.598 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:31:47.087890: step 25160/119245 (epoch 8/35), loss = 0.262974 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:31:57.474791: step 25180/119245 (epoch 8/35), loss = 0.691752 (0.267 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:32:07.170738: step 25200/119245 (epoch 8/35), loss = 0.221679 (0.468 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:32:16.409036: step 25220/119245 (epoch 8/35), loss = 0.309779 (0.332 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:32:25.618856: step 25240/119245 (epoch 8/35), loss = 0.014837 (0.492 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:32:35.171131: step 25260/119245 (epoch 8/35), loss = 0.016427 (0.496 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:32:44.866890: step 25280/119245 (epoch 8/35), loss = 0.009679 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:32:54.567512: step 25300/119245 (epoch 8/35), loss = 0.451162 (0.492 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:33:04.690413: step 25320/119245 (epoch 8/35), loss = 0.484164 (0.472 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:33:13.867764: step 25340/119245 (epoch 8/35), loss = 0.059807 (0.673 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:33:23.452446: step 25360/119245 (epoch 8/35), loss = 0.059126 (0.949 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:33:33.099438: step 25380/119245 (epoch 8/35), loss = 0.005698 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:33:43.269327: step 25400/119245 (epoch 8/35), loss = 0.069251 (0.774 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:33:52.986268: step 25420/119245 (epoch 8/35), loss = 0.017482 (0.665 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:34:02.746576: step 25440/119245 (epoch 8/35), loss = 0.034288 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:34:12.450537: step 25460/119245 (epoch 8/35), loss = 0.094531 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:34:22.486713: step 25480/119245 (epoch 8/35), loss = 0.240857 (0.762 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:34:32.481575: step 25500/119245 (epoch 8/35), loss = 0.109031 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:34:42.241874: step 25520/119245 (epoch 8/35), loss = 0.003477 (0.480 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:34:51.373461: step 25540/119245 (epoch 8/35), loss = 0.185403 (0.502 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:35:01.102670: step 25560/119245 (epoch 8/35), loss = 1.125534 (0.359 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:35:12.230038: step 25580/119245 (epoch 8/35), loss = 0.083915 (0.646 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:35:22.208522: step 25600/119245 (epoch 8/35), loss = 0.005018 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:35:31.607288: step 25620/119245 (epoch 8/35), loss = 0.427933 (0.277 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:35:41.820011: step 25640/119245 (epoch 8/35), loss = 0.127046 (0.367 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:35:50.819275: step 25660/119245 (epoch 8/35), loss = 0.208140 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:36:00.605633: step 25680/119245 (epoch 8/35), loss = 0.168132 (0.543 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:36:10.100563: step 25700/119245 (epoch 8/35), loss = 0.006647 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:36:21.832373: step 25720/119245 (epoch 8/35), loss = 0.006907 (0.380 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:36:33.338457: step 25740/119245 (epoch 8/35), loss = 0.048775 (0.508 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:36:42.541633: step 25760/119245 (epoch 8/35), loss = 0.079283 (0.583 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:36:52.249162: step 25780/119245 (epoch 8/35), loss = 0.057595 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:37:02.964552: step 25800/119245 (epoch 8/35), loss = 0.249800 (0.997 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:37:14.293664: step 25820/119245 (epoch 8/35), loss = 0.023138 (0.431 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:37:23.863848: step 25840/119245 (epoch 8/35), loss = 0.014661 (0.574 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:37:34.085723: step 25860/119245 (epoch 8/35), loss = 0.124428 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:37:43.513995: step 25880/119245 (epoch 8/35), loss = 0.079418 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:37:53.663950: step 25900/119245 (epoch 8/35), loss = 0.138529 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:38:03.694774: step 25920/119245 (epoch 8/35), loss = 0.037000 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:38:13.076991: step 25940/119245 (epoch 8/35), loss = 0.008025 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:38:23.284526: step 25960/119245 (epoch 8/35), loss = 0.243319 (0.485 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:38:32.330927: step 25980/119245 (epoch 8/35), loss = 0.024377 (0.332 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:38:41.874983: step 26000/119245 (epoch 8/35), loss = 0.029821 (0.341 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:38:53.138277: step 26020/119245 (epoch 8/35), loss = 0.119052 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:39:02.541229: step 26040/119245 (epoch 8/35), loss = 0.032467 (0.463 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:39:12.835266: step 26060/119245 (epoch 8/35), loss = 0.009740 (0.380 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:39:23.450379: step 26080/119245 (epoch 8/35), loss = 0.087382 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:39:32.049098: step 26100/119245 (epoch 8/35), loss = 0.126361 (0.418 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:39:41.990834: step 26120/119245 (epoch 8/35), loss = 0.050385 (0.481 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:39:50.931628: step 26140/119245 (epoch 8/35), loss = 0.013294 (0.326 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:40:01.132460: step 26160/119245 (epoch 8/35), loss = 0.001830 (0.435 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:40:11.849985: step 26180/119245 (epoch 8/35), loss = 0.015706 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:40:22.665047: step 26200/119245 (epoch 8/35), loss = 0.004880 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:40:32.570021: step 26220/119245 (epoch 8/35), loss = 0.101656 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:40:43.121259: step 26240/119245 (epoch 8/35), loss = 0.108861 (0.429 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:40:52.408588: step 26260/119245 (epoch 8/35), loss = 0.306652 (0.514 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:41:02.021739: step 26280/119245 (epoch 8/35), loss = 0.015812 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:41:13.027627: step 26300/119245 (epoch 8/35), loss = 0.002163 (0.611 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:41:22.739930: step 26320/119245 (epoch 8/35), loss = 0.028219 (0.466 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:41:31.926120: step 26340/119245 (epoch 8/35), loss = 0.016682 (0.494 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:41:42.488343: step 26360/119245 (epoch 8/35), loss = 0.331239 (0.420 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:41:51.732302: step 26380/119245 (epoch 8/35), loss = 0.078835 (0.428 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:42:01.013952: step 26400/119245 (epoch 8/35), loss = 0.197008 (0.635 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:42:11.483761: step 26420/119245 (epoch 8/35), loss = 0.622917 (1.024 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:42:19.956828: step 26440/119245 (epoch 8/35), loss = 0.260871 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:42:28.866069: step 26460/119245 (epoch 8/35), loss = 0.679505 (0.369 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:42:39.683068: step 26480/119245 (epoch 8/35), loss = 0.166287 (0.401 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:42:48.280847: step 26500/119245 (epoch 8/35), loss = 0.106889 (0.465 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:42:57.595244: step 26520/119245 (epoch 8/35), loss = 0.007634 (0.633 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:43:07.131274: step 26540/119245 (epoch 8/35), loss = 0.160260 (0.523 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:43:17.475428: step 26560/119245 (epoch 8/35), loss = 0.173889 (0.355 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:43:27.389057: step 26580/119245 (epoch 8/35), loss = 0.178271 (0.545 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:43:38.016802: step 26600/119245 (epoch 8/35), loss = 0.207150 (0.501 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:43:46.748664: step 26620/119245 (epoch 8/35), loss = 0.023406 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:43:56.363392: step 26640/119245 (epoch 8/35), loss = 0.021401 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:44:06.076394: step 26660/119245 (epoch 8/35), loss = 0.013300 (0.516 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:44:15.648462: step 26680/119245 (epoch 8/35), loss = 0.264952 (0.425 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:44:25.871208: step 26700/119245 (epoch 8/35), loss = 0.020823 (0.438 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:44:34.981102: step 26720/119245 (epoch 8/35), loss = 0.088711 (0.618 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:44:44.259491: step 26740/119245 (epoch 8/35), loss = 0.245804 (0.460 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:44:53.162527: step 26760/119245 (epoch 8/35), loss = 0.055805 (0.456 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:45:02.589181: step 26780/119245 (epoch 8/35), loss = 0.057124 (0.609 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:45:12.042504: step 26800/119245 (epoch 8/35), loss = 0.118007 (0.470 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:45:23.346219: step 26820/119245 (epoch 8/35), loss = 0.127929 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:45:32.458035: step 26840/119245 (epoch 8/35), loss = 0.119361 (0.499 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:45:43.280838: step 26860/119245 (epoch 8/35), loss = 0.190953 (0.495 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:45:52.649997: step 26880/119245 (epoch 8/35), loss = 0.033912 (0.407 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:46:02.312018: step 26900/119245 (epoch 8/35), loss = 0.236616 (0.565 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:46:13.106866: step 26920/119245 (epoch 8/35), loss = 0.042764 (0.440 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:46:21.851951: step 26940/119245 (epoch 8/35), loss = 0.014883 (0.311 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:46:32.321072: step 26960/119245 (epoch 8/35), loss = 0.455119 (0.307 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:46:41.976827: step 26980/119245 (epoch 8/35), loss = 0.017267 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:46:51.631271: step 27000/119245 (epoch 8/35), loss = 0.417997 (0.464 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:47:00.601943: step 27020/119245 (epoch 8/35), loss = 0.047267 (0.395 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:47:09.512789: step 27040/119245 (epoch 8/35), loss = 0.010146 (0.384 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:47:20.419849: step 27060/119245 (epoch 8/35), loss = 0.043160 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:47:29.240089: step 27080/119245 (epoch 8/35), loss = 0.420598 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:47:37.836658: step 27100/119245 (epoch 8/35), loss = 0.092098 (0.321 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:47:46.880893: step 27120/119245 (epoch 8/35), loss = 0.016722 (0.596 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:47:57.358621: step 27140/119245 (epoch 8/35), loss = 0.025748 (0.440 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:48:07.445944: step 27160/119245 (epoch 8/35), loss = 0.058356 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:48:17.121194: step 27180/119245 (epoch 8/35), loss = 0.027177 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:48:26.758499: step 27200/119245 (epoch 8/35), loss = 0.074693 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:48:36.503034: step 27220/119245 (epoch 8/35), loss = 0.034775 (0.451 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:48:48.142973: step 27240/119245 (epoch 8/35), loss = 0.009991 (0.358 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  74.59%  R:  80.77%  F1:  77.56%  #: 338\n",
            "org:city_of_headquarters             P:  64.62%  R:  38.53%  F1:  48.28%  #: 109\n",
            "org:country_of_headquarters          P:  56.89%  R:  53.67%  F1:  55.23%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  85.29%  R:  76.32%  F1:  80.56%  #: 38\n",
            "org:founded_by                       P:  71.15%  R:  48.68%  F1:  57.81%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  78.95%  R:  52.94%  F1:  63.38%  #: 85\n",
            "org:number_of_employees/members      P:  70.00%  R:  51.85%  F1:  59.57%  #: 27\n",
            "org:parents                          P:  33.33%  R:  43.75%  F1:  37.84%  #: 96\n",
            "org:political/religious_affiliation  P:  30.00%  R:  60.00%  F1:  40.00%  #: 10\n",
            "org:shareholders                     P:  35.71%  R:  18.18%  F1:  24.10%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  63.08%  R:  58.57%  F1:  60.74%  #: 70\n",
            "org:subsidiaries                     P:  50.94%  R:  23.89%  F1:  32.53%  #: 113\n",
            "org:top_members/employees            P:  72.14%  R:  79.03%  F1:  75.42%  #: 534\n",
            "org:website                          P:  89.25%  R:  96.51%  F1:  92.74%  #: 86\n",
            "per:age                              P:  83.33%  R:  92.59%  F1:  87.72%  #: 243\n",
            "per:alternate_names                  P:  68.42%  R:  34.21%  F1:  45.61%  #: 38\n",
            "per:cause_of_death                   P:  90.65%  R:  57.74%  F1:  70.55%  #: 168\n",
            "per:charges                          P:  78.89%  R:  67.62%  F1:  72.82%  #: 105\n",
            "per:children                         P:  73.33%  R:  55.56%  F1:  63.22%  #: 99\n",
            "per:cities_of_residence              P:  46.11%  R:  49.72%  F1:  47.85%  #: 179\n",
            "per:city_of_birth                    P:  66.67%  R:  60.61%  F1:  63.49%  #: 33\n",
            "per:city_of_death                    P:  73.81%  R:  52.54%  F1:  61.39%  #: 118\n",
            "per:countries_of_residence           P:  36.49%  R:  34.07%  F1:  35.24%  #: 226\n",
            "per:country_of_birth                 P:  75.00%  R:  30.00%  F1:  42.86%  #: 20\n",
            "per:country_of_death                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 46\n",
            "per:date_of_birth                    P:  93.33%  R:  90.32%  F1:  91.80%  #: 31\n",
            "per:date_of_death                    P:  80.38%  R:  61.65%  F1:  69.78%  #: 206\n",
            "per:employee_of                      P:  57.00%  R:  60.80%  F1:  58.84%  #: 375\n",
            "per:origin                           P:  62.21%  R:  50.95%  F1:  56.02%  #: 210\n",
            "per:other_family                     P:  35.00%  R:   8.75%  F1:  14.00%  #: 80\n",
            "per:parents                          P:  48.39%  R:  53.57%  F1:  50.85%  #: 56\n",
            "per:religion                         P:  52.24%  R:  66.04%  F1:  58.33%  #: 53\n",
            "per:schools_attended                 P:  72.55%  R:  74.00%  F1:  73.27%  #: 50\n",
            "per:siblings                         P:  81.82%  R:  60.00%  F1:  69.23%  #: 30\n",
            "per:spouse                           P:  73.97%  R:  67.92%  F1:  70.82%  #: 159\n",
            "per:stateorprovince_of_birth         P:  73.68%  R:  53.85%  F1:  62.22%  #: 26\n",
            "per:stateorprovince_of_death         P:  44.44%  R:   9.76%  F1:  16.00%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  32.11%  R:  48.61%  F1:  38.67%  #: 72\n",
            "per:title                            P:  76.15%  R:  84.44%  F1:  80.08%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17517 Guess as no relation\n",
            "5114 guess\n",
            "3435 correct\n",
            "5436 gold\n",
            "Precision (micro): 67.169%\n",
            "   Recall (micro): 63.190%\n",
            "       F1 (micro): 65.118%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 8: train_loss = 0.128494, dev_loss = 0.688633, dev_f1 = 0.6512\n",
            "model saved to ./saved_models/00/checkpoint_epoch_8.pt\n",
            "\n",
            "2020-12-02 03:51:45.176391: step 27260/119245 (epoch 9/35), loss = 0.322537 (0.480 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:51:54.598228: step 27280/119245 (epoch 9/35), loss = 0.261332 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:52:06.545902: step 27300/119245 (epoch 9/35), loss = 0.015049 (0.468 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:52:14.872385: step 27320/119245 (epoch 9/35), loss = 0.005347 (0.410 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:52:25.640842: step 27340/119245 (epoch 9/35), loss = 0.225359 (0.393 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:52:34.778303: step 27360/119245 (epoch 9/35), loss = 0.593579 (0.495 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:52:44.550905: step 27380/119245 (epoch 9/35), loss = 0.171133 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:52:53.985410: step 27400/119245 (epoch 9/35), loss = 0.086940 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:53:02.322380: step 27420/119245 (epoch 9/35), loss = 0.009700 (0.333 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:53:12.055453: step 27440/119245 (epoch 9/35), loss = 0.069807 (0.351 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:53:22.579267: step 27460/119245 (epoch 9/35), loss = 0.009599 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:53:31.633122: step 27480/119245 (epoch 9/35), loss = 0.097368 (0.538 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:53:40.817780: step 27500/119245 (epoch 9/35), loss = 0.060841 (0.969 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:53:51.802707: step 27520/119245 (epoch 9/35), loss = 0.451182 (1.541 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:54:00.358994: step 27540/119245 (epoch 9/35), loss = 0.009359 (0.517 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:54:08.975788: step 27560/119245 (epoch 9/35), loss = 0.018308 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:54:19.984262: step 27580/119245 (epoch 9/35), loss = 0.017327 (0.543 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:54:29.544563: step 27600/119245 (epoch 9/35), loss = 0.015565 (0.441 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:54:40.798431: step 27620/119245 (epoch 9/35), loss = 0.024624 (0.379 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:54:50.160043: step 27640/119245 (epoch 9/35), loss = 0.059084 (0.428 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:55:00.735609: step 27660/119245 (epoch 9/35), loss = 0.051552 (0.528 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:55:09.937872: step 27680/119245 (epoch 9/35), loss = 0.135746 (0.334 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:55:20.752880: step 27700/119245 (epoch 9/35), loss = 0.083509 (1.643 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:55:31.561409: step 27720/119245 (epoch 9/35), loss = 0.020034 (0.361 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:55:41.472040: step 27740/119245 (epoch 9/35), loss = 0.101828 (1.038 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:55:52.295988: step 27760/119245 (epoch 9/35), loss = 0.158114 (0.544 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:56:01.446170: step 27780/119245 (epoch 9/35), loss = 0.138226 (0.614 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:56:12.736513: step 27800/119245 (epoch 9/35), loss = 0.008474 (0.349 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:56:21.441746: step 27820/119245 (epoch 9/35), loss = 0.012834 (0.350 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:56:30.705527: step 27840/119245 (epoch 9/35), loss = 0.205098 (0.428 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:56:39.756215: step 27860/119245 (epoch 9/35), loss = 0.027235 (0.396 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:56:48.800302: step 27880/119245 (epoch 9/35), loss = 0.036598 (0.293 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:56:58.178347: step 27900/119245 (epoch 9/35), loss = 0.195677 (0.649 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:57:08.241592: step 27920/119245 (epoch 9/35), loss = 0.064502 (0.542 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:57:19.619557: step 27940/119245 (epoch 9/35), loss = 0.452616 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:57:28.193581: step 27960/119245 (epoch 9/35), loss = 0.373801 (0.438 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:57:38.491411: step 27980/119245 (epoch 9/35), loss = 0.187868 (0.410 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:57:48.586263: step 28000/119245 (epoch 9/35), loss = 0.075961 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:57:58.065468: step 28020/119245 (epoch 9/35), loss = 0.010723 (0.523 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:58:07.850000: step 28040/119245 (epoch 9/35), loss = 0.066832 (0.653 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:58:17.701344: step 28060/119245 (epoch 9/35), loss = 0.011359 (0.464 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:58:26.470682: step 28080/119245 (epoch 9/35), loss = 0.005037 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:58:36.141076: step 28100/119245 (epoch 9/35), loss = 0.246308 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:58:46.148867: step 28120/119245 (epoch 9/35), loss = 0.245915 (0.353 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:58:56.425499: step 28140/119245 (epoch 9/35), loss = 0.020075 (0.332 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:59:07.857803: step 28160/119245 (epoch 9/35), loss = 0.115989 (0.497 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:59:17.431927: step 28180/119245 (epoch 9/35), loss = 0.099105 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:59:26.200970: step 28200/119245 (epoch 9/35), loss = 0.003077 (0.503 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:59:34.976247: step 28220/119245 (epoch 9/35), loss = 0.040147 (0.461 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:59:43.469976: step 28240/119245 (epoch 9/35), loss = 0.210973 (0.538 sec/batch), lr: 0.030000\n",
            "2020-12-02 03:59:52.287722: step 28260/119245 (epoch 9/35), loss = 0.025080 (0.468 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:00:01.153690: step 28280/119245 (epoch 9/35), loss = 0.191217 (0.408 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:00:11.541901: step 28300/119245 (epoch 9/35), loss = 0.006240 (0.639 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:00:20.204015: step 28320/119245 (epoch 9/35), loss = 0.453229 (0.634 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:00:31.643753: step 28340/119245 (epoch 9/35), loss = 0.111391 (0.897 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:00:43.088849: step 28360/119245 (epoch 9/35), loss = 0.005599 (0.581 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:00:51.928685: step 28380/119245 (epoch 9/35), loss = 0.008739 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:01:02.069198: step 28400/119245 (epoch 9/35), loss = 0.111332 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:01:12.666296: step 28420/119245 (epoch 9/35), loss = 0.028753 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:01:21.612830: step 28440/119245 (epoch 9/35), loss = 0.067389 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:01:31.683866: step 28460/119245 (epoch 9/35), loss = 0.010652 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:01:41.254073: step 28480/119245 (epoch 9/35), loss = 0.107461 (0.386 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:01:50.425951: step 28500/119245 (epoch 9/35), loss = 0.115635 (0.504 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:02:00.129667: step 28520/119245 (epoch 9/35), loss = 0.075259 (0.362 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:02:11.806199: step 28540/119245 (epoch 9/35), loss = 0.082027 (0.431 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:02:23.882896: step 28560/119245 (epoch 9/35), loss = 0.175178 (0.561 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:02:34.029691: step 28580/119245 (epoch 9/35), loss = 0.002589 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:02:44.074671: step 28600/119245 (epoch 9/35), loss = 0.028443 (0.527 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:02:53.646785: step 28620/119245 (epoch 9/35), loss = 0.020850 (0.472 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:03:02.750037: step 28640/119245 (epoch 9/35), loss = 0.006431 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:03:11.604278: step 28660/119245 (epoch 9/35), loss = 0.067146 (0.480 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:03:21.920266: step 28680/119245 (epoch 9/35), loss = 0.200938 (0.481 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:03:31.499902: step 28700/119245 (epoch 9/35), loss = 0.160144 (0.599 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:03:41.469307: step 28720/119245 (epoch 9/35), loss = 0.068236 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:03:50.928483: step 28740/119245 (epoch 9/35), loss = 0.014260 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:03:59.646037: step 28760/119245 (epoch 9/35), loss = 0.045922 (0.377 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:04:09.630594: step 28780/119245 (epoch 9/35), loss = 0.011726 (0.422 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:04:19.874382: step 28800/119245 (epoch 9/35), loss = 0.003623 (0.395 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:04:29.791803: step 28820/119245 (epoch 9/35), loss = 0.009355 (0.635 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:04:39.576612: step 28840/119245 (epoch 9/35), loss = 0.005834 (0.452 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:04:48.780399: step 28860/119245 (epoch 9/35), loss = 0.020754 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:04:58.567239: step 28880/119245 (epoch 9/35), loss = 0.372508 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:05:09.455099: step 28900/119245 (epoch 9/35), loss = 0.003689 (1.536 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:05:18.661172: step 28920/119245 (epoch 9/35), loss = 0.064525 (0.434 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:05:28.148471: step 28940/119245 (epoch 9/35), loss = 0.033541 (0.277 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:05:37.700543: step 28960/119245 (epoch 9/35), loss = 0.443636 (0.313 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:05:48.650018: step 28980/119245 (epoch 9/35), loss = 0.060050 (0.391 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:05:58.743685: step 29000/119245 (epoch 9/35), loss = 0.333428 (0.401 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:06:08.554438: step 29020/119245 (epoch 9/35), loss = 0.091280 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:06:18.265060: step 29040/119245 (epoch 9/35), loss = 0.011061 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:06:27.825358: step 29060/119245 (epoch 9/35), loss = 0.007048 (0.494 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:06:37.233378: step 29080/119245 (epoch 9/35), loss = 0.016982 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:06:46.612611: step 29100/119245 (epoch 9/35), loss = 0.008104 (0.316 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:06:58.829423: step 29120/119245 (epoch 9/35), loss = 0.013882 (0.882 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:07:08.914856: step 29140/119245 (epoch 9/35), loss = 0.164448 (1.643 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:07:19.097992: step 29160/119245 (epoch 9/35), loss = 0.096855 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:07:27.542772: step 29180/119245 (epoch 9/35), loss = 0.001967 (0.353 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:07:39.041031: step 29200/119245 (epoch 9/35), loss = 0.168130 (0.550 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:07:49.985422: step 29220/119245 (epoch 9/35), loss = 0.242179 (0.882 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:08:00.185741: step 29240/119245 (epoch 9/35), loss = 0.047263 (0.339 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:08:10.323654: step 29260/119245 (epoch 9/35), loss = 0.012242 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:08:19.895128: step 29280/119245 (epoch 9/35), loss = 0.004188 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:08:30.404989: step 29300/119245 (epoch 9/35), loss = 0.376264 (0.343 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:08:39.105835: step 29320/119245 (epoch 9/35), loss = 0.150999 (0.301 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:08:49.725519: step 29340/119245 (epoch 9/35), loss = 0.145092 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:08:59.414846: step 29360/119245 (epoch 9/35), loss = 0.125552 (0.661 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:09:09.546900: step 29380/119245 (epoch 9/35), loss = 0.098138 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:09:18.807421: step 29400/119245 (epoch 9/35), loss = 0.008389 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:09:28.749940: step 29420/119245 (epoch 9/35), loss = 0.014931 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:09:38.708642: step 29440/119245 (epoch 9/35), loss = 0.146554 (0.446 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:09:49.161147: step 29460/119245 (epoch 9/35), loss = 0.014435 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:09:59.429994: step 29480/119245 (epoch 9/35), loss = 0.089394 (0.381 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:10:08.890248: step 29500/119245 (epoch 9/35), loss = 0.015311 (0.540 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:10:18.621431: step 29520/119245 (epoch 9/35), loss = 0.176491 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:10:27.404258: step 29540/119245 (epoch 9/35), loss = 0.015469 (0.350 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:10:37.253975: step 29560/119245 (epoch 9/35), loss = 0.001036 (0.377 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:10:47.963479: step 29580/119245 (epoch 9/35), loss = 0.015338 (0.569 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:10:57.329770: step 29600/119245 (epoch 9/35), loss = 0.024026 (0.452 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:11:08.996656: step 29620/119245 (epoch 9/35), loss = 0.012125 (0.527 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:11:19.321960: step 29640/119245 (epoch 9/35), loss = 0.064617 (0.450 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:11:28.802198: step 29660/119245 (epoch 9/35), loss = 0.059817 (0.538 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:11:38.033024: step 29680/119245 (epoch 9/35), loss = 0.566106 (0.445 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:11:48.804660: step 29700/119245 (epoch 9/35), loss = 0.022755 (0.757 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:11:58.918691: step 29720/119245 (epoch 9/35), loss = 0.286691 (1.029 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:12:08.614054: step 29740/119245 (epoch 9/35), loss = 0.508415 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:12:18.815967: step 29760/119245 (epoch 9/35), loss = 0.106273 (0.541 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:12:28.278121: step 29780/119245 (epoch 9/35), loss = 0.003777 (0.520 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:12:37.087423: step 29800/119245 (epoch 9/35), loss = 0.033940 (0.523 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:12:46.820355: step 29820/119245 (epoch 9/35), loss = 0.002323 (0.451 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:12:56.630115: step 29840/119245 (epoch 9/35), loss = 0.001883 (0.707 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:13:05.417397: step 29860/119245 (epoch 9/35), loss = 0.063603 (0.819 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:13:16.137193: step 29880/119245 (epoch 9/35), loss = 0.401180 (0.402 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:13:25.088858: step 29900/119245 (epoch 9/35), loss = 0.014522 (0.467 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:13:34.176090: step 29920/119245 (epoch 9/35), loss = 0.325830 (0.486 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:13:43.253591: step 29940/119245 (epoch 9/35), loss = 0.004841 (1.011 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:13:53.322488: step 29960/119245 (epoch 9/35), loss = 0.010633 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:14:03.837279: step 29980/119245 (epoch 9/35), loss = 0.037094 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:14:13.686568: step 30000/119245 (epoch 9/35), loss = 0.043596 (1.050 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:14:23.312307: step 30020/119245 (epoch 9/35), loss = 0.001549 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:14:32.775410: step 30040/119245 (epoch 9/35), loss = 0.006934 (0.644 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:14:42.446589: step 30060/119245 (epoch 9/35), loss = 0.096036 (0.359 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:14:52.334575: step 30080/119245 (epoch 9/35), loss = 0.292348 (0.622 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:15:01.514010: step 30100/119245 (epoch 9/35), loss = 0.009762 (0.491 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:15:11.495030: step 30120/119245 (epoch 9/35), loss = 0.012011 (0.634 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:15:20.778105: step 30140/119245 (epoch 9/35), loss = 0.059224 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:15:30.174547: step 30160/119245 (epoch 9/35), loss = 0.008271 (0.493 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:15:39.251806: step 30180/119245 (epoch 9/35), loss = 0.181012 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:15:48.567987: step 30200/119245 (epoch 9/35), loss = 0.086581 (0.410 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:16:00.225534: step 30220/119245 (epoch 9/35), loss = 0.043536 (0.502 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:16:08.954686: step 30240/119245 (epoch 9/35), loss = 0.074010 (0.444 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:16:18.548269: step 30260/119245 (epoch 9/35), loss = 0.074191 (0.550 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:16:29.348974: step 30280/119245 (epoch 9/35), loss = 0.005189 (0.544 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:16:38.841702: step 30300/119245 (epoch 9/35), loss = 0.007536 (0.521 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:16:48.806374: step 30320/119245 (epoch 9/35), loss = 0.273769 (0.586 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:16:58.918537: step 30340/119245 (epoch 9/35), loss = 0.014525 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:17:08.841431: step 30360/119245 (epoch 9/35), loss = 0.064014 (0.525 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:17:18.936229: step 30380/119245 (epoch 9/35), loss = 0.318974 (0.479 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:17:28.158317: step 30400/119245 (epoch 9/35), loss = 0.209515 (0.455 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:17:37.917417: step 30420/119245 (epoch 9/35), loss = 0.039228 (0.420 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:17:46.404549: step 30440/119245 (epoch 9/35), loss = 0.073968 (0.586 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:17:57.162055: step 30460/119245 (epoch 9/35), loss = 0.018956 (0.607 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:18:06.208737: step 30480/119245 (epoch 9/35), loss = 0.158349 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:18:15.003739: step 30500/119245 (epoch 9/35), loss = 0.005406 (0.380 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:18:23.920725: step 30520/119245 (epoch 9/35), loss = 0.021122 (0.567 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:18:33.920854: step 30540/119245 (epoch 9/35), loss = 0.302945 (0.333 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:18:43.062794: step 30560/119245 (epoch 9/35), loss = 0.008613 (0.344 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:18:54.135542: step 30580/119245 (epoch 9/35), loss = 0.005446 (0.509 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:19:03.785668: step 30600/119245 (epoch 9/35), loss = 0.175189 (0.550 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:19:13.374671: step 30620/119245 (epoch 9/35), loss = 0.007858 (0.315 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:19:23.108525: step 30640/119245 (epoch 9/35), loss = 0.010098 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:19:34.243678: step 30660/119245 (epoch 9/35), loss = 0.007532 (0.378 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  79.13%  R:  80.77%  F1:  79.94%  #: 338\n",
            "org:city_of_headquarters             P:  78.26%  R:  33.03%  F1:  46.45%  #: 109\n",
            "org:country_of_headquarters          P:  54.02%  R:  53.11%  F1:  53.56%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  82.35%  R:  73.68%  F1:  77.78%  #: 38\n",
            "org:founded_by                       P:  72.73%  R:  52.63%  F1:  61.07%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  61.63%  R:  62.35%  F1:  61.99%  #: 85\n",
            "org:number_of_employees/members      P: 100.00%  R:  62.96%  F1:  77.27%  #: 27\n",
            "org:parents                          P:  38.16%  R:  30.21%  F1:  33.72%  #: 96\n",
            "org:political/religious_affiliation  P:  22.22%  R:  20.00%  F1:  21.05%  #: 10\n",
            "org:shareholders                     P:  30.23%  R:  23.64%  F1:  26.53%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  62.50%  R:  64.29%  F1:  63.38%  #: 70\n",
            "org:subsidiaries                     P:  53.19%  R:  22.12%  F1:  31.25%  #: 113\n",
            "org:top_members/employees            P:  79.23%  R:  72.85%  F1:  75.90%  #: 534\n",
            "org:website                          P:  89.25%  R:  96.51%  F1:  92.74%  #: 86\n",
            "per:age                              P:  85.02%  R:  93.42%  F1:  89.02%  #: 243\n",
            "per:alternate_names                  P:  70.00%  R:  36.84%  F1:  48.28%  #: 38\n",
            "per:cause_of_death                   P:  81.21%  R:  72.02%  F1:  76.34%  #: 168\n",
            "per:charges                          P:  78.49%  R:  69.52%  F1:  73.74%  #: 105\n",
            "per:children                         P:  72.73%  R:  56.57%  F1:  63.64%  #: 99\n",
            "per:cities_of_residence              P:  46.70%  R:  47.49%  F1:  47.09%  #: 179\n",
            "per:city_of_birth                    P:  77.14%  R:  81.82%  F1:  79.41%  #: 33\n",
            "per:city_of_death                    P:  72.09%  R:  52.54%  F1:  60.78%  #: 118\n",
            "per:countries_of_residence           P:  37.12%  R:  59.29%  F1:  45.66%  #: 226\n",
            "per:country_of_birth                 P:  85.71%  R:  30.00%  F1:  44.44%  #: 20\n",
            "per:country_of_death                 P:  88.89%  R:  17.39%  F1:  29.09%  #: 46\n",
            "per:date_of_birth                    P:  84.85%  R:  90.32%  F1:  87.50%  #: 31\n",
            "per:date_of_death                    P:  77.84%  R:  66.50%  F1:  71.73%  #: 206\n",
            "per:employee_of                      P:  58.20%  R:  56.80%  F1:  57.49%  #: 375\n",
            "per:origin                           P:  53.66%  R:  62.86%  F1:  57.89%  #: 210\n",
            "per:other_family                     P:  35.71%  R:   6.25%  F1:  10.64%  #: 80\n",
            "per:parents                          P:  64.10%  R:  44.64%  F1:  52.63%  #: 56\n",
            "per:religion                         P:  54.69%  R:  66.04%  F1:  59.83%  #: 53\n",
            "per:schools_attended                 P:  70.73%  R:  58.00%  F1:  63.74%  #: 50\n",
            "per:siblings                         P:  72.00%  R:  60.00%  F1:  65.45%  #: 30\n",
            "per:spouse                           P:  67.52%  R:  66.67%  F1:  67.09%  #: 159\n",
            "per:stateorprovince_of_birth         P:  65.38%  R:  65.38%  F1:  65.38%  #: 26\n",
            "per:stateorprovince_of_death         P:  69.23%  R:  65.85%  F1:  67.50%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  39.81%  R:  59.72%  F1:  47.78%  #: 72\n",
            "per:title                            P:  78.27%  R:  79.98%  F1:  79.12%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17478 Guess as no relation\n",
            "5153 guess\n",
            "3490 correct\n",
            "5436 gold\n",
            "Precision (micro): 67.728%\n",
            "   Recall (micro): 64.202%\n",
            "       F1 (micro): 65.917%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 9: train_loss = 0.111000, dev_loss = 0.701308, dev_f1 = 0.6592\n",
            "model saved to ./saved_models/00/checkpoint_epoch_9.pt\n",
            "new best model saved.\n",
            "\n",
            "2020-12-02 04:22:35.332634: step 30680/119245 (epoch 10/35), loss = 0.188126 (0.482 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:22:46.568313: step 30700/119245 (epoch 10/35), loss = 0.077744 (0.545 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:22:56.149821: step 30720/119245 (epoch 10/35), loss = 0.011636 (0.365 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:23:06.911243: step 30740/119245 (epoch 10/35), loss = 0.151570 (0.401 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:23:15.684146: step 30760/119245 (epoch 10/35), loss = 0.093075 (0.490 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:23:26.071362: step 30780/119245 (epoch 10/35), loss = 0.040288 (0.996 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:23:35.277137: step 30800/119245 (epoch 10/35), loss = 0.169110 (0.386 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:23:44.026040: step 30820/119245 (epoch 10/35), loss = 0.003659 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:23:53.203511: step 30840/119245 (epoch 10/35), loss = 0.316013 (0.489 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:24:03.303966: step 30860/119245 (epoch 10/35), loss = 0.004744 (0.511 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:24:12.761872: step 30880/119245 (epoch 10/35), loss = 0.056543 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:24:21.722454: step 30900/119245 (epoch 10/35), loss = 0.002090 (0.294 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:24:31.795060: step 30920/119245 (epoch 10/35), loss = 0.000768 (0.533 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:24:42.044665: step 30940/119245 (epoch 10/35), loss = 0.043917 (0.374 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:24:50.378786: step 30960/119245 (epoch 10/35), loss = 0.060705 (0.371 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:24:59.862244: step 30980/119245 (epoch 10/35), loss = 0.025507 (0.496 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:25:10.767419: step 31000/119245 (epoch 10/35), loss = 0.027476 (0.437 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:25:21.984538: step 31020/119245 (epoch 10/35), loss = 0.254822 (0.509 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:25:31.191944: step 31040/119245 (epoch 10/35), loss = 0.004055 (0.618 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:25:40.098843: step 31060/119245 (epoch 10/35), loss = 0.021984 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:25:51.187108: step 31080/119245 (epoch 10/35), loss = 0.013909 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:26:00.428391: step 31100/119245 (epoch 10/35), loss = 0.327702 (0.548 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:26:12.937232: step 31120/119245 (epoch 10/35), loss = 0.017957 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:26:22.144539: step 31140/119245 (epoch 10/35), loss = 0.000949 (0.379 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:26:32.377410: step 31160/119245 (epoch 10/35), loss = 0.001976 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:26:42.643435: step 31180/119245 (epoch 10/35), loss = 0.009967 (0.489 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:26:52.582511: step 31200/119245 (epoch 10/35), loss = 0.164823 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:27:02.492607: step 31220/119245 (epoch 10/35), loss = 0.240767 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:27:12.217035: step 31240/119245 (epoch 10/35), loss = 0.072831 (0.438 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:27:20.842036: step 31260/119245 (epoch 10/35), loss = 0.590113 (0.328 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:27:29.806591: step 31280/119245 (epoch 10/35), loss = 0.001024 (0.381 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:27:39.303607: step 31300/119245 (epoch 10/35), loss = 0.031257 (0.355 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:27:48.685229: step 31320/119245 (epoch 10/35), loss = 0.005514 (0.444 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:27:59.794086: step 31340/119245 (epoch 10/35), loss = 0.264951 (0.618 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:28:09.828574: step 31360/119245 (epoch 10/35), loss = 0.399702 (0.456 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:28:19.563243: step 31380/119245 (epoch 10/35), loss = 0.012496 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:28:29.601819: step 31400/119245 (epoch 10/35), loss = 0.176088 (0.407 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:28:39.052958: step 31420/119245 (epoch 10/35), loss = 0.487287 (0.495 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:28:48.848756: step 31440/119245 (epoch 10/35), loss = 0.052633 (1.003 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:28:57.747273: step 31460/119245 (epoch 10/35), loss = 0.098077 (0.323 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:29:07.862933: step 31480/119245 (epoch 10/35), loss = 0.331740 (0.533 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:29:17.020515: step 31500/119245 (epoch 10/35), loss = 0.049247 (0.603 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:29:26.298780: step 31520/119245 (epoch 10/35), loss = 0.001700 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:29:37.313008: step 31540/119245 (epoch 10/35), loss = 0.040523 (0.558 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:29:47.905580: step 31560/119245 (epoch 10/35), loss = 0.021192 (0.537 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:29:58.590870: step 31580/119245 (epoch 10/35), loss = 0.002242 (0.305 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:30:07.677616: step 31600/119245 (epoch 10/35), loss = 0.036671 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:30:16.313847: step 31620/119245 (epoch 10/35), loss = 0.420278 (0.371 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:30:24.825590: step 31640/119245 (epoch 10/35), loss = 0.003791 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:30:33.913302: step 31660/119245 (epoch 10/35), loss = 0.001770 (0.637 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:30:42.287426: step 31680/119245 (epoch 10/35), loss = 0.362334 (0.350 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:30:51.692860: step 31700/119245 (epoch 10/35), loss = 0.003808 (0.518 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:31:01.849219: step 31720/119245 (epoch 10/35), loss = 0.075248 (0.473 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:31:11.972429: step 31740/119245 (epoch 10/35), loss = 0.013069 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:31:24.726308: step 31760/119245 (epoch 10/35), loss = 0.002459 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:31:33.563696: step 31780/119245 (epoch 10/35), loss = 0.001273 (0.498 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:31:43.173662: step 31800/119245 (epoch 10/35), loss = 0.001084 (0.615 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:31:53.552356: step 31820/119245 (epoch 10/35), loss = 0.014123 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:32:03.475637: step 31840/119245 (epoch 10/35), loss = 0.184857 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:32:13.112633: step 31860/119245 (epoch 10/35), loss = 0.044937 (0.435 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:32:22.393930: step 31880/119245 (epoch 10/35), loss = 0.235265 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:32:31.713977: step 31900/119245 (epoch 10/35), loss = 0.021591 (0.306 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:32:41.011274: step 31920/119245 (epoch 10/35), loss = 0.005709 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:32:51.179819: step 31940/119245 (epoch 10/35), loss = 0.097399 (0.333 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:33:04.648891: step 31960/119245 (epoch 10/35), loss = 0.003220 (0.554 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:33:15.495868: step 31980/119245 (epoch 10/35), loss = 0.224975 (0.958 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:33:25.494597: step 32000/119245 (epoch 10/35), loss = 0.002664 (0.656 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:33:34.929942: step 32020/119245 (epoch 10/35), loss = 0.019974 (0.406 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:33:44.273909: step 32040/119245 (epoch 10/35), loss = 0.123314 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:33:53.567370: step 32060/119245 (epoch 10/35), loss = 0.018717 (0.594 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:34:03.376271: step 32080/119245 (epoch 10/35), loss = 0.447464 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:34:13.052332: step 32100/119245 (epoch 10/35), loss = 0.018938 (0.498 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:34:22.709681: step 32120/119245 (epoch 10/35), loss = 0.015289 (0.741 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:34:32.789039: step 32140/119245 (epoch 10/35), loss = 0.041921 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:34:41.511368: step 32160/119245 (epoch 10/35), loss = 0.387146 (0.333 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:34:50.665529: step 32180/119245 (epoch 10/35), loss = 0.044248 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:35:01.761831: step 32200/119245 (epoch 10/35), loss = 0.056321 (0.499 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:35:10.651272: step 32220/119245 (epoch 10/35), loss = 0.018295 (0.312 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:35:20.804104: step 32240/119245 (epoch 10/35), loss = 0.099793 (0.424 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:35:30.362639: step 32260/119245 (epoch 10/35), loss = 0.324047 (0.381 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:35:40.385881: step 32280/119245 (epoch 10/35), loss = 0.023587 (0.372 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:35:50.371161: step 32300/119245 (epoch 10/35), loss = 0.025714 (0.516 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:36:00.012148: step 32320/119245 (epoch 10/35), loss = 0.018318 (0.340 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:36:09.704886: step 32340/119245 (epoch 10/35), loss = 0.027607 (0.379 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:36:18.874649: step 32360/119245 (epoch 10/35), loss = 0.000486 (0.504 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:36:29.713511: step 32380/119245 (epoch 10/35), loss = 0.020064 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:36:40.580056: step 32400/119245 (epoch 10/35), loss = 0.000495 (0.596 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:36:50.448460: step 32420/119245 (epoch 10/35), loss = 0.000938 (0.655 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:36:59.453673: step 32440/119245 (epoch 10/35), loss = 0.029722 (0.645 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:37:09.855898: step 32460/119245 (epoch 10/35), loss = 0.018826 (0.617 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:37:19.156130: step 32480/119245 (epoch 10/35), loss = 0.109651 (0.398 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:37:28.558114: step 32500/119245 (epoch 10/35), loss = 0.007781 (0.468 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:37:40.284185: step 32520/119245 (epoch 10/35), loss = 0.024801 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:37:49.530964: step 32540/119245 (epoch 10/35), loss = 0.194385 (0.422 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:38:00.606172: step 32560/119245 (epoch 10/35), loss = 0.026821 (0.348 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:38:09.807704: step 32580/119245 (epoch 10/35), loss = 0.416622 (0.313 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:38:20.172057: step 32600/119245 (epoch 10/35), loss = 0.334091 (0.640 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:38:31.141833: step 32620/119245 (epoch 10/35), loss = 0.081460 (0.395 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:38:41.931359: step 32640/119245 (epoch 10/35), loss = 0.158996 (0.584 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:38:51.991063: step 32660/119245 (epoch 10/35), loss = 0.021930 (0.359 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:39:02.100790: step 32680/119245 (epoch 10/35), loss = 0.006433 (0.507 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:39:11.126172: step 32700/119245 (epoch 10/35), loss = 0.002487 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:39:21.096759: step 32720/119245 (epoch 10/35), loss = 0.165701 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:39:31.227787: step 32740/119245 (epoch 10/35), loss = 0.327298 (0.361 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:39:40.863093: step 32760/119245 (epoch 10/35), loss = 0.016750 (0.331 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:39:50.911443: step 32780/119245 (epoch 10/35), loss = 0.003017 (0.312 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:40:00.224184: step 32800/119245 (epoch 10/35), loss = 0.012023 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:40:10.484312: step 32820/119245 (epoch 10/35), loss = 0.146652 (0.604 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:40:20.350650: step 32840/119245 (epoch 10/35), loss = 0.340093 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:40:30.241864: step 32860/119245 (epoch 10/35), loss = 0.079793 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:40:40.769200: step 32880/119245 (epoch 10/35), loss = 0.123013 (0.589 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:40:50.761551: step 32900/119245 (epoch 10/35), loss = 0.035338 (0.393 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:40:59.881803: step 32920/119245 (epoch 10/35), loss = 0.011099 (0.768 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:41:09.587249: step 32940/119245 (epoch 10/35), loss = 0.013978 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:41:19.229410: step 32960/119245 (epoch 10/35), loss = 0.112708 (0.541 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:41:28.327571: step 32980/119245 (epoch 10/35), loss = 0.115348 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:41:38.999164: step 33000/119245 (epoch 10/35), loss = 0.111524 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:41:50.654256: step 33020/119245 (epoch 10/35), loss = 0.060292 (0.878 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:42:00.556156: step 33040/119245 (epoch 10/35), loss = 0.026043 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:42:10.627627: step 33060/119245 (epoch 10/35), loss = 0.161702 (0.506 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:42:19.661459: step 33080/119245 (epoch 10/35), loss = 0.008433 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:42:30.385439: step 33100/119245 (epoch 10/35), loss = 0.029472 (1.512 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:42:40.377333: step 33120/119245 (epoch 10/35), loss = 0.001279 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:42:50.602543: step 33140/119245 (epoch 10/35), loss = 0.039927 (0.585 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:43:00.041533: step 33160/119245 (epoch 10/35), loss = 0.013579 (0.446 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:43:09.739973: step 33180/119245 (epoch 10/35), loss = 0.207378 (0.463 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:43:18.724108: step 33200/119245 (epoch 10/35), loss = 0.009251 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:43:28.178268: step 33220/119245 (epoch 10/35), loss = 0.004162 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:43:38.539018: step 33240/119245 (epoch 10/35), loss = 0.136382 (0.580 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:43:47.045578: step 33260/119245 (epoch 10/35), loss = 0.066434 (0.367 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:43:56.671898: step 33280/119245 (epoch 10/35), loss = 0.038910 (0.418 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:44:06.963205: step 33300/119245 (epoch 10/35), loss = 0.149007 (0.512 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:44:15.652482: step 33320/119245 (epoch 10/35), loss = 0.004905 (0.495 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:44:24.495200: step 33340/119245 (epoch 10/35), loss = 0.024481 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:44:33.987355: step 33360/119245 (epoch 10/35), loss = 0.176680 (0.483 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:44:44.500730: step 33380/119245 (epoch 10/35), loss = 0.517203 (0.694 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:44:54.611503: step 33400/119245 (epoch 10/35), loss = 0.003925 (0.515 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:45:04.975900: step 33420/119245 (epoch 10/35), loss = 0.008138 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:45:14.015357: step 33440/119245 (epoch 10/35), loss = 0.002974 (0.316 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:45:23.645373: step 33460/119245 (epoch 10/35), loss = 0.110740 (0.647 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:45:32.890618: step 33480/119245 (epoch 10/35), loss = 0.080218 (0.378 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:45:42.819730: step 33500/119245 (epoch 10/35), loss = 0.030770 (0.555 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:45:53.226610: step 33520/119245 (epoch 10/35), loss = 0.005073 (0.739 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:46:02.251466: step 33540/119245 (epoch 10/35), loss = 0.086055 (0.384 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:46:11.732611: step 33560/119245 (epoch 10/35), loss = 0.070800 (0.425 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:46:20.733807: step 33580/119245 (epoch 10/35), loss = 0.083994 (0.489 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:46:29.532928: step 33600/119245 (epoch 10/35), loss = 0.060062 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:46:40.825949: step 33620/119245 (epoch 10/35), loss = 0.066802 (0.329 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:46:50.227602: step 33640/119245 (epoch 10/35), loss = 0.011888 (0.505 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:46:59.905496: step 33660/119245 (epoch 10/35), loss = 0.005740 (0.899 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:47:10.605786: step 33680/119245 (epoch 10/35), loss = 0.159001 (0.435 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:47:19.860130: step 33700/119245 (epoch 10/35), loss = 0.005150 (0.501 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:47:29.169011: step 33720/119245 (epoch 10/35), loss = 0.036916 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:47:39.958195: step 33740/119245 (epoch 10/35), loss = 0.015175 (0.334 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:47:49.719359: step 33760/119245 (epoch 10/35), loss = 0.051026 (0.614 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:47:59.824208: step 33780/119245 (epoch 10/35), loss = 0.146372 (0.520 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:48:09.392586: step 33800/119245 (epoch 10/35), loss = 0.598265 (0.438 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:48:19.058137: step 33820/119245 (epoch 10/35), loss = 0.123211 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:48:27.030241: step 33840/119245 (epoch 10/35), loss = 0.090437 (0.329 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:48:36.298673: step 33860/119245 (epoch 10/35), loss = 0.003883 (0.336 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:48:47.069652: step 33880/119245 (epoch 10/35), loss = 0.000710 (0.373 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:48:56.161841: step 33900/119245 (epoch 10/35), loss = 0.525806 (0.444 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:49:04.644065: step 33920/119245 (epoch 10/35), loss = 0.005439 (0.440 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:49:13.827285: step 33940/119245 (epoch 10/35), loss = 0.314088 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:49:23.907725: step 33960/119245 (epoch 10/35), loss = 0.298259 (0.325 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:49:34.570273: step 33980/119245 (epoch 10/35), loss = 0.315543 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:49:44.434881: step 34000/119245 (epoch 10/35), loss = 0.019434 (0.463 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:49:54.289318: step 34020/119245 (epoch 10/35), loss = 0.014453 (0.528 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:50:03.600108: step 34040/119245 (epoch 10/35), loss = 0.037432 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:50:15.002033: step 34060/119245 (epoch 10/35), loss = 0.018486 (0.446 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  76.57%  R:  79.29%  F1:  77.91%  #: 338\n",
            "org:city_of_headquarters             P:  68.33%  R:  37.61%  F1:  48.52%  #: 109\n",
            "org:country_of_headquarters          P:  52.94%  R:  50.85%  F1:  51.87%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  86.67%  R:  68.42%  F1:  76.47%  #: 38\n",
            "org:founded_by                       P:  71.15%  R:  48.68%  F1:  57.81%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  64.37%  R:  65.88%  F1:  65.12%  #: 85\n",
            "org:number_of_employees/members      P:  59.38%  R:  70.37%  F1:  64.41%  #: 27\n",
            "org:parents                          P:  27.43%  R:  32.29%  F1:  29.67%  #: 96\n",
            "org:political/religious_affiliation  P:  16.00%  R:  40.00%  F1:  22.86%  #: 10\n",
            "org:shareholders                     P:  53.33%  R:  14.55%  F1:  22.86%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  57.69%  R:  64.29%  F1:  60.81%  #: 70\n",
            "org:subsidiaries                     P:  42.86%  R:  10.62%  F1:  17.02%  #: 113\n",
            "org:top_members/employees            P:  76.95%  R:  70.04%  F1:  73.33%  #: 534\n",
            "org:website                          P:  80.19%  R:  98.84%  F1:  88.54%  #: 86\n",
            "per:age                              P:  81.99%  R:  91.77%  F1:  86.60%  #: 243\n",
            "per:alternate_names                  P:  71.43%  R:  26.32%  F1:  38.46%  #: 38\n",
            "per:cause_of_death                   P:  85.25%  R:  61.90%  F1:  71.72%  #: 168\n",
            "per:charges                          P:  75.76%  R:  71.43%  F1:  73.53%  #: 105\n",
            "per:children                         P:  68.60%  R:  59.60%  F1:  63.78%  #: 99\n",
            "per:cities_of_residence              P:  49.37%  R:  43.58%  F1:  46.29%  #: 179\n",
            "per:city_of_birth                    P:  76.92%  R:  60.61%  F1:  67.80%  #: 33\n",
            "per:city_of_death                    P:  72.73%  R:  61.02%  F1:  66.36%  #: 118\n",
            "per:countries_of_residence           P:  37.57%  R:  56.19%  F1:  45.04%  #: 226\n",
            "per:country_of_birth                 P:  75.00%  R:  30.00%  F1:  42.86%  #: 20\n",
            "per:country_of_death                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 46\n",
            "per:date_of_birth                    P: 100.00%  R:  87.10%  F1:  93.10%  #: 31\n",
            "per:date_of_death                    P:  81.25%  R:  50.49%  F1:  62.28%  #: 206\n",
            "per:employee_of                      P:  59.54%  R:  62.40%  F1:  60.94%  #: 375\n",
            "per:origin                           P:  65.03%  R:  50.48%  F1:  56.84%  #: 210\n",
            "per:other_family                     P:  41.67%  R:   6.25%  F1:  10.87%  #: 80\n",
            "per:parents                          P:  67.57%  R:  44.64%  F1:  53.76%  #: 56\n",
            "per:religion                         P:  58.06%  R:  67.92%  F1:  62.61%  #: 53\n",
            "per:schools_attended                 P:  82.35%  R:  56.00%  F1:  66.67%  #: 50\n",
            "per:siblings                         P:  64.29%  R:  60.00%  F1:  62.07%  #: 30\n",
            "per:spouse                           P:  74.82%  R:  65.41%  F1:  69.80%  #: 159\n",
            "per:stateorprovince_of_birth         P:  77.78%  R:  53.85%  F1:  63.64%  #: 26\n",
            "per:stateorprovince_of_death         P:  55.56%  R:  12.20%  F1:  20.00%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  37.80%  R:  66.67%  F1:  48.24%  #: 72\n",
            "per:title                            P:  77.01%  R:  84.55%  F1:  80.60%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17582 Guess as no relation\n",
            "5049 guess\n",
            "3401 correct\n",
            "5436 gold\n",
            "Precision (micro): 67.360%\n",
            "   Recall (micro): 62.564%\n",
            "       F1 (micro): 64.874%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 10: train_loss = 0.092653, dev_loss = 0.779882, dev_f1 = 0.6487\n",
            "model saved to ./saved_models/00/checkpoint_epoch_10.pt\n",
            "\n",
            "2020-12-02 04:53:12.616403: step 34080/119245 (epoch 11/35), loss = 0.000780 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:53:23.938900: step 34100/119245 (epoch 11/35), loss = 0.053094 (0.975 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:53:33.843846: step 34120/119245 (epoch 11/35), loss = 0.027224 (0.352 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:53:42.797254: step 34140/119245 (epoch 11/35), loss = 0.026753 (0.418 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:53:52.624649: step 34160/119245 (epoch 11/35), loss = 0.003179 (0.412 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:54:02.520798: step 34180/119245 (epoch 11/35), loss = 0.213848 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:54:12.349054: step 34200/119245 (epoch 11/35), loss = 0.013044 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:54:21.212187: step 34220/119245 (epoch 11/35), loss = 0.315419 (0.438 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:54:30.112858: step 34240/119245 (epoch 11/35), loss = 0.093856 (0.454 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:54:40.206566: step 34260/119245 (epoch 11/35), loss = 0.110981 (0.559 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:54:50.192410: step 34280/119245 (epoch 11/35), loss = 0.139944 (0.624 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:54:58.962378: step 34300/119245 (epoch 11/35), loss = 0.002607 (0.338 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:55:08.859566: step 34320/119245 (epoch 11/35), loss = 0.001712 (0.355 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:55:19.269463: step 34340/119245 (epoch 11/35), loss = 0.005210 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:55:27.493601: step 34360/119245 (epoch 11/35), loss = 0.225293 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:55:36.594948: step 34380/119245 (epoch 11/35), loss = 0.002710 (0.414 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:55:47.761413: step 34400/119245 (epoch 11/35), loss = 0.006589 (0.487 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:55:57.215519: step 34420/119245 (epoch 11/35), loss = 0.354505 (0.477 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:56:08.228479: step 34440/119245 (epoch 11/35), loss = 0.061029 (0.360 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:56:17.406830: step 34460/119245 (epoch 11/35), loss = 0.470728 (0.322 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:56:28.217872: step 34480/119245 (epoch 11/35), loss = 0.068012 (0.483 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:56:37.064962: step 34500/119245 (epoch 11/35), loss = 0.012376 (0.412 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:56:49.134240: step 34520/119245 (epoch 11/35), loss = 0.002272 (0.482 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:56:58.953073: step 34540/119245 (epoch 11/35), loss = 0.080705 (0.348 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:57:09.265757: step 34560/119245 (epoch 11/35), loss = 0.014997 (0.662 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:57:19.610957: step 34580/119245 (epoch 11/35), loss = 0.012832 (0.349 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:57:28.953798: step 34600/119245 (epoch 11/35), loss = 0.033005 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:57:39.777531: step 34620/119245 (epoch 11/35), loss = 0.024977 (0.378 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:57:48.888865: step 34640/119245 (epoch 11/35), loss = 0.446181 (0.432 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:57:57.932783: step 34660/119245 (epoch 11/35), loss = 0.017291 (0.477 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:58:06.702243: step 34680/119245 (epoch 11/35), loss = 0.027996 (0.341 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:58:16.051279: step 34700/119245 (epoch 11/35), loss = 0.073935 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:58:25.215350: step 34720/119245 (epoch 11/35), loss = 0.003064 (0.381 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:58:36.816190: step 34740/119245 (epoch 11/35), loss = 0.002398 (1.628 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:58:46.508356: step 34760/119245 (epoch 11/35), loss = 0.461286 (0.329 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:58:56.366258: step 34780/119245 (epoch 11/35), loss = 0.051437 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:59:06.165910: step 34800/119245 (epoch 11/35), loss = 0.059799 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:59:15.223963: step 34820/119245 (epoch 11/35), loss = 0.005009 (0.543 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:59:25.023383: step 34840/119245 (epoch 11/35), loss = 0.022085 (0.465 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:59:34.343489: step 34860/119245 (epoch 11/35), loss = 0.161009 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:59:44.158398: step 34880/119245 (epoch 11/35), loss = 0.304251 (0.361 sec/batch), lr: 0.030000\n",
            "2020-12-02 04:59:53.364214: step 34900/119245 (epoch 11/35), loss = 0.425104 (0.496 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:00:02.494388: step 34920/119245 (epoch 11/35), loss = 0.003374 (0.276 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:00:12.932204: step 34940/119245 (epoch 11/35), loss = 0.005261 (0.376 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:00:24.591589: step 34960/119245 (epoch 11/35), loss = 0.045722 (0.321 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:00:34.916577: step 34980/119245 (epoch 11/35), loss = 0.172478 (0.643 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:00:43.901421: step 35000/119245 (epoch 11/35), loss = 0.002091 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:00:52.892398: step 35020/119245 (epoch 11/35), loss = 0.074307 (0.326 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:01:01.541296: step 35040/119245 (epoch 11/35), loss = 0.005975 (0.371 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:01:10.007632: step 35060/119245 (epoch 11/35), loss = 0.003388 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:01:18.534529: step 35080/119245 (epoch 11/35), loss = 0.003949 (0.378 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:01:27.452701: step 35100/119245 (epoch 11/35), loss = 0.011901 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:01:38.267802: step 35120/119245 (epoch 11/35), loss = 0.000509 (0.420 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:01:47.886131: step 35140/119245 (epoch 11/35), loss = 0.057326 (0.392 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:02:00.371564: step 35160/119245 (epoch 11/35), loss = 0.033915 (1.514 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:02:09.587667: step 35180/119245 (epoch 11/35), loss = 0.351452 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:02:18.514540: step 35200/119245 (epoch 11/35), loss = 0.004195 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:02:29.469496: step 35220/119245 (epoch 11/35), loss = 0.331570 (0.839 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:02:38.877857: step 35240/119245 (epoch 11/35), loss = 0.004328 (0.327 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:02:48.406964: step 35260/119245 (epoch 11/35), loss = 0.001965 (0.556 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:02:58.626864: step 35280/119245 (epoch 11/35), loss = 0.138261 (0.482 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:03:07.931167: step 35300/119245 (epoch 11/35), loss = 0.002060 (0.462 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:03:16.884044: step 35320/119245 (epoch 11/35), loss = 0.630055 (0.331 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:03:27.212161: step 35340/119245 (epoch 11/35), loss = 0.023050 (0.542 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:03:39.389288: step 35360/119245 (epoch 11/35), loss = 0.005453 (0.425 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:03:50.807386: step 35380/119245 (epoch 11/35), loss = 0.085083 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:04:01.358878: step 35400/119245 (epoch 11/35), loss = 0.013905 (0.582 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:04:10.797520: step 35420/119245 (epoch 11/35), loss = 0.002554 (0.763 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:04:20.161565: step 35440/119245 (epoch 11/35), loss = 0.000897 (0.339 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:04:29.244200: step 35460/119245 (epoch 11/35), loss = 0.002027 (0.452 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:04:38.797404: step 35480/119245 (epoch 11/35), loss = 0.004753 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:04:48.644146: step 35500/119245 (epoch 11/35), loss = 0.030210 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:04:58.091314: step 35520/119245 (epoch 11/35), loss = 0.008312 (0.313 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:05:08.236951: step 35540/119245 (epoch 11/35), loss = 0.110422 (0.544 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:05:17.174988: step 35560/119245 (epoch 11/35), loss = 0.057375 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:05:26.432457: step 35580/119245 (epoch 11/35), loss = 0.342167 (0.408 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:05:36.624208: step 35600/119245 (epoch 11/35), loss = 0.004415 (0.495 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:05:46.333490: step 35620/119245 (epoch 11/35), loss = 0.004936 (0.386 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:05:56.096218: step 35640/119245 (epoch 11/35), loss = 0.179712 (0.445 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:06:06.051314: step 35660/119245 (epoch 11/35), loss = 0.003423 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:06:15.744282: step 35680/119245 (epoch 11/35), loss = 0.399533 (0.359 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:06:25.392500: step 35700/119245 (epoch 11/35), loss = 0.022433 (0.386 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:06:35.712956: step 35720/119245 (epoch 11/35), loss = 0.001195 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:06:45.356292: step 35740/119245 (epoch 11/35), loss = 0.137681 (0.699 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:06:54.469690: step 35760/119245 (epoch 11/35), loss = 0.214101 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:07:04.306908: step 35780/119245 (epoch 11/35), loss = 0.019413 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:07:15.138113: step 35800/119245 (epoch 11/35), loss = 0.003177 (0.347 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:07:25.407715: step 35820/119245 (epoch 11/35), loss = 0.219297 (0.648 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:07:34.938787: step 35840/119245 (epoch 11/35), loss = 0.098643 (0.437 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:07:45.106412: step 35860/119245 (epoch 11/35), loss = 0.553889 (0.515 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:07:54.002314: step 35880/119245 (epoch 11/35), loss = 0.279680 (0.325 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:08:03.670708: step 35900/119245 (epoch 11/35), loss = 0.019606 (0.478 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:08:13.275939: step 35920/119245 (epoch 11/35), loss = 0.002343 (0.359 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:08:24.923735: step 35940/119245 (epoch 11/35), loss = 0.672884 (0.440 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:08:36.254545: step 35960/119245 (epoch 11/35), loss = 0.338175 (0.407 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:08:45.322320: step 35980/119245 (epoch 11/35), loss = 0.001052 (0.463 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:08:55.204092: step 36000/119245 (epoch 11/35), loss = 0.005538 (0.338 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:09:05.289356: step 36020/119245 (epoch 11/35), loss = 0.029329 (0.465 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:09:17.139590: step 36040/119245 (epoch 11/35), loss = 0.338879 (0.373 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:09:26.487555: step 36060/119245 (epoch 11/35), loss = 0.052883 (0.352 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:09:36.891771: step 36080/119245 (epoch 11/35), loss = 0.204715 (0.633 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:09:46.257756: step 36100/119245 (epoch 11/35), loss = 0.006733 (0.387 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:09:56.459382: step 36120/119245 (epoch 11/35), loss = 0.196839 (0.529 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:10:06.477008: step 36140/119245 (epoch 11/35), loss = 0.003648 (0.609 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:10:15.931109: step 36160/119245 (epoch 11/35), loss = 0.066962 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:10:25.990056: step 36180/119245 (epoch 11/35), loss = 0.001081 (1.002 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:10:35.181506: step 36200/119245 (epoch 11/35), loss = 0.019155 (0.349 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:10:44.685608: step 36220/119245 (epoch 11/35), loss = 0.179930 (0.587 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:10:55.823170: step 36240/119245 (epoch 11/35), loss = 0.007216 (0.316 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:11:05.123857: step 36260/119245 (epoch 11/35), loss = 0.139696 (0.539 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:11:15.492927: step 36280/119245 (epoch 11/35), loss = 0.471386 (0.961 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:11:26.058092: step 36300/119245 (epoch 11/35), loss = 0.007949 (0.439 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:11:34.644541: step 36320/119245 (epoch 11/35), loss = 0.001950 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:11:44.533787: step 36340/119245 (epoch 11/35), loss = 0.093499 (0.355 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:11:53.671551: step 36360/119245 (epoch 11/35), loss = 0.073850 (0.408 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:12:03.637775: step 36380/119245 (epoch 11/35), loss = 0.008445 (0.494 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:12:14.316818: step 36400/119245 (epoch 11/35), loss = 0.003378 (0.604 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:12:25.107179: step 36420/119245 (epoch 11/35), loss = 0.001044 (0.625 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:12:35.081401: step 36440/119245 (epoch 11/35), loss = 0.114381 (0.326 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:12:45.488999: step 36460/119245 (epoch 11/35), loss = 0.000427 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:12:54.602708: step 36480/119245 (epoch 11/35), loss = 0.011051 (0.433 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:13:04.359999: step 36500/119245 (epoch 11/35), loss = 0.001331 (0.548 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:13:15.122721: step 36520/119245 (epoch 11/35), loss = 0.215533 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:13:24.998808: step 36540/119245 (epoch 11/35), loss = 0.000408 (0.963 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:13:34.114174: step 36560/119245 (epoch 11/35), loss = 0.005256 (0.505 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:13:44.754773: step 36580/119245 (epoch 11/35), loss = 0.003274 (0.331 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:13:53.966202: step 36600/119245 (epoch 11/35), loss = 0.002546 (0.450 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:14:03.074400: step 36620/119245 (epoch 11/35), loss = 0.002590 (0.348 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:14:13.057444: step 36640/119245 (epoch 11/35), loss = 0.003702 (0.691 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:14:22.140187: step 36660/119245 (epoch 11/35), loss = 0.013630 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:14:31.060821: step 36680/119245 (epoch 11/35), loss = 0.006042 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:14:41.835351: step 36700/119245 (epoch 11/35), loss = 0.136388 (0.372 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:14:50.346023: step 36720/119245 (epoch 11/35), loss = 0.002212 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:14:59.511274: step 36740/119245 (epoch 11/35), loss = 0.371639 (0.353 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:15:09.092287: step 36760/119245 (epoch 11/35), loss = 0.006674 (0.486 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:15:19.612574: step 36780/119245 (epoch 11/35), loss = 0.038966 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:15:29.351158: step 36800/119245 (epoch 11/35), loss = 0.603962 (0.474 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:15:40.076390: step 36820/119245 (epoch 11/35), loss = 0.226549 (0.357 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:15:48.933479: step 36840/119245 (epoch 11/35), loss = 0.004011 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:15:58.562898: step 36860/119245 (epoch 11/35), loss = 0.005961 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:16:08.163639: step 36880/119245 (epoch 11/35), loss = 0.010065 (0.489 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:16:17.879477: step 36900/119245 (epoch 11/35), loss = 0.000421 (0.379 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:16:28.065594: step 36920/119245 (epoch 11/35), loss = 0.010527 (0.538 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:16:36.972721: step 36940/119245 (epoch 11/35), loss = 0.045257 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:16:46.417883: step 36960/119245 (epoch 11/35), loss = 0.028546 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:16:55.346814: step 36980/119245 (epoch 11/35), loss = 0.037390 (0.325 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:17:04.559666: step 37000/119245 (epoch 11/35), loss = 0.003348 (0.292 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:17:14.107582: step 37020/119245 (epoch 11/35), loss = 0.127476 (0.615 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:17:25.493280: step 37040/119245 (epoch 11/35), loss = 0.068383 (0.561 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:17:34.455387: step 37060/119245 (epoch 11/35), loss = 0.002826 (0.604 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:17:45.281792: step 37080/119245 (epoch 11/35), loss = 0.386578 (0.605 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:17:54.730355: step 37100/119245 (epoch 11/35), loss = 0.254431 (0.636 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:18:04.297321: step 37120/119245 (epoch 11/35), loss = 0.025540 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:18:15.249814: step 37140/119245 (epoch 11/35), loss = 0.043250 (0.455 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:18:24.148009: step 37160/119245 (epoch 11/35), loss = 0.008839 (0.365 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:18:34.585378: step 37180/119245 (epoch 11/35), loss = 0.022394 (0.627 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:18:44.095322: step 37200/119245 (epoch 11/35), loss = 0.272984 (0.313 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:18:53.683784: step 37220/119245 (epoch 11/35), loss = 0.355337 (0.481 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:19:02.705103: step 37240/119245 (epoch 11/35), loss = 0.166525 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:19:11.621419: step 37260/119245 (epoch 11/35), loss = 0.000325 (0.537 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:19:22.366103: step 37280/119245 (epoch 11/35), loss = 0.010396 (0.639 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:19:31.239546: step 37300/119245 (epoch 11/35), loss = 0.082021 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:19:39.867895: step 37320/119245 (epoch 11/35), loss = 0.143866 (0.369 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:19:48.698197: step 37340/119245 (epoch 11/35), loss = 0.009166 (0.432 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:19:59.374410: step 37360/119245 (epoch 11/35), loss = 0.083485 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:20:09.495423: step 37380/119245 (epoch 11/35), loss = 0.047279 (0.518 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:20:19.109509: step 37400/119245 (epoch 11/35), loss = 0.377062 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:20:28.828897: step 37420/119245 (epoch 11/35), loss = 0.022192 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:20:38.529256: step 37440/119245 (epoch 11/35), loss = 0.001962 (0.380 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:20:50.286570: step 37460/119245 (epoch 11/35), loss = 0.021248 (0.299 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  78.30%  R:  78.99%  F1:  78.65%  #: 338\n",
            "org:city_of_headquarters             P:  65.88%  R:  51.38%  F1:  57.73%  #: 109\n",
            "org:country_of_headquarters          P:  55.43%  R:  57.63%  F1:  56.51%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  88.24%  R:  78.95%  F1:  83.33%  #: 38\n",
            "org:founded_by                       P:  66.07%  R:  48.68%  F1:  56.06%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  76.81%  R:  62.35%  F1:  68.83%  #: 85\n",
            "org:number_of_employees/members      P:  86.67%  R:  48.15%  F1:  61.90%  #: 27\n",
            "org:parents                          P:  32.99%  R:  33.33%  F1:  33.16%  #: 96\n",
            "org:political/religious_affiliation  P:  33.33%  R:  30.00%  F1:  31.58%  #: 10\n",
            "org:shareholders                     P:  40.91%  R:  16.36%  F1:  23.38%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  54.88%  R:  64.29%  F1:  59.21%  #: 70\n",
            "org:subsidiaries                     P:  51.61%  R:  28.32%  F1:  36.57%  #: 113\n",
            "org:top_members/employees            P:  72.92%  R:  77.15%  F1:  74.98%  #: 534\n",
            "org:website                          P:  87.50%  R:  97.67%  F1:  92.31%  #: 86\n",
            "per:age                              P:  88.10%  R:  91.36%  F1:  89.70%  #: 243\n",
            "per:alternate_names                  P:  66.67%  R:  26.32%  F1:  37.74%  #: 38\n",
            "per:cause_of_death                   P:  90.00%  R:  58.93%  F1:  71.22%  #: 168\n",
            "per:charges                          P:  78.22%  R:  75.24%  F1:  76.70%  #: 105\n",
            "per:children                         P:  67.65%  R:  69.70%  F1:  68.66%  #: 99\n",
            "per:cities_of_residence              P:  40.69%  R:  52.51%  F1:  45.85%  #: 179\n",
            "per:city_of_birth                    P:  72.41%  R:  63.64%  F1:  67.74%  #: 33\n",
            "per:city_of_death                    P:  66.67%  R:  50.85%  F1:  57.69%  #: 118\n",
            "per:countries_of_residence           P:  34.88%  R:  66.37%  F1:  45.73%  #: 226\n",
            "per:country_of_birth                 P:  66.67%  R:  30.00%  F1:  41.38%  #: 20\n",
            "per:country_of_death                 P: 100.00%  R:   2.17%  F1:   4.26%  #: 46\n",
            "per:date_of_birth                    P:  93.33%  R:  90.32%  F1:  91.80%  #: 31\n",
            "per:date_of_death                    P:  80.00%  R:  60.19%  F1:  68.70%  #: 206\n",
            "per:employee_of                      P:  53.45%  R:  64.00%  F1:  58.25%  #: 375\n",
            "per:origin                           P:  67.52%  R:  50.48%  F1:  57.77%  #: 210\n",
            "per:other_family                     P:  72.22%  R:  16.25%  F1:  26.53%  #: 80\n",
            "per:parents                          P:  72.22%  R:  46.43%  F1:  56.52%  #: 56\n",
            "per:religion                         P:  54.10%  R:  62.26%  F1:  57.89%  #: 53\n",
            "per:schools_attended                 P:  89.74%  R:  70.00%  F1:  78.65%  #: 50\n",
            "per:siblings                         P:  79.17%  R:  63.33%  F1:  70.37%  #: 30\n",
            "per:spouse                           P:  71.22%  R:  62.26%  F1:  66.44%  #: 159\n",
            "per:stateorprovince_of_birth         P:  69.57%  R:  61.54%  F1:  65.31%  #: 26\n",
            "per:stateorprovince_of_death         P:  57.14%  R:  19.51%  F1:  29.09%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  34.35%  R:  62.50%  F1:  44.33%  #: 72\n",
            "per:title                            P:  78.94%  R:  79.54%  F1:  79.24%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17331 Guess as no relation\n",
            "5300 guess\n",
            "3509 correct\n",
            "5436 gold\n",
            "Precision (micro): 66.208%\n",
            "   Recall (micro): 64.551%\n",
            "       F1 (micro): 65.369%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 11: train_loss = 0.079845, dev_loss = 0.801471, dev_f1 = 0.6537\n",
            "model saved to ./saved_models/00/checkpoint_epoch_11.pt\n",
            "\n",
            "2020-12-02 05:23:47.518414: step 37480/119245 (epoch 12/35), loss = 0.003383 (0.388 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:23:57.016958: step 37500/119245 (epoch 12/35), loss = 0.006025 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:24:08.778091: step 37520/119245 (epoch 12/35), loss = 0.029117 (0.515 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:24:17.187539: step 37540/119245 (epoch 12/35), loss = 0.141298 (0.419 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:24:28.018050: step 37560/119245 (epoch 12/35), loss = 0.002775 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:24:37.031964: step 37580/119245 (epoch 12/35), loss = 0.073616 (0.355 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:24:46.883141: step 37600/119245 (epoch 12/35), loss = 0.009444 (0.414 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:24:56.308115: step 37620/119245 (epoch 12/35), loss = 0.467114 (0.444 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:25:04.656438: step 37640/119245 (epoch 12/35), loss = 0.107135 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:25:14.459875: step 37660/119245 (epoch 12/35), loss = 0.006785 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:25:24.968978: step 37680/119245 (epoch 12/35), loss = 0.002829 (0.957 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:25:33.874888: step 37700/119245 (epoch 12/35), loss = 0.050329 (0.353 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:25:42.632890: step 37720/119245 (epoch 12/35), loss = 0.001932 (0.498 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:25:53.052792: step 37740/119245 (epoch 12/35), loss = 0.005822 (0.757 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:26:02.610746: step 37760/119245 (epoch 12/35), loss = 0.093455 (0.345 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:26:11.257853: step 37780/119245 (epoch 12/35), loss = 0.034935 (0.485 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:26:22.166587: step 37800/119245 (epoch 12/35), loss = 0.000582 (0.634 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:26:31.775849: step 37820/119245 (epoch 12/35), loss = 0.069268 (0.362 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:26:43.111091: step 37840/119245 (epoch 12/35), loss = 0.001824 (0.427 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:26:52.404121: step 37860/119245 (epoch 12/35), loss = 0.004038 (1.002 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:27:02.876240: step 37880/119245 (epoch 12/35), loss = 0.018176 (1.015 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:27:12.260963: step 37900/119245 (epoch 12/35), loss = 0.084351 (0.556 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:27:21.708562: step 37920/119245 (epoch 12/35), loss = 0.003577 (0.645 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:27:33.767500: step 37940/119245 (epoch 12/35), loss = 0.032949 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:27:43.004393: step 37960/119245 (epoch 12/35), loss = 0.035924 (0.471 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:27:54.341326: step 37980/119245 (epoch 12/35), loss = 0.038962 (0.522 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:28:03.346065: step 38000/119245 (epoch 12/35), loss = 0.002649 (0.330 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:28:14.939375: step 38020/119245 (epoch 12/35), loss = 0.003421 (0.448 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:28:23.668287: step 38040/119245 (epoch 12/35), loss = 0.001859 (0.375 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:28:32.833690: step 38060/119245 (epoch 12/35), loss = 0.000110 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:28:41.929513: step 38080/119245 (epoch 12/35), loss = 0.040901 (0.539 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:28:51.059206: step 38100/119245 (epoch 12/35), loss = 0.107506 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:29:00.027994: step 38120/119245 (epoch 12/35), loss = 0.002648 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:29:10.157351: step 38140/119245 (epoch 12/35), loss = 0.007398 (0.382 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:29:21.653998: step 38160/119245 (epoch 12/35), loss = 0.000990 (1.557 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:29:30.147818: step 38180/119245 (epoch 12/35), loss = 0.005497 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:29:40.412789: step 38200/119245 (epoch 12/35), loss = 0.002847 (0.481 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:29:50.506123: step 38220/119245 (epoch 12/35), loss = 0.076082 (0.585 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:29:59.806082: step 38240/119245 (epoch 12/35), loss = 0.001226 (0.608 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:30:09.467872: step 38260/119245 (epoch 12/35), loss = 0.149467 (0.367 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:30:19.480435: step 38280/119245 (epoch 12/35), loss = 0.000415 (0.760 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:30:28.343891: step 38300/119245 (epoch 12/35), loss = 0.002639 (0.335 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:30:37.787363: step 38320/119245 (epoch 12/35), loss = 0.001589 (0.754 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:30:47.929978: step 38340/119245 (epoch 12/35), loss = 0.066405 (1.749 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:30:58.139865: step 38360/119245 (epoch 12/35), loss = 0.021686 (0.369 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:31:09.406019: step 38380/119245 (epoch 12/35), loss = 0.031627 (0.507 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:31:19.063469: step 38400/119245 (epoch 12/35), loss = 0.001123 (0.502 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:31:27.662789: step 38420/119245 (epoch 12/35), loss = 0.229822 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:31:36.461705: step 38440/119245 (epoch 12/35), loss = 0.001498 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:31:44.840213: step 38460/119245 (epoch 12/35), loss = 0.095263 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:31:53.722646: step 38480/119245 (epoch 12/35), loss = 0.250423 (0.416 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:32:02.607756: step 38500/119245 (epoch 12/35), loss = 0.100912 (0.449 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:32:12.709976: step 38520/119245 (epoch 12/35), loss = 0.181398 (0.365 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:32:21.341393: step 38540/119245 (epoch 12/35), loss = 0.047356 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:32:32.558315: step 38560/119245 (epoch 12/35), loss = 0.000239 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:32:44.306485: step 38580/119245 (epoch 12/35), loss = 0.004248 (0.446 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:32:53.296074: step 38600/119245 (epoch 12/35), loss = 0.023083 (0.471 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:33:03.392354: step 38620/119245 (epoch 12/35), loss = 0.012864 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:33:13.909960: step 38640/119245 (epoch 12/35), loss = 0.002421 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:33:22.845824: step 38660/119245 (epoch 12/35), loss = 0.067534 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:33:32.777067: step 38680/119245 (epoch 12/35), loss = 0.095239 (0.363 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:33:42.362038: step 38700/119245 (epoch 12/35), loss = 0.288674 (1.013 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:33:51.399942: step 38720/119245 (epoch 12/35), loss = 0.707368 (0.468 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:34:01.130185: step 38740/119245 (epoch 12/35), loss = 0.016514 (0.622 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:34:12.664651: step 38760/119245 (epoch 12/35), loss = 0.000786 (0.339 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:34:24.601346: step 38780/119245 (epoch 12/35), loss = 0.002424 (0.405 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:34:34.874086: step 38800/119245 (epoch 12/35), loss = 0.178194 (0.590 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:34:44.788162: step 38820/119245 (epoch 12/35), loss = 0.020788 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:34:54.405005: step 38840/119245 (epoch 12/35), loss = 0.000964 (0.488 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:35:03.532128: step 38860/119245 (epoch 12/35), loss = 0.205821 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:35:12.398175: step 38880/119245 (epoch 12/35), loss = 0.001598 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:35:22.729446: step 38900/119245 (epoch 12/35), loss = 0.003218 (0.679 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:35:32.210070: step 38920/119245 (epoch 12/35), loss = 0.037688 (0.355 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:35:42.191534: step 38940/119245 (epoch 12/35), loss = 0.170652 (0.406 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:35:51.618639: step 38960/119245 (epoch 12/35), loss = 0.127424 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:36:00.354139: step 38980/119245 (epoch 12/35), loss = 0.039993 (0.560 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:36:10.288750: step 39000/119245 (epoch 12/35), loss = 0.012054 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:36:20.560393: step 39020/119245 (epoch 12/35), loss = 0.365084 (0.424 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:36:30.192260: step 39040/119245 (epoch 12/35), loss = 0.004500 (0.613 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:36:40.145658: step 39060/119245 (epoch 12/35), loss = 0.000903 (0.517 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:36:49.375026: step 39080/119245 (epoch 12/35), loss = 0.000330 (0.638 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:36:59.217410: step 39100/119245 (epoch 12/35), loss = 0.001340 (0.469 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:37:08.930787: step 39120/119245 (epoch 12/35), loss = 0.000408 (0.352 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:37:19.222653: step 39140/119245 (epoch 12/35), loss = 0.000918 (0.994 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:37:28.790968: step 39160/119245 (epoch 12/35), loss = 0.351223 (0.366 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:37:38.284698: step 39180/119245 (epoch 12/35), loss = 0.132075 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:37:49.126889: step 39200/119245 (epoch 12/35), loss = 0.000465 (0.497 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:37:59.217909: step 39220/119245 (epoch 12/35), loss = 0.098838 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:38:09.021259: step 39240/119245 (epoch 12/35), loss = 0.001926 (0.432 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:38:18.678006: step 39260/119245 (epoch 12/35), loss = 0.028523 (1.169 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:38:28.183453: step 39280/119245 (epoch 12/35), loss = 0.000309 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:38:37.686573: step 39300/119245 (epoch 12/35), loss = 0.230137 (0.466 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:38:47.179456: step 39320/119245 (epoch 12/35), loss = 0.016089 (0.518 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:38:58.820443: step 39340/119245 (epoch 12/35), loss = 0.001857 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:39:08.103028: step 39360/119245 (epoch 12/35), loss = 0.007586 (0.616 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:39:19.370371: step 39380/119245 (epoch 12/35), loss = 0.004153 (0.407 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:39:27.867503: step 39400/119245 (epoch 12/35), loss = 0.003689 (0.384 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:39:39.091626: step 39420/119245 (epoch 12/35), loss = 0.001076 (0.634 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:39:49.717461: step 39440/119245 (epoch 12/35), loss = 0.206479 (0.947 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:40:00.487388: step 39460/119245 (epoch 12/35), loss = 0.008195 (0.440 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:40:10.512286: step 39480/119245 (epoch 12/35), loss = 0.038837 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:40:20.112544: step 39500/119245 (epoch 12/35), loss = 0.001349 (0.334 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:40:30.695659: step 39520/119245 (epoch 12/35), loss = 0.042016 (0.407 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:40:39.382595: step 39540/119245 (epoch 12/35), loss = 0.037114 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:40:49.860648: step 39560/119245 (epoch 12/35), loss = 0.089770 (0.521 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:40:59.193274: step 39580/119245 (epoch 12/35), loss = 0.006003 (0.612 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:41:09.469232: step 39600/119245 (epoch 12/35), loss = 0.014293 (0.460 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:41:18.715026: step 39620/119245 (epoch 12/35), loss = 0.202837 (0.471 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:41:28.661998: step 39640/119245 (epoch 12/35), loss = 0.056541 (0.352 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:41:38.456520: step 39660/119245 (epoch 12/35), loss = 0.154442 (0.425 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:41:48.942038: step 39680/119245 (epoch 12/35), loss = 0.008452 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:41:59.258026: step 39700/119245 (epoch 12/35), loss = 0.121726 (0.435 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:42:08.577869: step 39720/119245 (epoch 12/35), loss = 0.029021 (0.348 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:42:18.500215: step 39740/119245 (epoch 12/35), loss = 0.013552 (0.437 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:42:27.324841: step 39760/119245 (epoch 12/35), loss = 0.046076 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:42:37.160519: step 39780/119245 (epoch 12/35), loss = 0.025691 (0.493 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:42:47.658616: step 39800/119245 (epoch 12/35), loss = 0.009748 (1.538 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:42:57.139031: step 39820/119245 (epoch 12/35), loss = 0.000356 (0.545 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:43:08.710517: step 39840/119245 (epoch 12/35), loss = 0.169752 (0.393 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:43:19.079938: step 39860/119245 (epoch 12/35), loss = 0.019800 (0.338 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:43:28.421584: step 39880/119245 (epoch 12/35), loss = 0.101073 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:43:37.765783: step 39900/119245 (epoch 12/35), loss = 0.003270 (0.332 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:43:48.175582: step 39920/119245 (epoch 12/35), loss = 0.610786 (0.371 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:43:57.983344: step 39940/119245 (epoch 12/35), loss = 0.027205 (0.402 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:44:08.288154: step 39960/119245 (epoch 12/35), loss = 0.000552 (0.378 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:44:18.330751: step 39980/119245 (epoch 12/35), loss = 0.006986 (0.430 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:44:27.739369: step 40000/119245 (epoch 12/35), loss = 0.070740 (0.485 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:44:36.484253: step 40020/119245 (epoch 12/35), loss = 0.002393 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:44:46.306660: step 40040/119245 (epoch 12/35), loss = 0.066652 (0.538 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:44:55.826757: step 40060/119245 (epoch 12/35), loss = 0.000181 (0.413 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:45:04.477022: step 40080/119245 (epoch 12/35), loss = 0.033225 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:45:15.641538: step 40100/119245 (epoch 12/35), loss = 0.001041 (0.801 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:45:24.534610: step 40120/119245 (epoch 12/35), loss = 0.599645 (0.369 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:45:33.543796: step 40140/119245 (epoch 12/35), loss = 0.159507 (0.557 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:45:42.040490: step 40160/119245 (epoch 12/35), loss = 0.000622 (0.505 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:45:52.696470: step 40180/119245 (epoch 12/35), loss = 0.144243 (1.352 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:46:03.198739: step 40200/119245 (epoch 12/35), loss = 0.223439 (0.436 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:46:12.337760: step 40220/119245 (epoch 12/35), loss = 0.162856 (0.404 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:46:22.536605: step 40240/119245 (epoch 12/35), loss = 0.057328 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:46:31.693435: step 40260/119245 (epoch 12/35), loss = 0.007103 (0.600 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:46:41.643829: step 40280/119245 (epoch 12/35), loss = 0.002823 (0.833 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:46:51.236243: step 40300/119245 (epoch 12/35), loss = 0.019667 (0.542 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:47:00.569565: step 40320/119245 (epoch 12/35), loss = 0.086985 (0.340 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:47:10.345568: step 40340/119245 (epoch 12/35), loss = 0.230647 (0.354 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:47:19.792900: step 40360/119245 (epoch 12/35), loss = 0.000622 (0.376 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:47:29.134469: step 40380/119245 (epoch 12/35), loss = 0.021754 (0.359 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:47:38.151897: step 40400/119245 (epoch 12/35), loss = 0.004502 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:47:47.511367: step 40420/119245 (epoch 12/35), loss = 0.208886 (0.481 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:47:58.967199: step 40440/119245 (epoch 12/35), loss = 0.000633 (0.397 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:48:07.708224: step 40460/119245 (epoch 12/35), loss = 0.687620 (0.464 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:48:17.149337: step 40480/119245 (epoch 12/35), loss = 0.000579 (0.564 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:48:27.909364: step 40500/119245 (epoch 12/35), loss = 0.018538 (0.571 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:48:37.380806: step 40520/119245 (epoch 12/35), loss = 0.000611 (0.788 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:48:47.281498: step 40540/119245 (epoch 12/35), loss = 0.001753 (0.451 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:48:57.472872: step 40560/119245 (epoch 12/35), loss = 0.004669 (0.381 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:49:07.222032: step 40580/119245 (epoch 12/35), loss = 0.001023 (0.471 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:49:17.305169: step 40600/119245 (epoch 12/35), loss = 0.001113 (0.581 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:49:26.474530: step 40620/119245 (epoch 12/35), loss = 0.037251 (0.412 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:49:36.188639: step 40640/119245 (epoch 12/35), loss = 0.002737 (0.412 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:49:44.424111: step 40660/119245 (epoch 12/35), loss = 0.007822 (0.615 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:49:55.165406: step 40680/119245 (epoch 12/35), loss = 0.421121 (0.679 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:50:04.394022: step 40700/119245 (epoch 12/35), loss = 0.061626 (0.554 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:50:13.156568: step 40720/119245 (epoch 12/35), loss = 0.001381 (0.350 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:50:21.832628: step 40740/119245 (epoch 12/35), loss = 0.332768 (0.543 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:50:32.106554: step 40760/119245 (epoch 12/35), loss = 0.000242 (0.455 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:50:41.267144: step 40780/119245 (epoch 12/35), loss = 0.000450 (0.642 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:50:52.173098: step 40800/119245 (epoch 12/35), loss = 0.002077 (0.532 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:51:01.820802: step 40820/119245 (epoch 12/35), loss = 0.001868 (0.342 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:51:11.607625: step 40840/119245 (epoch 12/35), loss = 0.017227 (0.604 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:51:21.327685: step 40860/119245 (epoch 12/35), loss = 0.042489 (0.478 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:51:32.445025: step 40880/119245 (epoch 12/35), loss = 0.386856 (0.558 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  79.82%  R:  78.40%  F1:  79.10%  #: 338\n",
            "org:city_of_headquarters             P:  60.38%  R:  58.72%  F1:  59.53%  #: 109\n",
            "org:country_of_headquarters          P:  54.59%  R:  60.45%  F1:  57.37%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  82.35%  R:  73.68%  F1:  77.78%  #: 38\n",
            "org:founded_by                       P:  65.67%  R:  57.89%  F1:  61.54%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  67.11%  R:  60.00%  F1:  63.35%  #: 85\n",
            "org:number_of_employees/members      P:  56.67%  R:  62.96%  F1:  59.65%  #: 27\n",
            "org:parents                          P:  43.86%  R:  26.04%  F1:  32.68%  #: 96\n",
            "org:political/religious_affiliation  P:  30.77%  R:  40.00%  F1:  34.78%  #: 10\n",
            "org:shareholders                     P:  29.82%  R:  30.91%  F1:  30.36%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  58.33%  R:  70.00%  F1:  63.64%  #: 70\n",
            "org:subsidiaries                     P:  43.04%  R:  30.09%  F1:  35.42%  #: 113\n",
            "org:top_members/employees            P:  74.91%  R:  75.47%  F1:  75.19%  #: 534\n",
            "org:website                          P:  88.54%  R:  98.84%  F1:  93.41%  #: 86\n",
            "per:age                              P:  82.22%  R:  91.36%  F1:  86.55%  #: 243\n",
            "per:alternate_names                  P:  62.50%  R:  26.32%  F1:  37.04%  #: 38\n",
            "per:cause_of_death                   P:  90.29%  R:  55.36%  F1:  68.63%  #: 168\n",
            "per:charges                          P:  74.26%  R:  71.43%  F1:  72.82%  #: 105\n",
            "per:children                         P:  75.95%  R:  60.61%  F1:  67.42%  #: 99\n",
            "per:cities_of_residence              P:  50.00%  R:  46.93%  F1:  48.41%  #: 179\n",
            "per:city_of_birth                    P:  70.00%  R:  84.85%  F1:  76.71%  #: 33\n",
            "per:city_of_death                    P:  73.53%  R:  63.56%  F1:  68.18%  #: 118\n",
            "per:countries_of_residence           P:  33.90%  R:  52.65%  F1:  41.25%  #: 226\n",
            "per:country_of_birth                 P:  54.55%  R:  30.00%  F1:  38.71%  #: 20\n",
            "per:country_of_death                 P:  50.00%  R:   2.17%  F1:   4.17%  #: 46\n",
            "per:date_of_birth                    P:  90.00%  R:  87.10%  F1:  88.52%  #: 31\n",
            "per:date_of_death                    P:  82.69%  R:  62.62%  F1:  71.27%  #: 206\n",
            "per:employee_of                      P:  62.29%  R:  58.13%  F1:  60.14%  #: 375\n",
            "per:origin                           P:  60.35%  R:  65.24%  F1:  62.70%  #: 210\n",
            "per:other_family                     P:  64.29%  R:  11.25%  F1:  19.15%  #: 80\n",
            "per:parents                          P:  54.90%  R:  50.00%  F1:  52.34%  #: 56\n",
            "per:religion                         P:  52.31%  R:  64.15%  F1:  57.63%  #: 53\n",
            "per:schools_attended                 P:  80.56%  R:  58.00%  F1:  67.44%  #: 50\n",
            "per:siblings                         P:  60.71%  R:  56.67%  F1:  58.62%  #: 30\n",
            "per:spouse                           P:  64.81%  R:  66.04%  F1:  65.42%  #: 159\n",
            "per:stateorprovince_of_birth         P:  65.22%  R:  57.69%  F1:  61.22%  #: 26\n",
            "per:stateorprovince_of_death         P:  64.29%  R:  43.90%  F1:  52.17%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  37.70%  R:  63.89%  F1:  47.42%  #: 72\n",
            "per:title                            P:  79.53%  R:  81.18%  F1:  80.34%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17372 Guess as no relation\n",
            "5259 guess\n",
            "3524 correct\n",
            "5436 gold\n",
            "Precision (micro): 67.009%\n",
            "   Recall (micro): 64.827%\n",
            "       F1 (micro): 65.900%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 12: train_loss = 0.067990, dev_loss = 0.881415, dev_f1 = 0.6590\n",
            "model saved to ./saved_models/00/checkpoint_epoch_12.pt\n",
            "\n",
            "2020-12-02 05:54:29.699433: step 40900/119245 (epoch 13/35), loss = 0.060167 (0.771 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:54:39.970087: step 40920/119245 (epoch 13/35), loss = 0.014286 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:54:49.536496: step 40940/119245 (epoch 13/35), loss = 0.146709 (0.394 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:55:00.097658: step 40960/119245 (epoch 13/35), loss = 0.071979 (0.393 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:55:08.716351: step 40980/119245 (epoch 13/35), loss = 0.010966 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:55:18.629962: step 41000/119245 (epoch 13/35), loss = 0.059799 (0.739 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:55:28.515362: step 41020/119245 (epoch 13/35), loss = 0.001522 (0.543 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:55:37.255598: step 41040/119245 (epoch 13/35), loss = 0.016290 (0.407 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:55:46.370311: step 41060/119245 (epoch 13/35), loss = 0.000654 (0.486 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:55:56.466384: step 41080/119245 (epoch 13/35), loss = 0.007955 (0.360 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:56:06.074427: step 41100/119245 (epoch 13/35), loss = 0.207915 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:56:15.077655: step 41120/119245 (epoch 13/35), loss = 0.016325 (0.383 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:56:24.847099: step 41140/119245 (epoch 13/35), loss = 0.006752 (0.377 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:56:35.250206: step 41160/119245 (epoch 13/35), loss = 0.046008 (0.289 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:56:43.537138: step 41180/119245 (epoch 13/35), loss = 0.013233 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:56:52.903777: step 41200/119245 (epoch 13/35), loss = 0.127219 (0.359 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:57:03.810513: step 41220/119245 (epoch 13/35), loss = 0.027280 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:57:14.906336: step 41240/119245 (epoch 13/35), loss = 0.113909 (1.552 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:57:23.976052: step 41260/119245 (epoch 13/35), loss = 0.003065 (0.424 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:57:33.049624: step 41280/119245 (epoch 13/35), loss = 0.001223 (0.510 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:57:44.017288: step 41300/119245 (epoch 13/35), loss = 0.001840 (0.442 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:57:53.124029: step 41320/119245 (epoch 13/35), loss = 0.426716 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:58:05.792251: step 41340/119245 (epoch 13/35), loss = 0.017561 (0.505 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:58:15.028603: step 41360/119245 (epoch 13/35), loss = 0.004657 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:58:25.114606: step 41380/119245 (epoch 13/35), loss = 0.092511 (0.399 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:58:35.366989: step 41400/119245 (epoch 13/35), loss = 0.001727 (0.474 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:58:45.358221: step 41420/119245 (epoch 13/35), loss = 0.003964 (0.406 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:58:55.164710: step 41440/119245 (epoch 13/35), loss = 0.115279 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:59:04.836321: step 41460/119245 (epoch 13/35), loss = 0.116336 (0.645 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:59:13.552831: step 41480/119245 (epoch 13/35), loss = 0.126064 (0.450 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:59:22.408633: step 41500/119245 (epoch 13/35), loss = 0.164725 (0.770 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:59:31.916407: step 41520/119245 (epoch 13/35), loss = 0.000958 (0.599 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:59:41.122935: step 41540/119245 (epoch 13/35), loss = 0.000252 (0.367 sec/batch), lr: 0.030000\n",
            "2020-12-02 05:59:52.015828: step 41560/119245 (epoch 13/35), loss = 0.015259 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:00:02.182847: step 41580/119245 (epoch 13/35), loss = 0.007435 (0.615 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:00:12.012170: step 41600/119245 (epoch 13/35), loss = 0.061360 (0.553 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:00:22.007511: step 41620/119245 (epoch 13/35), loss = 0.061590 (0.358 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:00:31.362654: step 41640/119245 (epoch 13/35), loss = 0.227617 (0.523 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:00:40.615188: step 41660/119245 (epoch 13/35), loss = 0.307371 (0.605 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:00:50.150553: step 41680/119245 (epoch 13/35), loss = 0.017070 (0.441 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:01:00.011432: step 41700/119245 (epoch 13/35), loss = 0.008380 (0.606 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:01:09.030975: step 41720/119245 (epoch 13/35), loss = 0.001819 (0.380 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:01:18.522257: step 41740/119245 (epoch 13/35), loss = 0.006665 (0.718 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:01:29.331054: step 41760/119245 (epoch 13/35), loss = 0.001480 (0.403 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:01:39.948806: step 41780/119245 (epoch 13/35), loss = 0.002663 (0.381 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:01:50.820643: step 41800/119245 (epoch 13/35), loss = 0.066275 (0.395 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:01:59.841282: step 41820/119245 (epoch 13/35), loss = 0.000522 (0.719 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:02:08.403832: step 41840/119245 (epoch 13/35), loss = 0.170785 (0.514 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:02:16.882600: step 41860/119245 (epoch 13/35), loss = 0.004439 (0.423 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:02:25.687014: step 41880/119245 (epoch 13/35), loss = 0.173682 (0.704 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:02:34.312912: step 41900/119245 (epoch 13/35), loss = 0.016270 (0.482 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:02:43.493952: step 41920/119245 (epoch 13/35), loss = 0.001618 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:02:53.684370: step 41940/119245 (epoch 13/35), loss = 0.026505 (0.415 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:03:03.815026: step 41960/119245 (epoch 13/35), loss = 0.009642 (0.502 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:03:16.558512: step 41980/119245 (epoch 13/35), loss = 0.005833 (0.890 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:03:25.161643: step 42000/119245 (epoch 13/35), loss = 0.001197 (0.477 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:03:34.621555: step 42020/119245 (epoch 13/35), loss = 0.002622 (0.570 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:03:45.206841: step 42040/119245 (epoch 13/35), loss = 0.004680 (0.482 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:03:55.141476: step 42060/119245 (epoch 13/35), loss = 0.009253 (0.333 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:04:04.663454: step 42080/119245 (epoch 13/35), loss = 0.118938 (0.776 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:04:13.890496: step 42100/119245 (epoch 13/35), loss = 0.237258 (0.333 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:04:23.351231: step 42120/119245 (epoch 13/35), loss = 0.102717 (0.620 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:04:32.598655: step 42140/119245 (epoch 13/35), loss = 0.075380 (0.614 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:04:42.753931: step 42160/119245 (epoch 13/35), loss = 0.001438 (0.463 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:04:55.939713: step 42180/119245 (epoch 13/35), loss = 0.003206 (0.354 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:05:06.284753: step 42200/119245 (epoch 13/35), loss = 0.006283 (0.460 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:05:16.543556: step 42220/119245 (epoch 13/35), loss = 0.004370 (0.466 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:05:26.168287: step 42240/119245 (epoch 13/35), loss = 0.004376 (0.414 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:05:35.534607: step 42260/119245 (epoch 13/35), loss = 0.005850 (0.516 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:05:44.540606: step 42280/119245 (epoch 13/35), loss = 0.039659 (0.507 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:05:54.523258: step 42300/119245 (epoch 13/35), loss = 0.003551 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:06:04.093464: step 42320/119245 (epoch 13/35), loss = 0.012686 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:06:13.520141: step 42340/119245 (epoch 13/35), loss = 0.059183 (0.636 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:06:23.780819: step 42360/119245 (epoch 13/35), loss = 0.002031 (0.743 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:06:32.663201: step 42380/119245 (epoch 13/35), loss = 0.002230 (0.507 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:06:41.772254: step 42400/119245 (epoch 13/35), loss = 0.051151 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:06:52.706528: step 42420/119245 (epoch 13/35), loss = 0.000277 (0.448 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:07:01.733344: step 42440/119245 (epoch 13/35), loss = 0.081401 (0.537 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:07:11.737720: step 42460/119245 (epoch 13/35), loss = 0.026775 (0.444 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:07:21.264492: step 42480/119245 (epoch 13/35), loss = 0.004613 (0.406 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:07:31.276167: step 42500/119245 (epoch 13/35), loss = 0.035413 (0.490 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:07:41.066989: step 42520/119245 (epoch 13/35), loss = 0.071512 (0.463 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:07:50.883407: step 42540/119245 (epoch 13/35), loss = 0.086318 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:08:00.518769: step 42560/119245 (epoch 13/35), loss = 0.022695 (0.374 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:08:09.496949: step 42580/119245 (epoch 13/35), loss = 0.032295 (0.354 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:08:20.450865: step 42600/119245 (epoch 13/35), loss = 0.258405 (0.510 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:08:31.039928: step 42620/119245 (epoch 13/35), loss = 0.002236 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:08:40.737002: step 42640/119245 (epoch 13/35), loss = 0.001340 (0.873 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:08:49.697583: step 42660/119245 (epoch 13/35), loss = 0.034581 (0.404 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:09:00.029646: step 42680/119245 (epoch 13/35), loss = 0.004125 (0.451 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:09:09.514962: step 42700/119245 (epoch 13/35), loss = 0.534744 (0.618 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:09:18.789214: step 42720/119245 (epoch 13/35), loss = 0.006321 (0.458 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:09:30.436477: step 42740/119245 (epoch 13/35), loss = 0.071572 (0.391 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:09:39.724138: step 42760/119245 (epoch 13/35), loss = 0.001255 (0.495 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:09:50.833528: step 42780/119245 (epoch 13/35), loss = 0.000294 (0.379 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:10:00.045772: step 42800/119245 (epoch 13/35), loss = 0.172483 (0.385 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:10:10.028066: step 42820/119245 (epoch 13/35), loss = 0.363920 (0.613 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:10:21.244043: step 42840/119245 (epoch 13/35), loss = 0.000842 (0.520 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:10:31.786460: step 42860/119245 (epoch 13/35), loss = 0.002112 (0.705 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:10:42.084761: step 42880/119245 (epoch 13/35), loss = 0.000191 (0.506 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:10:52.006360: step 42900/119245 (epoch 13/35), loss = 0.024442 (0.519 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:11:01.095987: step 42920/119245 (epoch 13/35), loss = 0.001856 (0.502 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:11:11.181241: step 42940/119245 (epoch 13/35), loss = 0.004726 (0.624 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:11:21.286199: step 42960/119245 (epoch 13/35), loss = 0.106612 (0.328 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:11:30.885848: step 42980/119245 (epoch 13/35), loss = 0.375368 (0.550 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:11:40.906520: step 43000/119245 (epoch 13/35), loss = 0.003082 (0.523 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:11:49.985757: step 43020/119245 (epoch 13/35), loss = 0.100185 (0.332 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:12:00.044636: step 43040/119245 (epoch 13/35), loss = 0.002678 (0.362 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:12:10.105703: step 43060/119245 (epoch 13/35), loss = 0.002954 (0.346 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:12:19.901702: step 43080/119245 (epoch 13/35), loss = 0.029215 (0.534 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:12:30.254843: step 43100/119245 (epoch 13/35), loss = 0.145115 (0.585 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:12:40.386990: step 43120/119245 (epoch 13/35), loss = 0.034522 (0.513 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:12:49.099441: step 43140/119245 (epoch 13/35), loss = 0.001090 (0.474 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:12:59.103526: step 43160/119245 (epoch 13/35), loss = 0.001478 (0.617 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:13:08.613688: step 43180/119245 (epoch 13/35), loss = 0.000326 (0.406 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:13:17.941945: step 43200/119245 (epoch 13/35), loss = 0.411533 (0.426 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:13:28.415481: step 43220/119245 (epoch 13/35), loss = 0.016897 (0.328 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:13:39.651542: step 43240/119245 (epoch 13/35), loss = 0.000303 (0.457 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:13:50.020579: step 43260/119245 (epoch 13/35), loss = 0.001272 (0.610 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:13:59.953352: step 43280/119245 (epoch 13/35), loss = 0.020618 (0.453 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:14:09.079745: step 43300/119245 (epoch 13/35), loss = 0.005466 (0.425 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:14:18.608563: step 43320/119245 (epoch 13/35), loss = 0.001585 (0.361 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:14:29.530842: step 43340/119245 (epoch 13/35), loss = 0.004792 (0.378 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:14:39.685950: step 43360/119245 (epoch 13/35), loss = 0.017499 (0.963 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:14:49.217302: step 43380/119245 (epoch 13/35), loss = 0.001140 (0.597 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:14:58.901703: step 43400/119245 (epoch 13/35), loss = 0.067172 (0.409 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:15:07.925939: step 43420/119245 (epoch 13/35), loss = 0.023377 (0.479 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:15:17.240102: step 43440/119245 (epoch 13/35), loss = 0.003405 (0.590 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:15:27.413364: step 43460/119245 (epoch 13/35), loss = 0.012648 (0.443 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:15:36.095384: step 43480/119245 (epoch 13/35), loss = 0.302250 (0.502 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:15:45.685224: step 43500/119245 (epoch 13/35), loss = 0.038681 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:15:55.919187: step 43520/119245 (epoch 13/35), loss = 0.009521 (0.523 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:16:04.576258: step 43540/119245 (epoch 13/35), loss = 0.005393 (0.452 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:16:13.402786: step 43560/119245 (epoch 13/35), loss = 0.001394 (0.473 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:16:22.832183: step 43580/119245 (epoch 13/35), loss = 0.005108 (0.389 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:16:33.135524: step 43600/119245 (epoch 13/35), loss = 0.072577 (0.356 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:16:43.414904: step 43620/119245 (epoch 13/35), loss = 0.026079 (0.447 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:16:53.960079: step 43640/119245 (epoch 13/35), loss = 0.111119 (0.459 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:17:03.031414: step 43660/119245 (epoch 13/35), loss = 0.009979 (0.591 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:17:12.255212: step 43680/119245 (epoch 13/35), loss = 0.005637 (0.477 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:17:21.713697: step 43700/119245 (epoch 13/35), loss = 0.000354 (0.337 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:17:31.410643: step 43720/119245 (epoch 13/35), loss = 0.000549 (0.502 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:17:41.605490: step 43740/119245 (epoch 13/35), loss = 0.126456 (0.400 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:17:50.912069: step 43760/119245 (epoch 13/35), loss = 0.017755 (0.339 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:18:00.327739: step 43780/119245 (epoch 13/35), loss = 0.020854 (0.612 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:18:09.241607: step 43800/119245 (epoch 13/35), loss = 0.164285 (0.472 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:18:18.025224: step 43820/119245 (epoch 13/35), loss = 0.001339 (0.402 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:18:29.397865: step 43840/119245 (epoch 13/35), loss = 0.008214 (0.956 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:18:38.598480: step 43860/119245 (epoch 13/35), loss = 0.155218 (0.417 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:18:47.864147: step 43880/119245 (epoch 13/35), loss = 0.001653 (0.337 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:18:58.984000: step 43900/119245 (epoch 13/35), loss = 0.000130 (0.579 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:19:08.196966: step 43920/119245 (epoch 13/35), loss = 0.000581 (0.500 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:19:17.634451: step 43940/119245 (epoch 13/35), loss = 0.001671 (0.461 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:19:28.453359: step 43960/119245 (epoch 13/35), loss = 0.105124 (0.395 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:19:37.976206: step 43980/119245 (epoch 13/35), loss = 0.078120 (0.624 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:19:48.121168: step 44000/119245 (epoch 13/35), loss = 0.389311 (0.411 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:19:57.785572: step 44020/119245 (epoch 13/35), loss = 0.000127 (0.359 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:20:07.428170: step 44040/119245 (epoch 13/35), loss = 0.170100 (0.539 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:20:15.443361: step 44060/119245 (epoch 13/35), loss = 0.090372 (0.390 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:20:24.651330: step 44080/119245 (epoch 13/35), loss = 0.007103 (0.421 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:20:35.300883: step 44100/119245 (epoch 13/35), loss = 0.032605 (0.449 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:20:44.264758: step 44120/119245 (epoch 13/35), loss = 0.000647 (0.523 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:20:52.739180: step 44140/119245 (epoch 13/35), loss = 0.014487 (0.448 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:21:01.864042: step 44160/119245 (epoch 13/35), loss = 0.005017 (0.519 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:21:12.028315: step 44180/119245 (epoch 13/35), loss = 0.012986 (0.477 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:21:22.575869: step 44200/119245 (epoch 13/35), loss = 0.000715 (0.582 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:21:32.342216: step 44220/119245 (epoch 13/35), loss = 0.004694 (0.748 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:21:42.103795: step 44240/119245 (epoch 13/35), loss = 0.000919 (0.514 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:21:51.566484: step 44260/119245 (epoch 13/35), loss = 0.000655 (0.577 sec/batch), lr: 0.030000\n",
            "2020-12-02 06:22:02.935070: step 44280/119245 (epoch 13/35), loss = 0.002178 (0.382 sec/batch), lr: 0.030000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  78.46%  R:  75.44%  F1:  76.92%  #: 338\n",
            "org:city_of_headquarters             P:  68.12%  R:  43.12%  F1:  52.81%  #: 109\n",
            "org:country_of_headquarters          P:  68.38%  R:  52.54%  F1:  59.42%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  86.67%  R:  68.42%  F1:  76.47%  #: 38\n",
            "org:founded_by                       P:  63.64%  R:  36.84%  F1:  46.67%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  80.33%  R:  57.65%  F1:  67.12%  #: 85\n",
            "org:number_of_employees/members      P:  69.57%  R:  59.26%  F1:  64.00%  #: 27\n",
            "org:parents                          P:  38.38%  R:  39.58%  F1:  38.97%  #: 96\n",
            "org:political/religious_affiliation  P:   0.00%  R:   0.00%  F1:   0.00%  #: 10\n",
            "org:shareholders                     P:  44.12%  R:  27.27%  F1:  33.71%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  64.18%  R:  61.43%  F1:  62.77%  #: 70\n",
            "org:subsidiaries                     P:  44.05%  R:  32.74%  F1:  37.56%  #: 113\n",
            "org:top_members/employees            P:  76.57%  R:  72.85%  F1:  74.66%  #: 534\n",
            "org:website                          P:  91.11%  R:  95.35%  F1:  93.18%  #: 86\n",
            "per:age                              P:  85.66%  R:  90.95%  F1:  88.22%  #: 243\n",
            "per:alternate_names                  P:  52.38%  R:  28.95%  F1:  37.29%  #: 38\n",
            "per:cause_of_death                   P:  89.17%  R:  63.69%  F1:  74.31%  #: 168\n",
            "per:charges                          P:  74.76%  R:  73.33%  F1:  74.04%  #: 105\n",
            "per:children                         P:  70.83%  R:  68.69%  F1:  69.74%  #: 99\n",
            "per:cities_of_residence              P:  42.72%  R:  49.16%  F1:  45.71%  #: 179\n",
            "per:city_of_birth                    P:  83.33%  R:  60.61%  F1:  70.18%  #: 33\n",
            "per:city_of_death                    P:  70.53%  R:  56.78%  F1:  62.91%  #: 118\n",
            "per:countries_of_residence           P:  32.48%  R:  61.50%  F1:  42.51%  #: 226\n",
            "per:country_of_birth                 P:  71.43%  R:  25.00%  F1:  37.04%  #: 20\n",
            "per:country_of_death                 P: 100.00%  R:   2.17%  F1:   4.26%  #: 46\n",
            "per:date_of_birth                    P:  93.10%  R:  87.10%  F1:  90.00%  #: 31\n",
            "per:date_of_death                    P:  78.34%  R:  59.71%  F1:  67.77%  #: 206\n",
            "per:employee_of                      P:  57.79%  R:  64.27%  F1:  60.86%  #: 375\n",
            "per:origin                           P:  66.67%  R:  45.71%  F1:  54.24%  #: 210\n",
            "per:other_family                     P:  62.50%  R:  12.50%  F1:  20.83%  #: 80\n",
            "per:parents                          P:  56.36%  R:  55.36%  F1:  55.86%  #: 56\n",
            "per:religion                         P:  55.56%  R:  56.60%  F1:  56.07%  #: 53\n",
            "per:schools_attended                 P:  81.25%  R:  52.00%  F1:  63.41%  #: 50\n",
            "per:siblings                         P:  59.38%  R:  63.33%  F1:  61.29%  #: 30\n",
            "per:spouse                           P:  62.90%  R:  73.58%  F1:  67.83%  #: 159\n",
            "per:stateorprovince_of_birth         P:  77.78%  R:  53.85%  F1:  63.64%  #: 26\n",
            "per:stateorprovince_of_death         P:  80.00%  R:  29.27%  F1:  42.86%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  37.40%  R:  63.89%  F1:  47.18%  #: 72\n",
            "per:title                            P:  78.28%  R:  81.18%  F1:  79.70%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17445 Guess as no relation\n",
            "5186 guess\n",
            "3460 correct\n",
            "5436 gold\n",
            "Precision (micro): 66.718%\n",
            "   Recall (micro): 63.650%\n",
            "       F1 (micro): 65.148%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 13: train_loss = 0.057386, dev_loss = 0.933005, dev_f1 = 0.6515\n",
            "model saved to ./saved_models/00/checkpoint_epoch_13.pt\n",
            "\n",
            "2020-12-02 06:25:00.077121: step 44300/119245 (epoch 14/35), loss = 0.218108 (0.350 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:25:10.440362: step 44320/119245 (epoch 14/35), loss = 0.051156 (0.629 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:25:20.938628: step 44340/119245 (epoch 14/35), loss = 0.004728 (0.434 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:25:29.773848: step 44360/119245 (epoch 14/35), loss = 0.001727 (0.507 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:25:39.587015: step 44380/119245 (epoch 14/35), loss = 0.037677 (0.300 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:25:49.418427: step 44400/119245 (epoch 14/35), loss = 0.004574 (0.338 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:25:59.211538: step 44420/119245 (epoch 14/35), loss = 0.000349 (0.352 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:26:08.082548: step 44440/119245 (epoch 14/35), loss = 0.000163 (0.463 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:26:16.960168: step 44460/119245 (epoch 14/35), loss = 0.059189 (0.389 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:26:26.894896: step 44480/119245 (epoch 14/35), loss = 0.129952 (0.383 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:26:36.782204: step 44500/119245 (epoch 14/35), loss = 0.376686 (0.420 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:26:45.826573: step 44520/119245 (epoch 14/35), loss = 0.014198 (0.304 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:26:55.591140: step 44540/119245 (epoch 14/35), loss = 0.000974 (0.624 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:27:05.928036: step 44560/119245 (epoch 14/35), loss = 0.036977 (0.375 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:27:14.023432: step 44580/119245 (epoch 14/35), loss = 0.006009 (0.432 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:27:23.094593: step 44600/119245 (epoch 14/35), loss = 0.025595 (0.406 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:27:34.173899: step 44620/119245 (epoch 14/35), loss = 0.068101 (0.973 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:27:43.636018: step 44640/119245 (epoch 14/35), loss = 0.258482 (0.528 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:27:54.749517: step 44660/119245 (epoch 14/35), loss = 0.000740 (0.634 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:28:03.958412: step 44680/119245 (epoch 14/35), loss = 0.003360 (0.447 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:28:14.558424: step 44700/119245 (epoch 14/35), loss = 0.000212 (0.517 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:28:23.376664: step 44720/119245 (epoch 14/35), loss = 0.347563 (0.372 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:28:35.381745: step 44740/119245 (epoch 14/35), loss = 0.004820 (0.600 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:28:45.329965: step 44760/119245 (epoch 14/35), loss = 0.000637 (0.532 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:28:55.265613: step 44780/119245 (epoch 14/35), loss = 0.004484 (0.608 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:29:05.818954: step 44800/119245 (epoch 14/35), loss = 0.000447 (0.381 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:29:14.992050: step 44820/119245 (epoch 14/35), loss = 0.076749 (0.391 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:29:25.845954: step 44840/119245 (epoch 14/35), loss = 0.002210 (0.412 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:29:34.853509: step 44860/119245 (epoch 14/35), loss = 0.074833 (0.461 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:29:43.780936: step 44880/119245 (epoch 14/35), loss = 0.000533 (0.378 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:29:52.671253: step 44900/119245 (epoch 14/35), loss = 0.023511 (0.449 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:30:01.917795: step 44920/119245 (epoch 14/35), loss = 0.001605 (0.381 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:30:11.066676: step 44940/119245 (epoch 14/35), loss = 0.000301 (0.539 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:30:21.450349: step 44960/119245 (epoch 14/35), loss = 0.000543 (0.580 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:30:32.478544: step 44980/119245 (epoch 14/35), loss = 0.000059 (0.379 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:30:42.322662: step 45000/119245 (epoch 14/35), loss = 0.031515 (0.426 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:30:52.163128: step 45020/119245 (epoch 14/35), loss = 0.000351 (1.036 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:31:01.044354: step 45040/119245 (epoch 14/35), loss = 0.002837 (0.391 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:31:10.922752: step 45060/119245 (epoch 14/35), loss = 0.211641 (0.510 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:31:20.249492: step 45080/119245 (epoch 14/35), loss = 0.037815 (0.459 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:31:30.132205: step 45100/119245 (epoch 14/35), loss = 0.001454 (0.499 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:31:39.218544: step 45120/119245 (epoch 14/35), loss = 0.015618 (0.521 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:31:48.516292: step 45140/119245 (epoch 14/35), loss = 0.047855 (0.326 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:31:58.831437: step 45160/119245 (epoch 14/35), loss = 0.000382 (0.420 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:32:10.499417: step 45180/119245 (epoch 14/35), loss = 0.452642 (0.602 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:32:20.484406: step 45200/119245 (epoch 14/35), loss = 0.007033 (0.504 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:32:29.646155: step 45220/119245 (epoch 14/35), loss = 0.005596 (0.321 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:32:38.672528: step 45240/119245 (epoch 14/35), loss = 0.062095 (0.473 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:32:47.303980: step 45260/119245 (epoch 14/35), loss = 0.000254 (0.312 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:32:55.767761: step 45280/119245 (epoch 14/35), loss = 0.074316 (0.339 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:33:04.330022: step 45300/119245 (epoch 14/35), loss = 0.138274 (0.338 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:33:13.245437: step 45320/119245 (epoch 14/35), loss = 0.000623 (0.368 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:33:24.055826: step 45340/119245 (epoch 14/35), loss = 0.002925 (0.620 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:33:33.749715: step 45360/119245 (epoch 14/35), loss = 0.000256 (1.025 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:33:45.072996: step 45380/119245 (epoch 14/35), loss = 0.052853 (0.585 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:33:55.328025: step 45400/119245 (epoch 14/35), loss = 0.102723 (0.315 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:34:04.332231: step 45420/119245 (epoch 14/35), loss = 0.016581 (0.511 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:34:14.805912: step 45440/119245 (epoch 14/35), loss = 0.007426 (0.432 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:34:24.668671: step 45460/119245 (epoch 14/35), loss = 0.000443 (0.290 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:34:33.887818: step 45480/119245 (epoch 14/35), loss = 0.001233 (0.447 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:34:44.098321: step 45500/119245 (epoch 14/35), loss = 0.003616 (0.495 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:34:53.360009: step 45520/119245 (epoch 14/35), loss = 0.000476 (0.311 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:35:02.457510: step 45540/119245 (epoch 14/35), loss = 0.201201 (0.524 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:35:12.529266: step 45560/119245 (epoch 14/35), loss = 0.000370 (0.574 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:35:24.774665: step 45580/119245 (epoch 14/35), loss = 0.035518 (0.538 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:35:36.143328: step 45600/119245 (epoch 14/35), loss = 0.001575 (0.552 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:35:46.522550: step 45620/119245 (epoch 14/35), loss = 0.113556 (0.585 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:35:55.795335: step 45640/119245 (epoch 14/35), loss = 0.005356 (0.446 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:36:05.561064: step 45660/119245 (epoch 14/35), loss = 0.000510 (0.399 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:36:14.450846: step 45680/119245 (epoch 14/35), loss = 0.054971 (0.388 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:36:24.067774: step 45700/119245 (epoch 14/35), loss = 0.075713 (0.542 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:36:33.870631: step 45720/119245 (epoch 14/35), loss = 0.170998 (0.435 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:36:43.436194: step 45740/119245 (epoch 14/35), loss = 0.001431 (0.642 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:36:53.317006: step 45760/119245 (epoch 14/35), loss = 0.006381 (0.410 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:37:02.471172: step 45780/119245 (epoch 14/35), loss = 0.001182 (0.391 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:37:11.698934: step 45800/119245 (epoch 14/35), loss = 0.000179 (0.354 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:37:21.805871: step 45820/119245 (epoch 14/35), loss = 0.016839 (0.508 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:37:31.661881: step 45840/119245 (epoch 14/35), loss = 0.001194 (0.407 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:37:41.369799: step 45860/119245 (epoch 14/35), loss = 0.001252 (0.329 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:37:51.330199: step 45880/119245 (epoch 14/35), loss = 0.006411 (0.427 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:38:01.050871: step 45900/119245 (epoch 14/35), loss = 0.000438 (0.396 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:38:10.599668: step 45920/119245 (epoch 14/35), loss = 0.004649 (0.798 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:38:20.796283: step 45940/119245 (epoch 14/35), loss = 0.000711 (0.441 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:38:30.176122: step 45960/119245 (epoch 14/35), loss = 0.388047 (0.762 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:38:39.555759: step 45980/119245 (epoch 14/35), loss = 0.009495 (0.434 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:38:49.174489: step 46000/119245 (epoch 14/35), loss = 0.127295 (0.404 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:39:00.099751: step 46020/119245 (epoch 14/35), loss = 0.008879 (0.520 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:39:09.999000: step 46040/119245 (epoch 14/35), loss = 0.000333 (0.324 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:39:19.704392: step 46060/119245 (epoch 14/35), loss = 0.003295 (0.452 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:39:29.731005: step 46080/119245 (epoch 14/35), loss = 0.001931 (0.400 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:39:38.811210: step 46100/119245 (epoch 14/35), loss = 0.001083 (0.384 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:39:48.276531: step 46120/119245 (epoch 14/35), loss = 0.001887 (0.487 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:39:58.007731: step 46140/119245 (epoch 14/35), loss = 0.003201 (0.759 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:40:09.545994: step 46160/119245 (epoch 14/35), loss = 0.000635 (0.403 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:40:20.883287: step 46180/119245 (epoch 14/35), loss = 0.025738 (0.438 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:40:29.890083: step 46200/119245 (epoch 14/35), loss = 0.001090 (0.633 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:40:39.846643: step 46220/119245 (epoch 14/35), loss = 0.004795 (0.536 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:40:49.806084: step 46240/119245 (epoch 14/35), loss = 0.012291 (0.472 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:41:01.767876: step 46260/119245 (epoch 14/35), loss = 0.007304 (0.739 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:41:11.166130: step 46280/119245 (epoch 14/35), loss = 0.001637 (0.448 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:41:21.229415: step 46300/119245 (epoch 14/35), loss = 0.001502 (0.424 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:41:30.816425: step 46320/119245 (epoch 14/35), loss = 0.000460 (0.337 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:41:40.825808: step 46340/119245 (epoch 14/35), loss = 0.000022 (0.433 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:41:50.694763: step 46360/119245 (epoch 14/35), loss = 0.000262 (0.338 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:42:00.289059: step 46380/119245 (epoch 14/35), loss = 0.000531 (0.720 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:42:09.684050: step 46400/119245 (epoch 14/35), loss = 0.001089 (0.431 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:42:19.493742: step 46420/119245 (epoch 14/35), loss = 0.002504 (0.357 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:42:28.797334: step 46440/119245 (epoch 14/35), loss = 0.034411 (0.399 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:42:40.126313: step 46460/119245 (epoch 14/35), loss = 0.024095 (1.636 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:42:49.222137: step 46480/119245 (epoch 14/35), loss = 0.001065 (0.349 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:42:59.142345: step 46500/119245 (epoch 14/35), loss = 0.046756 (0.423 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:43:10.163215: step 46520/119245 (epoch 14/35), loss = 0.229404 (0.400 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:43:18.738160: step 46540/119245 (epoch 14/35), loss = 0.010824 (0.410 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:43:28.710749: step 46560/119245 (epoch 14/35), loss = 0.029609 (0.459 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:43:37.712310: step 46580/119245 (epoch 14/35), loss = 0.003936 (0.383 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:43:47.644122: step 46600/119245 (epoch 14/35), loss = 0.000489 (0.379 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:43:58.186128: step 46620/119245 (epoch 14/35), loss = 0.000905 (0.417 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:44:08.934724: step 46640/119245 (epoch 14/35), loss = 0.000221 (0.740 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:44:19.154130: step 46660/119245 (epoch 14/35), loss = 0.004010 (0.615 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:44:29.518278: step 46680/119245 (epoch 14/35), loss = 0.000754 (0.828 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:44:38.590018: step 46700/119245 (epoch 14/35), loss = 0.000870 (0.607 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:44:48.258493: step 46720/119245 (epoch 14/35), loss = 0.004422 (0.477 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:44:59.215492: step 46740/119245 (epoch 14/35), loss = 0.084014 (0.771 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:45:08.384372: step 46760/119245 (epoch 14/35), loss = 0.001085 (0.408 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:45:17.915473: step 46780/119245 (epoch 14/35), loss = 0.010100 (0.284 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:45:28.639056: step 46800/119245 (epoch 14/35), loss = 0.012454 (0.416 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:45:37.702604: step 46820/119245 (epoch 14/35), loss = 0.003280 (0.552 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:45:46.872599: step 46840/119245 (epoch 14/35), loss = 0.002046 (0.443 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:45:56.530839: step 46860/119245 (epoch 14/35), loss = 0.158695 (0.458 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:46:05.836391: step 46880/119245 (epoch 14/35), loss = 0.000984 (0.385 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:46:14.719530: step 46900/119245 (epoch 14/35), loss = 0.032060 (0.528 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:46:25.513272: step 46920/119245 (epoch 14/35), loss = 0.220403 (0.428 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:46:33.979215: step 46940/119245 (epoch 14/35), loss = 0.017649 (0.323 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:46:43.156140: step 46960/119245 (epoch 14/35), loss = 0.000727 (0.367 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:46:52.629450: step 46980/119245 (epoch 14/35), loss = 0.136962 (0.409 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:47:03.081707: step 47000/119245 (epoch 14/35), loss = 0.018721 (0.378 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:47:12.770965: step 47020/119245 (epoch 14/35), loss = 0.001433 (0.386 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:47:23.488422: step 47040/119245 (epoch 14/35), loss = 0.276209 (0.445 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:47:32.200058: step 47060/119245 (epoch 14/35), loss = 0.001130 (0.339 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:47:41.663824: step 47080/119245 (epoch 14/35), loss = 0.000253 (0.460 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:47:51.261807: step 47100/119245 (epoch 14/35), loss = 0.349982 (0.532 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:48:01.027710: step 47120/119245 (epoch 14/35), loss = 0.368371 (0.516 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:48:11.056065: step 47140/119245 (epoch 14/35), loss = 0.003951 (0.353 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:48:19.984985: step 47160/119245 (epoch 14/35), loss = 0.098431 (0.360 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:48:29.495964: step 47180/119245 (epoch 14/35), loss = 0.001785 (0.451 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:48:38.463857: step 47200/119245 (epoch 14/35), loss = 0.375288 (0.319 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:48:47.691513: step 47220/119245 (epoch 14/35), loss = 0.015566 (0.415 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:48:56.935503: step 47240/119245 (epoch 14/35), loss = 0.001204 (0.404 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:49:08.333377: step 47260/119245 (epoch 14/35), loss = 0.172490 (0.441 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:49:17.232079: step 47280/119245 (epoch 14/35), loss = 0.006071 (0.460 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:49:28.023177: step 47300/119245 (epoch 14/35), loss = 0.007849 (0.441 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:49:37.367904: step 47320/119245 (epoch 14/35), loss = 0.012617 (0.306 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:49:47.070603: step 47340/119245 (epoch 14/35), loss = 0.010904 (0.388 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:49:57.856376: step 47360/119245 (epoch 14/35), loss = 0.203738 (0.510 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:50:06.729617: step 47380/119245 (epoch 14/35), loss = 0.017557 (0.421 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:50:16.870129: step 47400/119245 (epoch 14/35), loss = 0.054131 (0.462 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:50:26.605475: step 47420/119245 (epoch 14/35), loss = 0.000078 (0.507 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:50:35.985335: step 47440/119245 (epoch 14/35), loss = 0.000299 (0.507 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:50:44.974849: step 47460/119245 (epoch 14/35), loss = 0.161932 (0.415 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:50:53.743513: step 47480/119245 (epoch 14/35), loss = 0.004570 (0.405 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:51:04.327175: step 47500/119245 (epoch 14/35), loss = 0.001482 (0.504 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:51:13.425718: step 47520/119245 (epoch 14/35), loss = 0.001667 (0.607 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:51:22.049455: step 47540/119245 (epoch 14/35), loss = 0.068408 (0.471 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:51:30.700409: step 47560/119245 (epoch 14/35), loss = 0.236236 (0.395 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:51:41.346533: step 47580/119245 (epoch 14/35), loss = 0.150715 (0.371 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:51:51.254496: step 47600/119245 (epoch 14/35), loss = 0.000969 (1.504 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:52:00.959452: step 47620/119245 (epoch 14/35), loss = 0.021686 (0.334 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:52:10.665653: step 47640/119245 (epoch 14/35), loss = 0.001466 (0.406 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:52:20.314777: step 47660/119245 (epoch 14/35), loss = 0.016153 (0.392 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:52:32.106942: step 47680/119245 (epoch 14/35), loss = 0.008992 (1.631 sec/batch), lr: 0.027000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  78.21%  R:  77.51%  F1:  77.86%  #: 338\n",
            "org:city_of_headquarters             P:  65.91%  R:  53.21%  F1:  58.88%  #: 109\n",
            "org:country_of_headquarters          P:  58.89%  R:  59.89%  F1:  59.38%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  87.50%  R:  73.68%  F1:  80.00%  #: 38\n",
            "org:founded_by                       P:  71.11%  R:  42.11%  F1:  52.89%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  71.62%  R:  62.35%  F1:  66.67%  #: 85\n",
            "org:number_of_employees/members      P:  60.71%  R:  62.96%  F1:  61.82%  #: 27\n",
            "org:parents                          P:  43.55%  R:  28.12%  F1:  34.18%  #: 96\n",
            "org:political/religious_affiliation  P:  23.08%  R:  30.00%  F1:  26.09%  #: 10\n",
            "org:shareholders                     P:  36.36%  R:  29.09%  F1:  32.32%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  56.18%  R:  71.43%  F1:  62.89%  #: 70\n",
            "org:subsidiaries                     P:  37.86%  R:  34.51%  F1:  36.11%  #: 113\n",
            "org:top_members/employees            P:  70.40%  R:  78.84%  F1:  74.38%  #: 534\n",
            "org:website                          P:  93.18%  R:  95.35%  F1:  94.25%  #: 86\n",
            "per:age                              P:  84.58%  R:  88.07%  F1:  86.29%  #: 243\n",
            "per:alternate_names                  P:  62.50%  R:  26.32%  F1:  37.04%  #: 38\n",
            "per:cause_of_death                   P:  83.23%  R:  76.79%  F1:  79.88%  #: 168\n",
            "per:charges                          P:  68.25%  R:  81.90%  F1:  74.46%  #: 105\n",
            "per:children                         P:  72.41%  R:  63.64%  F1:  67.74%  #: 99\n",
            "per:cities_of_residence              P:  47.85%  R:  49.72%  F1:  48.77%  #: 179\n",
            "per:city_of_birth                    P:  70.27%  R:  78.79%  F1:  74.29%  #: 33\n",
            "per:city_of_death                    P:  69.23%  R:  61.02%  F1:  64.86%  #: 118\n",
            "per:countries_of_residence           P:  38.16%  R:  51.33%  F1:  43.77%  #: 226\n",
            "per:country_of_birth                 P:  54.55%  R:  30.00%  F1:  38.71%  #: 20\n",
            "per:country_of_death                 P:  91.67%  R:  23.91%  F1:  37.93%  #: 46\n",
            "per:date_of_birth                    P:  96.55%  R:  90.32%  F1:  93.33%  #: 31\n",
            "per:date_of_death                    P:  80.66%  R:  70.87%  F1:  75.45%  #: 206\n",
            "per:employee_of                      P:  58.94%  R:  68.53%  F1:  63.38%  #: 375\n",
            "per:origin                           P:  54.43%  R:  61.43%  F1:  57.72%  #: 210\n",
            "per:other_family                     P:  62.50%  R:  18.75%  F1:  28.85%  #: 80\n",
            "per:parents                          P:  62.79%  R:  48.21%  F1:  54.55%  #: 56\n",
            "per:religion                         P:  52.05%  R:  71.70%  F1:  60.32%  #: 53\n",
            "per:schools_attended                 P:  76.09%  R:  70.00%  F1:  72.92%  #: 50\n",
            "per:siblings                         P:  63.33%  R:  63.33%  F1:  63.33%  #: 30\n",
            "per:spouse                           P:  74.78%  R:  54.09%  F1:  62.77%  #: 159\n",
            "per:stateorprovince_of_birth         P:  66.67%  R:  53.85%  F1:  59.57%  #: 26\n",
            "per:stateorprovince_of_death         P:  56.76%  R:  51.22%  F1:  53.85%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  38.10%  R:  66.67%  F1:  48.48%  #: 72\n",
            "per:title                            P:  74.44%  R:  87.16%  F1:  80.30%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17076 Guess as no relation\n",
            "5555 guess\n",
            "3680 correct\n",
            "5436 gold\n",
            "Precision (micro): 66.247%\n",
            "   Recall (micro): 67.697%\n",
            "       F1 (micro): 66.964%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 14: train_loss = 0.045767, dev_loss = 0.959055, dev_f1 = 0.6696\n",
            "model saved to ./saved_models/00/checkpoint_epoch_14.pt\n",
            "new best model saved.\n",
            "\n",
            "2020-12-02 06:55:31.476573: step 47700/119245 (epoch 15/35), loss = 0.023286 (0.382 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:55:41.238007: step 47720/119245 (epoch 15/35), loss = 0.000603 (0.423 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:55:53.137425: step 47740/119245 (epoch 15/35), loss = 0.004825 (0.751 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:56:01.685040: step 47760/119245 (epoch 15/35), loss = 0.002190 (0.505 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:56:12.406350: step 47780/119245 (epoch 15/35), loss = 0.094941 (0.619 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:56:21.530887: step 47800/119245 (epoch 15/35), loss = 0.002743 (0.584 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:56:31.240993: step 47820/119245 (epoch 15/35), loss = 0.074742 (0.531 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:56:40.586187: step 47840/119245 (epoch 15/35), loss = 0.141278 (0.489 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:56:48.925135: step 47860/119245 (epoch 15/35), loss = 0.001115 (0.526 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:56:58.615987: step 47880/119245 (epoch 15/35), loss = 0.002243 (0.424 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:57:08.543460: step 47900/119245 (epoch 15/35), loss = 0.001543 (0.515 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:57:18.041575: step 47920/119245 (epoch 15/35), loss = 0.161673 (0.474 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:57:26.587971: step 47940/119245 (epoch 15/35), loss = 0.003104 (0.516 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:57:36.747087: step 47960/119245 (epoch 15/35), loss = 0.005641 (0.678 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:57:46.679528: step 47980/119245 (epoch 15/35), loss = 0.058022 (0.475 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:57:55.123642: step 48000/119245 (epoch 15/35), loss = 0.018713 (0.342 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:58:05.836904: step 48020/119245 (epoch 15/35), loss = 0.095572 (0.899 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:58:15.723352: step 48040/119245 (epoch 15/35), loss = 0.003198 (0.773 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:58:26.981212: step 48060/119245 (epoch 15/35), loss = 0.086376 (0.340 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:58:35.622745: step 48080/119245 (epoch 15/35), loss = 0.001053 (0.382 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:58:46.014099: step 48100/119245 (epoch 15/35), loss = 0.003260 (0.612 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:58:55.797801: step 48120/119245 (epoch 15/35), loss = 0.001810 (0.389 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:59:05.122385: step 48140/119245 (epoch 15/35), loss = 0.050282 (0.445 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:59:17.356044: step 48160/119245 (epoch 15/35), loss = 0.010506 (0.462 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:59:26.471726: step 48180/119245 (epoch 15/35), loss = 0.002060 (0.380 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:59:37.691810: step 48200/119245 (epoch 15/35), loss = 0.003237 (0.409 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:59:46.876058: step 48220/119245 (epoch 15/35), loss = 0.076681 (0.366 sec/batch), lr: 0.027000\n",
            "2020-12-02 06:59:58.288634: step 48240/119245 (epoch 15/35), loss = 0.009384 (0.436 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:00:06.992067: step 48260/119245 (epoch 15/35), loss = 0.007560 (0.512 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:00:16.145237: step 48280/119245 (epoch 15/35), loss = 0.012074 (0.338 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:00:25.025834: step 48300/119245 (epoch 15/35), loss = 0.008262 (0.583 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:00:34.300882: step 48320/119245 (epoch 15/35), loss = 0.001198 (0.608 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:00:43.233652: step 48340/119245 (epoch 15/35), loss = 0.000890 (0.590 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:00:53.267129: step 48360/119245 (epoch 15/35), loss = 0.001252 (0.350 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:01:03.581252: step 48380/119245 (epoch 15/35), loss = 0.024316 (0.418 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:01:13.219488: step 48400/119245 (epoch 15/35), loss = 0.001763 (0.361 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:01:23.339723: step 48420/119245 (epoch 15/35), loss = 0.074697 (0.644 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:01:33.288432: step 48440/119245 (epoch 15/35), loss = 0.002623 (0.393 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:01:42.502774: step 48460/119245 (epoch 15/35), loss = 0.066869 (0.309 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:01:52.402269: step 48480/119245 (epoch 15/35), loss = 0.020959 (0.421 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:02:02.003602: step 48500/119245 (epoch 15/35), loss = 0.194763 (0.443 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:02:11.229451: step 48520/119245 (epoch 15/35), loss = 0.088304 (0.469 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:02:20.312089: step 48540/119245 (epoch 15/35), loss = 0.000096 (0.539 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:02:29.426267: step 48560/119245 (epoch 15/35), loss = 0.000343 (0.424 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:02:41.033303: step 48580/119245 (epoch 15/35), loss = 0.003261 (0.551 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:02:52.109704: step 48600/119245 (epoch 15/35), loss = 0.161782 (0.607 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:03:01.738018: step 48620/119245 (epoch 15/35), loss = 0.001342 (0.608 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:03:10.359241: step 48640/119245 (epoch 15/35), loss = 0.002661 (0.382 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:03:19.199750: step 48660/119245 (epoch 15/35), loss = 0.000638 (0.465 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:03:27.689587: step 48680/119245 (epoch 15/35), loss = 0.000213 (0.485 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:03:36.500481: step 48700/119245 (epoch 15/35), loss = 0.003479 (0.386 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:03:45.315359: step 48720/119245 (epoch 15/35), loss = 0.213932 (0.536 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:03:55.516139: step 48740/119245 (epoch 15/35), loss = 0.004473 (1.319 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:04:04.080168: step 48760/119245 (epoch 15/35), loss = 0.000130 (0.389 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:04:15.159217: step 48780/119245 (epoch 15/35), loss = 0.284615 (0.577 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:04:26.890529: step 48800/119245 (epoch 15/35), loss = 0.024066 (0.375 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:04:35.836501: step 48820/119245 (epoch 15/35), loss = 0.000566 (0.408 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:04:45.904169: step 48840/119245 (epoch 15/35), loss = 0.095098 (0.970 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:04:56.523138: step 48860/119245 (epoch 15/35), loss = 0.339116 (0.474 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:05:05.549742: step 48880/119245 (epoch 15/35), loss = 0.000393 (0.332 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:05:15.547170: step 48900/119245 (epoch 15/35), loss = 0.137537 (0.388 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:05:24.454146: step 48920/119245 (epoch 15/35), loss = 0.124389 (0.417 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:05:34.026895: step 48940/119245 (epoch 15/35), loss = 0.000460 (0.891 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:05:43.604218: step 48960/119245 (epoch 15/35), loss = 0.000172 (0.677 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:05:55.391275: step 48980/119245 (epoch 15/35), loss = 0.001239 (1.654 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:06:07.185792: step 49000/119245 (epoch 15/35), loss = 0.003166 (0.324 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:06:17.291013: step 49020/119245 (epoch 15/35), loss = 0.141650 (0.535 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:06:27.347376: step 49040/119245 (epoch 15/35), loss = 0.005260 (0.488 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:06:36.801701: step 49060/119245 (epoch 15/35), loss = 0.004594 (0.776 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:06:45.984348: step 49080/119245 (epoch 15/35), loss = 0.019847 (0.457 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:06:54.680770: step 49100/119245 (epoch 15/35), loss = 0.000331 (0.420 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:07:04.790879: step 49120/119245 (epoch 15/35), loss = 0.005801 (0.490 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:07:14.540199: step 49140/119245 (epoch 15/35), loss = 0.000248 (0.608 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:07:24.474896: step 49160/119245 (epoch 15/35), loss = 0.004852 (0.483 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:07:33.906222: step 49180/119245 (epoch 15/35), loss = 0.009390 (0.428 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:07:42.473396: step 49200/119245 (epoch 15/35), loss = 0.001197 (0.383 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:07:52.495542: step 49220/119245 (epoch 15/35), loss = 0.002717 (0.401 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:08:02.706687: step 49240/119245 (epoch 15/35), loss = 0.059480 (0.370 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:08:12.120147: step 49260/119245 (epoch 15/35), loss = 0.000485 (0.430 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:08:22.150264: step 49280/119245 (epoch 15/35), loss = 0.041234 (0.554 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:08:31.211658: step 49300/119245 (epoch 15/35), loss = 0.001339 (0.437 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:08:41.177373: step 49320/119245 (epoch 15/35), loss = 0.000069 (0.464 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:08:50.993304: step 49340/119245 (epoch 15/35), loss = 0.000026 (0.498 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:09:00.620319: step 49360/119245 (epoch 15/35), loss = 0.009849 (0.393 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:09:10.817559: step 49380/119245 (epoch 15/35), loss = 0.000212 (0.588 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:09:20.235023: step 49400/119245 (epoch 15/35), loss = 0.473671 (0.465 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:09:30.982808: step 49420/119245 (epoch 15/35), loss = 0.001768 (0.324 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:09:41.125316: step 49440/119245 (epoch 15/35), loss = 0.001298 (0.528 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:09:50.855080: step 49460/119245 (epoch 15/35), loss = 0.000599 (0.513 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:09:59.724149: step 49480/119245 (epoch 15/35), loss = 0.001806 (0.492 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:10:09.905796: step 49500/119245 (epoch 15/35), loss = 0.000149 (0.368 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:10:19.382210: step 49520/119245 (epoch 15/35), loss = 0.001265 (0.452 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:10:28.802347: step 49540/119245 (epoch 15/35), loss = 0.000145 (0.609 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:10:40.475081: step 49560/119245 (epoch 15/35), loss = 0.003330 (0.535 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:10:49.598016: step 49580/119245 (epoch 15/35), loss = 0.136497 (0.433 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:11:01.081799: step 49600/119245 (epoch 15/35), loss = 0.016476 (0.746 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:11:09.558347: step 49620/119245 (epoch 15/35), loss = 0.000247 (0.458 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:11:20.565666: step 49640/119245 (epoch 15/35), loss = 0.000330 (0.420 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:11:30.813946: step 49660/119245 (epoch 15/35), loss = 0.022553 (0.377 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:11:42.054407: step 49680/119245 (epoch 15/35), loss = 0.005384 (0.408 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:11:51.957016: step 49700/119245 (epoch 15/35), loss = 0.000390 (0.603 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:12:01.679011: step 49720/119245 (epoch 15/35), loss = 0.003944 (0.439 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:12:12.136116: step 49740/119245 (epoch 15/35), loss = 0.131688 (0.335 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:12:20.795262: step 49760/119245 (epoch 15/35), loss = 0.004552 (0.555 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:12:31.159561: step 49780/119245 (epoch 15/35), loss = 0.006315 (0.427 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:12:40.400216: step 49800/119245 (epoch 15/35), loss = 0.001085 (0.400 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:12:50.821748: step 49820/119245 (epoch 15/35), loss = 0.026975 (0.543 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:13:00.072415: step 49840/119245 (epoch 15/35), loss = 0.002819 (0.644 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:13:10.117461: step 49860/119245 (epoch 15/35), loss = 0.118212 (0.386 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:13:19.816703: step 49880/119245 (epoch 15/35), loss = 0.061468 (0.625 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:13:30.345773: step 49900/119245 (epoch 15/35), loss = 0.001912 (0.473 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:13:40.658718: step 49920/119245 (epoch 15/35), loss = 0.029260 (0.619 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:13:50.017997: step 49940/119245 (epoch 15/35), loss = 0.046698 (0.467 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:13:59.824372: step 49960/119245 (epoch 15/35), loss = 0.002319 (0.628 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:14:08.615239: step 49980/119245 (epoch 15/35), loss = 0.013070 (0.490 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:14:18.382359: step 50000/119245 (epoch 15/35), loss = 0.000747 (0.475 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:14:27.798662: step 50020/119245 (epoch 15/35), loss = 0.002127 (0.389 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:14:38.258467: step 50040/119245 (epoch 15/35), loss = 0.009609 (0.369 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:14:49.906139: step 50060/119245 (epoch 15/35), loss = 0.020752 (0.621 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:15:00.252930: step 50080/119245 (epoch 15/35), loss = 0.071465 (0.456 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:15:09.519710: step 50100/119245 (epoch 15/35), loss = 0.001201 (0.506 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:15:18.809865: step 50120/119245 (epoch 15/35), loss = 0.000038 (0.828 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:15:29.128509: step 50140/119245 (epoch 15/35), loss = 0.000244 (0.489 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:15:38.830069: step 50160/119245 (epoch 15/35), loss = 0.000472 (0.495 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:15:49.143971: step 50180/119245 (epoch 15/35), loss = 0.000430 (0.580 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:15:59.154339: step 50200/119245 (epoch 15/35), loss = 0.013341 (0.550 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:16:08.464795: step 50220/119245 (epoch 15/35), loss = 0.025114 (0.432 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:16:17.287879: step 50240/119245 (epoch 15/35), loss = 0.001694 (0.428 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:16:27.004045: step 50260/119245 (epoch 15/35), loss = 0.000221 (0.373 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:16:36.624458: step 50280/119245 (epoch 15/35), loss = 0.004110 (0.342 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:16:45.253516: step 50300/119245 (epoch 15/35), loss = 0.002180 (0.392 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:16:55.970075: step 50320/119245 (epoch 15/35), loss = 0.001138 (0.551 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:17:05.239508: step 50340/119245 (epoch 15/35), loss = 0.000175 (0.401 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:17:14.099838: step 50360/119245 (epoch 15/35), loss = 0.018282 (0.432 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:17:22.617596: step 50380/119245 (epoch 15/35), loss = 0.008925 (0.400 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:17:32.395086: step 50400/119245 (epoch 15/35), loss = 0.048815 (0.628 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:17:43.782864: step 50420/119245 (epoch 15/35), loss = 0.000086 (0.423 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:17:52.937144: step 50440/119245 (epoch 15/35), loss = 0.001085 (0.335 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:18:03.093886: step 50460/119245 (epoch 15/35), loss = 0.000943 (0.424 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:18:12.060654: step 50480/119245 (epoch 15/35), loss = 0.012886 (0.429 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:18:21.691966: step 50500/119245 (epoch 15/35), loss = 0.001125 (0.431 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:18:31.508175: step 50520/119245 (epoch 15/35), loss = 0.000382 (0.486 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:18:41.047698: step 50540/119245 (epoch 15/35), loss = 0.014406 (0.411 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:18:50.791796: step 50560/119245 (epoch 15/35), loss = 0.005293 (0.452 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:19:00.195815: step 50580/119245 (epoch 15/35), loss = 0.001319 (0.384 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:19:09.460235: step 50600/119245 (epoch 15/35), loss = 0.008463 (0.413 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:19:18.474592: step 50620/119245 (epoch 15/35), loss = 0.000084 (0.388 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:19:27.723456: step 50640/119245 (epoch 15/35), loss = 0.000027 (0.889 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:19:39.281206: step 50660/119245 (epoch 15/35), loss = 0.003013 (0.441 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:19:47.957889: step 50680/119245 (epoch 15/35), loss = 0.000497 (0.349 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:19:57.268098: step 50700/119245 (epoch 15/35), loss = 0.000401 (0.343 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:20:08.008205: step 50720/119245 (epoch 15/35), loss = 0.004027 (0.522 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:20:17.228824: step 50740/119245 (epoch 15/35), loss = 0.000205 (0.458 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:20:27.382169: step 50760/119245 (epoch 15/35), loss = 0.048742 (0.629 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:20:37.624296: step 50780/119245 (epoch 15/35), loss = 0.000432 (0.656 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:20:47.244910: step 50800/119245 (epoch 15/35), loss = 0.000896 (0.436 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:20:57.133908: step 50820/119245 (epoch 15/35), loss = 0.000712 (0.378 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:21:06.428644: step 50840/119245 (epoch 15/35), loss = 0.175136 (0.386 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:21:16.058516: step 50860/119245 (epoch 15/35), loss = 0.000790 (0.586 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:21:24.060589: step 50880/119245 (epoch 15/35), loss = 0.001785 (0.433 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:21:34.620174: step 50900/119245 (epoch 15/35), loss = 0.002494 (0.385 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:21:44.006581: step 50920/119245 (epoch 15/35), loss = 0.001280 (0.323 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:21:52.894064: step 50940/119245 (epoch 15/35), loss = 0.004675 (0.409 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:22:01.293901: step 50960/119245 (epoch 15/35), loss = 0.000790 (0.415 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:22:11.556549: step 50980/119245 (epoch 15/35), loss = 0.003768 (0.537 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:22:20.468368: step 51000/119245 (epoch 15/35), loss = 0.000426 (0.330 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:22:31.399249: step 51020/119245 (epoch 15/35), loss = 0.020775 (0.383 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:22:41.170396: step 51040/119245 (epoch 15/35), loss = 0.019924 (0.392 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:22:50.682094: step 51060/119245 (epoch 15/35), loss = 0.213912 (0.627 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:23:00.473110: step 51080/119245 (epoch 15/35), loss = 0.087400 (0.400 sec/batch), lr: 0.027000\n",
            "2020-12-02 07:23:11.467687: step 51100/119245 (epoch 15/35), loss = 0.002682 (0.415 sec/batch), lr: 0.027000\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  77.97%  R:  54.44%  F1:  64.11%  #: 338\n",
            "org:city_of_headquarters             P:  71.79%  R:  51.38%  F1:  59.89%  #: 109\n",
            "org:country_of_headquarters          P:  51.43%  R:  71.19%  F1:  59.72%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  82.14%  R:  60.53%  F1:  69.70%  #: 38\n",
            "org:founded_by                       P:  49.46%  R:  60.53%  F1:  54.44%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  75.41%  R:  54.12%  F1:  63.01%  #: 85\n",
            "org:number_of_employees/members      P:  85.71%  R:  44.44%  F1:  58.54%  #: 27\n",
            "org:parents                          P:  42.19%  R:  28.12%  F1:  33.75%  #: 96\n",
            "org:political/religious_affiliation  P:  14.29%  R:  10.00%  F1:  11.76%  #: 10\n",
            "org:shareholders                     P:  39.53%  R:  30.91%  F1:  34.69%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  68.57%  R:  68.57%  F1:  68.57%  #: 70\n",
            "org:subsidiaries                     P:  48.89%  R:  38.94%  F1:  43.35%  #: 113\n",
            "org:top_members/employees            P:  85.06%  R:  69.29%  F1:  76.37%  #: 534\n",
            "org:website                          P:  92.22%  R:  96.51%  F1:  94.32%  #: 86\n",
            "per:age                              P:  88.38%  R:  87.65%  F1:  88.02%  #: 243\n",
            "per:alternate_names                  P:  71.43%  R:  39.47%  F1:  50.85%  #: 38\n",
            "per:cause_of_death                   P:  86.89%  R:  63.10%  F1:  73.10%  #: 168\n",
            "per:charges                          P:  76.92%  R:  66.67%  F1:  71.43%  #: 105\n",
            "per:children                         P:  66.67%  R:  64.65%  F1:  65.64%  #: 99\n",
            "per:cities_of_residence              P:  46.90%  R:  37.99%  F1:  41.98%  #: 179\n",
            "per:city_of_birth                    P:  76.47%  R:  78.79%  F1:  77.61%  #: 33\n",
            "per:city_of_death                    P:  68.63%  R:  59.32%  F1:  63.64%  #: 118\n",
            "per:countries_of_residence           P:  35.48%  R:  53.54%  F1:  42.68%  #: 226\n",
            "per:country_of_birth                 P:  50.00%  R:  35.00%  F1:  41.18%  #: 20\n",
            "per:country_of_death                 P: 100.00%  R:   2.17%  F1:   4.26%  #: 46\n",
            "per:date_of_birth                    P:  96.55%  R:  90.32%  F1:  93.33%  #: 31\n",
            "per:date_of_death                    P:  83.33%  R:  58.25%  F1:  68.57%  #: 206\n",
            "per:employee_of                      P:  61.43%  R:  59.47%  F1:  60.43%  #: 375\n",
            "per:origin                           P:  69.23%  R:  51.43%  F1:  59.02%  #: 210\n",
            "per:other_family                     P:  39.29%  R:  13.75%  F1:  20.37%  #: 80\n",
            "per:parents                          P:  58.49%  R:  55.36%  F1:  56.88%  #: 56\n",
            "per:religion                         P:  55.77%  R:  54.72%  F1:  55.24%  #: 53\n",
            "per:schools_attended                 P:  82.05%  R:  64.00%  F1:  71.91%  #: 50\n",
            "per:siblings                         P:  48.65%  R:  60.00%  F1:  53.73%  #: 30\n",
            "per:spouse                           P:  64.67%  R:  67.92%  F1:  66.26%  #: 159\n",
            "per:stateorprovince_of_birth         P:  66.67%  R:  53.85%  F1:  59.57%  #: 26\n",
            "per:stateorprovince_of_death         P:  72.73%  R:  58.54%  F1:  64.86%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  41.43%  R:  40.28%  F1:  40.85%  #: 72\n",
            "per:title                            P:  79.52%  R:  79.43%  F1:  79.48%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17738 Guess as no relation\n",
            "4893 guess\n",
            "3349 correct\n",
            "5436 gold\n",
            "Precision (micro): 68.445%\n",
            "   Recall (micro): 61.608%\n",
            "       F1 (micro): 64.847%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 15: train_loss = 0.039035, dev_loss = 1.025360, dev_f1 = 0.6485\n",
            "model saved to ./saved_models/00/checkpoint_epoch_15.pt\n",
            "\n",
            "2020-12-02 07:26:08.568610: step 51120/119245 (epoch 16/35), loss = 0.057711 (0.390 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:26:19.182652: step 51140/119245 (epoch 16/35), loss = 0.015681 (0.464 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:26:28.729787: step 51160/119245 (epoch 16/35), loss = 0.000250 (0.419 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:26:39.309227: step 51180/119245 (epoch 16/35), loss = 0.019701 (1.646 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:26:47.938528: step 51200/119245 (epoch 16/35), loss = 0.000126 (0.489 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:26:57.424335: step 51220/119245 (epoch 16/35), loss = 0.004510 (0.319 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:27:07.402482: step 51240/119245 (epoch 16/35), loss = 0.021906 (0.417 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:27:16.267446: step 51260/119245 (epoch 16/35), loss = 0.000264 (0.332 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:27:25.207949: step 51280/119245 (epoch 16/35), loss = 0.000181 (0.515 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:27:35.347554: step 51300/119245 (epoch 16/35), loss = 0.000345 (0.431 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:27:44.893761: step 51320/119245 (epoch 16/35), loss = 0.106091 (0.476 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:27:53.829436: step 51340/119245 (epoch 16/35), loss = 0.000821 (0.390 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:28:03.543742: step 51360/119245 (epoch 16/35), loss = 0.000028 (0.409 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:28:13.970747: step 51380/119245 (epoch 16/35), loss = 0.331278 (0.647 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:28:22.092391: step 51400/119245 (epoch 16/35), loss = 0.007639 (0.641 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:28:31.458373: step 51420/119245 (epoch 16/35), loss = 0.000653 (0.308 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:28:42.376029: step 51440/119245 (epoch 16/35), loss = 0.000085 (0.673 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:28:52.277058: step 51460/119245 (epoch 16/35), loss = 0.000255 (0.698 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:29:02.462650: step 51480/119245 (epoch 16/35), loss = 0.152860 (0.356 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:29:11.456124: step 51500/119245 (epoch 16/35), loss = 0.000260 (0.340 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:29:22.555507: step 51520/119245 (epoch 16/35), loss = 0.023022 (0.633 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:29:31.653104: step 51540/119245 (epoch 16/35), loss = 0.000481 (0.474 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:29:44.153009: step 51560/119245 (epoch 16/35), loss = 0.000286 (0.438 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:29:53.420090: step 51580/119245 (epoch 16/35), loss = 0.005090 (0.487 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:30:03.458651: step 51600/119245 (epoch 16/35), loss = 0.358198 (0.614 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:30:13.575004: step 51620/119245 (epoch 16/35), loss = 0.000759 (0.345 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:30:23.627420: step 51640/119245 (epoch 16/35), loss = 0.117077 (0.474 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:30:33.442621: step 51660/119245 (epoch 16/35), loss = 0.103114 (0.453 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:30:42.791330: step 51680/119245 (epoch 16/35), loss = 0.000077 (0.530 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:30:51.657440: step 51700/119245 (epoch 16/35), loss = 0.001448 (0.574 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:31:00.179105: step 51720/119245 (epoch 16/35), loss = 0.027733 (0.549 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:31:09.797303: step 51740/119245 (epoch 16/35), loss = 0.133912 (0.513 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:31:19.207296: step 51760/119245 (epoch 16/35), loss = 0.003029 (0.379 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:31:30.020697: step 51780/119245 (epoch 16/35), loss = 0.000295 (0.418 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:31:39.961749: step 51800/119245 (epoch 16/35), loss = 0.073223 (0.381 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:31:49.809424: step 51820/119245 (epoch 16/35), loss = 0.000253 (0.394 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:31:59.979258: step 51840/119245 (epoch 16/35), loss = 0.000062 (0.545 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:32:09.145094: step 51860/119245 (epoch 16/35), loss = 0.175705 (0.372 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:32:18.255693: step 51880/119245 (epoch 16/35), loss = 0.005006 (0.354 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:32:27.941217: step 51900/119245 (epoch 16/35), loss = 0.000540 (0.589 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:32:37.639575: step 51920/119245 (epoch 16/35), loss = 0.011203 (0.398 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:32:46.911492: step 51940/119245 (epoch 16/35), loss = 0.027107 (0.427 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:32:56.019037: step 51960/119245 (epoch 16/35), loss = 0.000311 (0.405 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:33:07.132040: step 51980/119245 (epoch 16/35), loss = 0.001042 (0.489 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:33:17.719677: step 52000/119245 (epoch 16/35), loss = 0.043548 (0.376 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:33:28.596536: step 52020/119245 (epoch 16/35), loss = 0.205610 (0.593 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:33:37.241055: step 52040/119245 (epoch 16/35), loss = 0.001688 (0.645 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:33:45.992993: step 52060/119245 (epoch 16/35), loss = 0.000063 (0.364 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:33:54.535581: step 52080/119245 (epoch 16/35), loss = 0.051480 (0.350 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:34:03.063251: step 52100/119245 (epoch 16/35), loss = 0.026933 (0.449 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:34:11.924280: step 52120/119245 (epoch 16/35), loss = 0.000058 (0.407 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:34:21.152567: step 52140/119245 (epoch 16/35), loss = 0.000171 (0.515 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:34:31.362059: step 52160/119245 (epoch 16/35), loss = 0.000199 (0.360 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:34:41.397461: step 52180/119245 (epoch 16/35), loss = 0.238789 (0.387 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:34:53.742455: step 52200/119245 (epoch 16/35), loss = 0.005885 (0.449 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:35:02.777678: step 52220/119245 (epoch 16/35), loss = 0.000646 (0.440 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:35:12.109849: step 52240/119245 (epoch 16/35), loss = 0.098349 (0.523 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:35:22.712955: step 52260/119245 (epoch 16/35), loss = 0.003493 (0.443 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:35:32.828615: step 52280/119245 (epoch 16/35), loss = 0.000436 (0.614 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:35:41.896136: step 52300/119245 (epoch 16/35), loss = 0.000149 (0.451 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:35:51.573449: step 52320/119245 (epoch 16/35), loss = 0.011822 (0.339 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:36:00.736321: step 52340/119245 (epoch 16/35), loss = 0.004132 (0.307 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:36:09.973566: step 52360/119245 (epoch 16/35), loss = 0.005459 (0.506 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:36:20.352128: step 52380/119245 (epoch 16/35), loss = 0.027228 (0.349 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:36:33.658926: step 52400/119245 (epoch 16/35), loss = 0.002765 (0.405 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:36:43.916656: step 52420/119245 (epoch 16/35), loss = 0.000266 (0.404 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:36:54.180189: step 52440/119245 (epoch 16/35), loss = 0.000125 (0.403 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:37:03.861492: step 52460/119245 (epoch 16/35), loss = 0.000451 (0.583 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:37:13.046251: step 52480/119245 (epoch 16/35), loss = 0.001418 (0.507 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:37:22.091131: step 52500/119245 (epoch 16/35), loss = 0.000687 (0.505 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:37:32.024538: step 52520/119245 (epoch 16/35), loss = 0.000293 (0.607 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:37:41.613537: step 52540/119245 (epoch 16/35), loss = 0.000174 (0.479 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:37:50.819048: step 52560/119245 (epoch 16/35), loss = 0.005226 (0.330 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:38:01.002144: step 52580/119245 (epoch 16/35), loss = 0.000214 (0.586 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:38:10.123220: step 52600/119245 (epoch 16/35), loss = 0.000240 (0.507 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:38:19.284371: step 52620/119245 (epoch 16/35), loss = 0.003645 (0.367 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:38:30.263933: step 52640/119245 (epoch 16/35), loss = 0.005727 (1.026 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:38:39.200307: step 52660/119245 (epoch 16/35), loss = 0.000048 (0.439 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:38:49.282901: step 52680/119245 (epoch 16/35), loss = 0.006807 (0.456 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:38:58.827826: step 52700/119245 (epoch 16/35), loss = 0.009972 (0.518 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:39:08.757533: step 52720/119245 (epoch 16/35), loss = 0.069356 (0.547 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:39:18.566210: step 52740/119245 (epoch 16/35), loss = 0.000423 (0.475 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:39:28.475240: step 52760/119245 (epoch 16/35), loss = 0.000102 (0.402 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:39:38.115356: step 52780/119245 (epoch 16/35), loss = 0.000385 (0.435 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:39:47.146192: step 52800/119245 (epoch 16/35), loss = 0.450141 (0.655 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:39:57.926105: step 52820/119245 (epoch 16/35), loss = 0.030141 (1.555 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:40:08.515008: step 52840/119245 (epoch 16/35), loss = 0.000129 (0.856 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:40:17.777977: step 52860/119245 (epoch 16/35), loss = 0.017703 (0.379 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:40:27.265801: step 52880/119245 (epoch 16/35), loss = 0.000242 (0.444 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:40:37.560865: step 52900/119245 (epoch 16/35), loss = 0.000400 (0.525 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:40:46.877377: step 52920/119245 (epoch 16/35), loss = 0.000255 (0.414 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:40:56.348680: step 52940/119245 (epoch 16/35), loss = 0.004235 (0.608 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:41:08.032216: step 52960/119245 (epoch 16/35), loss = 0.003158 (0.696 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:41:17.208132: step 52980/119245 (epoch 16/35), loss = 0.011071 (0.385 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:41:28.496852: step 53000/119245 (epoch 16/35), loss = 0.128280 (0.559 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:41:37.758023: step 53020/119245 (epoch 16/35), loss = 0.026522 (0.413 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:41:47.526808: step 53040/119245 (epoch 16/35), loss = 0.000979 (0.574 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:41:58.822933: step 53060/119245 (epoch 16/35), loss = 0.002815 (0.850 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:42:09.227955: step 53080/119245 (epoch 16/35), loss = 0.001908 (0.443 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:42:19.674630: step 53100/119245 (epoch 16/35), loss = 0.000142 (0.963 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:42:29.592725: step 53120/119245 (epoch 16/35), loss = 0.001012 (0.875 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:42:38.700080: step 53140/119245 (epoch 16/35), loss = 0.012164 (0.825 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:42:48.630065: step 53160/119245 (epoch 16/35), loss = 0.000331 (0.411 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:42:58.967421: step 53180/119245 (epoch 16/35), loss = 0.010947 (0.614 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:43:08.305471: step 53200/119245 (epoch 16/35), loss = 0.001415 (0.515 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:43:18.278097: step 53220/119245 (epoch 16/35), loss = 0.001510 (0.503 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:43:27.580782: step 53240/119245 (epoch 16/35), loss = 0.067501 (0.335 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:43:37.596898: step 53260/119245 (epoch 16/35), loss = 0.035640 (0.433 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:43:47.712365: step 53280/119245 (epoch 16/35), loss = 0.001181 (0.542 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:43:57.302348: step 53300/119245 (epoch 16/35), loss = 0.455886 (0.679 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:44:07.651696: step 53320/119245 (epoch 16/35), loss = 0.000245 (0.418 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:44:17.867150: step 53340/119245 (epoch 16/35), loss = 0.000297 (0.509 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:44:26.622757: step 53360/119245 (epoch 16/35), loss = 0.000167 (0.352 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:44:36.434215: step 53380/119245 (epoch 16/35), loss = 0.004081 (0.450 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:44:46.194150: step 53400/119245 (epoch 16/35), loss = 0.035896 (0.469 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:44:55.499362: step 53420/119245 (epoch 16/35), loss = 0.000181 (0.411 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:45:06.082771: step 53440/119245 (epoch 16/35), loss = 0.001930 (0.341 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:45:17.208694: step 53460/119245 (epoch 16/35), loss = 0.047546 (0.625 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:45:27.453490: step 53480/119245 (epoch 16/35), loss = 0.004254 (0.315 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:45:37.516678: step 53500/119245 (epoch 16/35), loss = 0.001700 (0.599 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:45:46.664531: step 53520/119245 (epoch 16/35), loss = 0.014077 (0.379 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:45:56.246050: step 53540/119245 (epoch 16/35), loss = 0.066550 (0.427 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:46:07.092704: step 53560/119245 (epoch 16/35), loss = 0.005382 (0.352 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:46:16.586628: step 53580/119245 (epoch 16/35), loss = 0.000160 (0.310 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:46:26.439117: step 53600/119245 (epoch 16/35), loss = 0.014521 (0.489 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:46:36.295974: step 53620/119245 (epoch 16/35), loss = 0.007836 (0.348 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:46:45.277116: step 53640/119245 (epoch 16/35), loss = 0.000331 (0.367 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:46:54.462420: step 53660/119245 (epoch 16/35), loss = 0.002981 (0.460 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:47:04.805077: step 53680/119245 (epoch 16/35), loss = 0.000141 (0.333 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:47:13.403585: step 53700/119245 (epoch 16/35), loss = 0.015921 (0.403 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:47:22.957338: step 53720/119245 (epoch 16/35), loss = 0.001128 (0.519 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:47:33.133441: step 53740/119245 (epoch 16/35), loss = 0.000190 (0.354 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:47:41.863825: step 53760/119245 (epoch 16/35), loss = 0.015220 (0.443 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:47:50.662086: step 53780/119245 (epoch 16/35), loss = 0.043422 (0.377 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:48:00.148570: step 53800/119245 (epoch 16/35), loss = 0.296142 (0.387 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:48:10.473160: step 53820/119245 (epoch 16/35), loss = 0.000083 (0.423 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:48:20.618178: step 53840/119245 (epoch 16/35), loss = 0.123400 (0.485 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:48:31.104271: step 53860/119245 (epoch 16/35), loss = 0.000142 (0.392 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:48:40.059093: step 53880/119245 (epoch 16/35), loss = 0.001401 (0.575 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:48:49.464381: step 53900/119245 (epoch 16/35), loss = 0.004083 (0.387 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:48:59.135584: step 53920/119245 (epoch 16/35), loss = 0.025259 (0.415 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:49:08.674893: step 53940/119245 (epoch 16/35), loss = 0.001026 (0.491 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:49:18.966763: step 53960/119245 (epoch 16/35), loss = 0.001370 (0.393 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:49:28.435816: step 53980/119245 (epoch 16/35), loss = 0.000248 (0.308 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:49:37.623268: step 54000/119245 (epoch 16/35), loss = 0.000333 (0.403 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:49:46.705453: step 54020/119245 (epoch 16/35), loss = 0.263672 (0.596 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:49:55.564167: step 54040/119245 (epoch 16/35), loss = 0.000118 (0.475 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:50:06.409875: step 54060/119245 (epoch 16/35), loss = 0.012849 (0.462 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:50:16.129007: step 54080/119245 (epoch 16/35), loss = 0.003605 (0.464 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:50:25.441957: step 54100/119245 (epoch 16/35), loss = 0.017366 (0.479 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:50:36.290342: step 54120/119245 (epoch 16/35), loss = 0.032497 (0.521 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:50:45.549966: step 54140/119245 (epoch 16/35), loss = 0.002117 (0.359 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:50:55.002223: step 54160/119245 (epoch 16/35), loss = 0.000148 (0.508 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:51:05.867005: step 54180/119245 (epoch 16/35), loss = 0.004695 (0.540 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:51:15.087576: step 54200/119245 (epoch 16/35), loss = 0.303735 (0.614 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:51:25.477492: step 54220/119245 (epoch 16/35), loss = 0.005044 (0.477 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:51:35.198163: step 54240/119245 (epoch 16/35), loss = 0.000544 (0.441 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:51:44.671048: step 54260/119245 (epoch 16/35), loss = 0.000027 (0.388 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:51:52.838136: step 54280/119245 (epoch 16/35), loss = 0.000621 (0.312 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:52:02.034525: step 54300/119245 (epoch 16/35), loss = 0.000426 (0.383 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:52:12.694861: step 54320/119245 (epoch 16/35), loss = 0.125107 (0.329 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:52:21.595505: step 54340/119245 (epoch 16/35), loss = 0.001865 (0.456 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:52:30.094199: step 54360/119245 (epoch 16/35), loss = 0.000862 (0.347 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:52:39.155661: step 54380/119245 (epoch 16/35), loss = 0.000186 (0.399 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:52:49.371737: step 54400/119245 (epoch 16/35), loss = 0.405184 (0.300 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:52:59.807037: step 54420/119245 (epoch 16/35), loss = 0.016917 (0.535 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:53:09.386276: step 54440/119245 (epoch 16/35), loss = 0.000122 (0.459 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:53:19.378702: step 54460/119245 (epoch 16/35), loss = 0.000048 (0.523 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:53:28.795801: step 54480/119245 (epoch 16/35), loss = 0.005635 (0.773 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:53:40.322231: step 54500/119245 (epoch 16/35), loss = 0.020137 (0.451 sec/batch), lr: 0.024300\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  78.44%  R:  74.26%  F1:  76.29%  #: 338\n",
            "org:city_of_headquarters             P:  65.88%  R:  51.38%  F1:  57.73%  #: 109\n",
            "org:country_of_headquarters          P:  51.58%  R:  64.41%  F1:  57.29%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  84.38%  R:  71.05%  F1:  77.14%  #: 38\n",
            "org:founded_by                       P:  73.02%  R:  60.53%  F1:  66.19%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  81.36%  R:  56.47%  F1:  66.67%  #: 85\n",
            "org:number_of_employees/members      P:  71.43%  R:  55.56%  F1:  62.50%  #: 27\n",
            "org:parents                          P:  32.54%  R:  42.71%  F1:  36.94%  #: 96\n",
            "org:political/religious_affiliation  P:  14.29%  R:  10.00%  F1:  11.76%  #: 10\n",
            "org:shareholders                     P:  33.33%  R:  16.36%  F1:  21.95%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  62.65%  R:  74.29%  F1:  67.97%  #: 70\n",
            "org:subsidiaries                     P:  41.79%  R:  24.78%  F1:  31.11%  #: 113\n",
            "org:top_members/employees            P:  76.72%  R:  77.15%  F1:  76.94%  #: 534\n",
            "org:website                          P:  89.36%  R:  97.67%  F1:  93.33%  #: 86\n",
            "per:age                              P:  83.58%  R:  92.18%  F1:  87.67%  #: 243\n",
            "per:alternate_names                  P:  52.00%  R:  34.21%  F1:  41.27%  #: 38\n",
            "per:cause_of_death                   P:  82.99%  R:  72.62%  F1:  77.46%  #: 168\n",
            "per:charges                          P:  71.55%  R:  79.05%  F1:  75.11%  #: 105\n",
            "per:children                         P:  68.48%  R:  63.64%  F1:  65.97%  #: 99\n",
            "per:cities_of_residence              P:  46.52%  R:  48.60%  F1:  47.54%  #: 179\n",
            "per:city_of_birth                    P:  71.05%  R:  81.82%  F1:  76.06%  #: 33\n",
            "per:city_of_death                    P:  59.17%  R:  60.17%  F1:  59.66%  #: 118\n",
            "per:countries_of_residence           P:  35.09%  R:  44.25%  F1:  39.14%  #: 226\n",
            "per:country_of_birth                 P:  42.86%  R:  30.00%  F1:  35.29%  #: 20\n",
            "per:country_of_death                 P:  80.00%  R:   8.70%  F1:  15.69%  #: 46\n",
            "per:date_of_birth                    P:  87.50%  R:  90.32%  F1:  88.89%  #: 31\n",
            "per:date_of_death                    P:  76.60%  R:  69.90%  F1:  73.10%  #: 206\n",
            "per:employee_of                      P:  60.71%  R:  58.93%  F1:  59.81%  #: 375\n",
            "per:origin                           P:  54.36%  R:  62.38%  F1:  58.09%  #: 210\n",
            "per:other_family                     P:  50.00%  R:   8.75%  F1:  14.89%  #: 80\n",
            "per:parents                          P:  58.18%  R:  57.14%  F1:  57.66%  #: 56\n",
            "per:religion                         P:  64.86%  R:  45.28%  F1:  53.33%  #: 53\n",
            "per:schools_attended                 P:  72.92%  R:  70.00%  F1:  71.43%  #: 50\n",
            "per:siblings                         P:  62.07%  R:  60.00%  F1:  61.02%  #: 30\n",
            "per:spouse                           P:  66.26%  R:  67.92%  F1:  67.08%  #: 159\n",
            "per:stateorprovince_of_birth         P:  68.18%  R:  57.69%  F1:  62.50%  #: 26\n",
            "per:stateorprovince_of_death         P:  60.00%  R:  43.90%  F1:  50.70%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  38.14%  R:  51.39%  F1:  43.79%  #: 72\n",
            "per:title                            P:  78.62%  R:  82.05%  F1:  80.30%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17302 Guess as no relation\n",
            "5329 guess\n",
            "3556 correct\n",
            "5436 gold\n",
            "Precision (micro): 66.729%\n",
            "   Recall (micro): 65.416%\n",
            "       F1 (micro): 66.066%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 16: train_loss = 0.031407, dev_loss = 1.056612, dev_f1 = 0.6607\n",
            "model saved to ./saved_models/00/checkpoint_epoch_16.pt\n",
            "\n",
            "2020-12-02 07:56:38.178503: step 54520/119245 (epoch 17/35), loss = 0.012408 (0.364 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:56:48.263581: step 54540/119245 (epoch 17/35), loss = 0.000171 (0.404 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:56:58.949700: step 54560/119245 (epoch 17/35), loss = 0.000282 (0.525 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:57:07.718799: step 54580/119245 (epoch 17/35), loss = 0.106531 (0.396 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:57:17.770788: step 54600/119245 (epoch 17/35), loss = 0.010338 (0.448 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:57:27.617926: step 54620/119245 (epoch 17/35), loss = 0.379696 (0.972 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:57:37.385816: step 54640/119245 (epoch 17/35), loss = 0.012258 (0.349 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:57:46.197060: step 54660/119245 (epoch 17/35), loss = 0.000051 (0.534 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:57:55.149431: step 54680/119245 (epoch 17/35), loss = 0.006659 (0.351 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:58:05.099466: step 54700/119245 (epoch 17/35), loss = 0.363173 (0.422 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:58:14.902935: step 54720/119245 (epoch 17/35), loss = 0.000425 (0.363 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:58:24.057988: step 54740/119245 (epoch 17/35), loss = 0.000536 (0.519 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:58:33.589759: step 54760/119245 (epoch 17/35), loss = 0.000078 (0.552 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:58:44.182959: step 54780/119245 (epoch 17/35), loss = 0.000566 (0.504 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:58:52.183010: step 54800/119245 (epoch 17/35), loss = 0.002499 (0.367 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:59:01.285187: step 54820/119245 (epoch 17/35), loss = 0.004464 (0.312 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:59:11.801358: step 54840/119245 (epoch 17/35), loss = 0.054892 (0.362 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:59:21.684145: step 54860/119245 (epoch 17/35), loss = 0.000158 (0.521 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:59:32.692253: step 54880/119245 (epoch 17/35), loss = 0.001957 (0.464 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:59:42.065201: step 54900/119245 (epoch 17/35), loss = 0.000170 (0.384 sec/batch), lr: 0.024300\n",
            "2020-12-02 07:59:52.601595: step 54920/119245 (epoch 17/35), loss = 0.000620 (0.355 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:00:01.605634: step 54940/119245 (epoch 17/35), loss = 0.000724 (0.481 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:00:13.335975: step 54960/119245 (epoch 17/35), loss = 0.000216 (0.345 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:00:23.452114: step 54980/119245 (epoch 17/35), loss = 0.000049 (0.413 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:00:33.289320: step 55000/119245 (epoch 17/35), loss = 0.000340 (0.515 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:00:44.137869: step 55020/119245 (epoch 17/35), loss = 0.003231 (0.549 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:00:53.346645: step 55040/119245 (epoch 17/35), loss = 0.000035 (0.383 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:01:04.199818: step 55060/119245 (epoch 17/35), loss = 0.000030 (0.412 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:01:13.154299: step 55080/119245 (epoch 17/35), loss = 0.041785 (0.445 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:01:22.178353: step 55100/119245 (epoch 17/35), loss = 0.013635 (0.376 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:01:31.028453: step 55120/119245 (epoch 17/35), loss = 0.021389 (0.428 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:01:40.315676: step 55140/119245 (epoch 17/35), loss = 0.000695 (0.389 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:01:49.365474: step 55160/119245 (epoch 17/35), loss = 0.000609 (0.547 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:01:59.708240: step 55180/119245 (epoch 17/35), loss = 0.000693 (0.371 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:02:10.881498: step 55200/119245 (epoch 17/35), loss = 0.000173 (0.424 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:02:20.648720: step 55220/119245 (epoch 17/35), loss = 0.000054 (1.539 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:02:29.865035: step 55240/119245 (epoch 17/35), loss = 0.033502 (0.445 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:02:39.351585: step 55260/119245 (epoch 17/35), loss = 0.000303 (0.320 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:02:49.096795: step 55280/119245 (epoch 17/35), loss = 0.000026 (0.532 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:02:58.490136: step 55300/119245 (epoch 17/35), loss = 0.000678 (0.395 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:03:08.310126: step 55320/119245 (epoch 17/35), loss = 0.317656 (0.349 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:03:17.354583: step 55340/119245 (epoch 17/35), loss = 0.135435 (0.361 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:03:26.853164: step 55360/119245 (epoch 17/35), loss = 0.006415 (0.444 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:03:37.062317: step 55380/119245 (epoch 17/35), loss = 0.000218 (0.386 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:03:48.559387: step 55400/119245 (epoch 17/35), loss = 0.000622 (1.643 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:03:58.604473: step 55420/119245 (epoch 17/35), loss = 0.000036 (0.422 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:04:07.994033: step 55440/119245 (epoch 17/35), loss = 0.000250 (0.379 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:04:16.873056: step 55460/119245 (epoch 17/35), loss = 0.000071 (0.593 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:04:25.580535: step 55480/119245 (epoch 17/35), loss = 0.007664 (0.607 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:04:33.984057: step 55500/119245 (epoch 17/35), loss = 0.000361 (0.512 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:04:42.507149: step 55520/119245 (epoch 17/35), loss = 0.052682 (0.355 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:04:51.331442: step 55540/119245 (epoch 17/35), loss = 0.005014 (0.406 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:05:01.787225: step 55560/119245 (epoch 17/35), loss = 0.000364 (0.501 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:05:11.026553: step 55580/119245 (epoch 17/35), loss = 0.001477 (0.635 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:05:22.769403: step 55600/119245 (epoch 17/35), loss = 0.001605 (1.332 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:05:33.339469: step 55620/119245 (epoch 17/35), loss = 0.000547 (0.387 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:05:42.119732: step 55640/119245 (epoch 17/35), loss = 0.001009 (0.515 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:05:52.676032: step 55660/119245 (epoch 17/35), loss = 0.003656 (0.633 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:06:02.698706: step 55680/119245 (epoch 17/35), loss = 0.212895 (0.319 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:06:11.752909: step 55700/119245 (epoch 17/35), loss = 0.000225 (0.358 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:06:21.936852: step 55720/119245 (epoch 17/35), loss = 0.000492 (0.361 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:06:31.365259: step 55740/119245 (epoch 17/35), loss = 0.006018 (0.584 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:06:40.214135: step 55760/119245 (epoch 17/35), loss = 0.017807 (0.420 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:06:50.242170: step 55780/119245 (epoch 17/35), loss = 0.000167 (0.333 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:07:02.457859: step 55800/119245 (epoch 17/35), loss = 0.000036 (1.018 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:07:13.743172: step 55820/119245 (epoch 17/35), loss = 0.000091 (0.417 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:07:24.004526: step 55840/119245 (epoch 17/35), loss = 0.017430 (0.552 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:07:33.407457: step 55860/119245 (epoch 17/35), loss = 0.034782 (0.375 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:07:43.209077: step 55880/119245 (epoch 17/35), loss = 0.005165 (0.363 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:07:52.157056: step 55900/119245 (epoch 17/35), loss = 0.003225 (0.351 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:08:01.661632: step 55920/119245 (epoch 17/35), loss = 0.000661 (0.417 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:08:11.586498: step 55940/119245 (epoch 17/35), loss = 0.019033 (0.621 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:08:20.952723: step 55960/119245 (epoch 17/35), loss = 0.000614 (0.536 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:08:31.042111: step 55980/119245 (epoch 17/35), loss = 0.012804 (0.355 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:08:40.210902: step 56000/119245 (epoch 17/35), loss = 0.196812 (0.389 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:08:49.470522: step 56020/119245 (epoch 17/35), loss = 0.093988 (0.485 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:08:59.387880: step 56040/119245 (epoch 17/35), loss = 0.002229 (0.642 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:09:09.281909: step 56060/119245 (epoch 17/35), loss = 0.002095 (0.508 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:09:19.044221: step 56080/119245 (epoch 17/35), loss = 0.095595 (0.395 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:09:28.973375: step 56100/119245 (epoch 17/35), loss = 0.000704 (0.556 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:09:38.746165: step 56120/119245 (epoch 17/35), loss = 0.000170 (0.315 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:09:47.916143: step 56140/119245 (epoch 17/35), loss = 0.003282 (0.429 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:09:58.504795: step 56160/119245 (epoch 17/35), loss = 0.000835 (0.364 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:10:07.616714: step 56180/119245 (epoch 17/35), loss = 0.012046 (0.355 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:10:17.332223: step 56200/119245 (epoch 17/35), loss = 0.010265 (0.319 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:10:27.055185: step 56220/119245 (epoch 17/35), loss = 0.000530 (0.825 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:10:37.838072: step 56240/119245 (epoch 17/35), loss = 0.000981 (0.591 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:10:47.979983: step 56260/119245 (epoch 17/35), loss = 0.068020 (0.509 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:10:57.613220: step 56280/119245 (epoch 17/35), loss = 0.000042 (0.616 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:11:07.698719: step 56300/119245 (epoch 17/35), loss = 0.008345 (0.475 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:11:16.784648: step 56320/119245 (epoch 17/35), loss = 0.001747 (0.339 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:11:26.130100: step 56340/119245 (epoch 17/35), loss = 0.000090 (0.529 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:11:35.542286: step 56360/119245 (epoch 17/35), loss = 0.000653 (0.361 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:11:47.409044: step 56380/119245 (epoch 17/35), loss = 0.000944 (0.490 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:11:58.715873: step 56400/119245 (epoch 17/35), loss = 0.000186 (0.337 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:12:07.539429: step 56420/119245 (epoch 17/35), loss = 0.007339 (0.354 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:12:17.595882: step 56440/119245 (epoch 17/35), loss = 0.000160 (0.742 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:12:27.605558: step 56460/119245 (epoch 17/35), loss = 0.000406 (0.538 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:12:39.223303: step 56480/119245 (epoch 17/35), loss = 0.000296 (0.761 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:12:48.868716: step 56500/119245 (epoch 17/35), loss = 0.006006 (0.485 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:12:58.925055: step 56520/119245 (epoch 17/35), loss = 0.002241 (0.341 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:13:08.557678: step 56540/119245 (epoch 17/35), loss = 0.000359 (0.458 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:13:18.445763: step 56560/119245 (epoch 17/35), loss = 0.000416 (0.412 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:13:28.414650: step 56580/119245 (epoch 17/35), loss = 0.000786 (0.426 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:13:37.659894: step 56600/119245 (epoch 17/35), loss = 0.003022 (0.383 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:13:47.395952: step 56620/119245 (epoch 17/35), loss = 0.088682 (0.323 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:13:57.248761: step 56640/119245 (epoch 17/35), loss = 0.000344 (0.387 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:14:06.455234: step 56660/119245 (epoch 17/35), loss = 0.004900 (0.500 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:14:16.528905: step 56680/119245 (epoch 17/35), loss = 0.094467 (0.463 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:14:26.886064: step 56700/119245 (epoch 17/35), loss = 0.008255 (0.502 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:14:36.737865: step 56720/119245 (epoch 17/35), loss = 0.000097 (0.345 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:14:47.764386: step 56740/119245 (epoch 17/35), loss = 0.001711 (0.602 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:14:56.311688: step 56760/119245 (epoch 17/35), loss = 0.004960 (0.316 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:15:06.165866: step 56780/119245 (epoch 17/35), loss = 0.000255 (0.462 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:15:15.264195: step 56800/119245 (epoch 17/35), loss = 0.001358 (0.381 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:15:25.209773: step 56820/119245 (epoch 17/35), loss = 0.002273 (0.394 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:15:35.686318: step 56840/119245 (epoch 17/35), loss = 0.002287 (0.970 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:15:46.102677: step 56860/119245 (epoch 17/35), loss = 0.000420 (1.506 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:15:56.494846: step 56880/119245 (epoch 17/35), loss = 0.305433 (0.452 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:16:06.646602: step 56900/119245 (epoch 17/35), loss = 0.006473 (0.444 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:16:15.941264: step 56920/119245 (epoch 17/35), loss = 0.000418 (0.362 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:16:25.694840: step 56940/119245 (epoch 17/35), loss = 0.000602 (1.003 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:16:36.352531: step 56960/119245 (epoch 17/35), loss = 0.000430 (0.525 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:16:45.897695: step 56980/119245 (epoch 17/35), loss = 0.000480 (0.378 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:16:55.574441: step 57000/119245 (epoch 17/35), loss = 0.002412 (0.483 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:17:06.184616: step 57020/119245 (epoch 17/35), loss = 0.007643 (0.318 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:17:15.060757: step 57040/119245 (epoch 17/35), loss = 0.002394 (0.427 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:17:24.318855: step 57060/119245 (epoch 17/35), loss = 0.114771 (0.466 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:17:33.937668: step 57080/119245 (epoch 17/35), loss = 0.000771 (0.591 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:17:43.366773: step 57100/119245 (epoch 17/35), loss = 0.093720 (0.416 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:17:52.118440: step 57120/119245 (epoch 17/35), loss = 0.002879 (0.400 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:18:02.989838: step 57140/119245 (epoch 17/35), loss = 0.000769 (0.533 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:18:11.531726: step 57160/119245 (epoch 17/35), loss = 0.045293 (0.391 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:18:20.577902: step 57180/119245 (epoch 17/35), loss = 0.000091 (0.313 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:18:29.946614: step 57200/119245 (epoch 17/35), loss = 0.000180 (0.472 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:18:40.404179: step 57220/119245 (epoch 17/35), loss = 0.100502 (0.356 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:18:50.036791: step 57240/119245 (epoch 17/35), loss = 0.000088 (0.355 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:19:00.704662: step 57260/119245 (epoch 17/35), loss = 0.002502 (0.811 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:19:09.522821: step 57280/119245 (epoch 17/35), loss = 0.002315 (0.522 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:19:18.821514: step 57300/119245 (epoch 17/35), loss = 0.000127 (0.496 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:19:28.353479: step 57320/119245 (epoch 17/35), loss = 0.020477 (0.403 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:19:38.090484: step 57340/119245 (epoch 17/35), loss = 0.000052 (0.531 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:19:48.260987: step 57360/119245 (epoch 17/35), loss = 0.008158 (0.758 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:19:57.161990: step 57380/119245 (epoch 17/35), loss = 0.001151 (0.353 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:20:06.552523: step 57400/119245 (epoch 17/35), loss = 0.000229 (0.718 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:20:15.718302: step 57420/119245 (epoch 17/35), loss = 0.000396 (0.311 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:20:24.878613: step 57440/119245 (epoch 17/35), loss = 0.000276 (0.456 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:20:34.139879: step 57460/119245 (epoch 17/35), loss = 0.000198 (0.619 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:20:45.427264: step 57480/119245 (epoch 17/35), loss = 0.000996 (0.410 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:20:54.253449: step 57500/119245 (epoch 17/35), loss = 0.000031 (0.586 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:21:05.087652: step 57520/119245 (epoch 17/35), loss = 0.000063 (0.491 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:21:14.599596: step 57540/119245 (epoch 17/35), loss = 0.000829 (0.444 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:21:24.221961: step 57560/119245 (epoch 17/35), loss = 0.176782 (0.595 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:21:34.946499: step 57580/119245 (epoch 17/35), loss = 0.000015 (1.534 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:21:43.881416: step 57600/119245 (epoch 17/35), loss = 0.002460 (0.378 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:21:53.988717: step 57620/119245 (epoch 17/35), loss = 0.017526 (0.547 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:22:03.696038: step 57640/119245 (epoch 17/35), loss = 0.086978 (0.528 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:22:13.085173: step 57660/119245 (epoch 17/35), loss = 0.000730 (0.544 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:22:22.191372: step 57680/119245 (epoch 17/35), loss = 0.000678 (0.445 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:22:30.951247: step 57700/119245 (epoch 17/35), loss = 0.110317 (0.602 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:22:41.483141: step 57720/119245 (epoch 17/35), loss = 0.131712 (0.490 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:22:50.467829: step 57740/119245 (epoch 17/35), loss = 0.122456 (0.375 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:22:59.272703: step 57760/119245 (epoch 17/35), loss = 0.000290 (0.341 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:23:07.988187: step 57780/119245 (epoch 17/35), loss = 0.000051 (0.432 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:23:18.649404: step 57800/119245 (epoch 17/35), loss = 0.000345 (0.387 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:23:27.425683: step 57820/119245 (epoch 17/35), loss = 0.047353 (0.398 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:23:38.239821: step 57840/119245 (epoch 17/35), loss = 0.004653 (0.508 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:23:47.850555: step 57860/119245 (epoch 17/35), loss = 0.000057 (0.393 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:23:57.511586: step 57880/119245 (epoch 17/35), loss = 0.000037 (0.345 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:24:08.069852: step 57900/119245 (epoch 17/35), loss = 0.209538 (1.005 sec/batch), lr: 0.024300\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  79.17%  R:  73.08%  F1:  76.00%  #: 338\n",
            "org:city_of_headquarters             P:  63.64%  R:  51.38%  F1:  56.85%  #: 109\n",
            "org:country_of_headquarters          P:  58.70%  R:  45.76%  F1:  51.43%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  84.38%  R:  71.05%  F1:  77.14%  #: 38\n",
            "org:founded_by                       P:  54.55%  R:  55.26%  F1:  54.90%  #: 76\n",
            "org:member_of                        P:  11.11%  R:   3.23%  F1:   5.00%  #: 31\n",
            "org:members                          P:  70.59%  R:  56.47%  F1:  62.75%  #: 85\n",
            "org:number_of_employees/members      P:  76.19%  R:  59.26%  F1:  66.67%  #: 27\n",
            "org:parents                          P:  35.35%  R:  36.46%  F1:  35.90%  #: 96\n",
            "org:political/religious_affiliation  P:  30.00%  R:  30.00%  F1:  30.00%  #: 10\n",
            "org:shareholders                     P:  33.33%  R:  23.64%  F1:  27.66%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  62.35%  R:  75.71%  F1:  68.39%  #: 70\n",
            "org:subsidiaries                     P:  52.38%  R:  19.47%  F1:  28.39%  #: 113\n",
            "org:top_members/employees            P:  71.65%  R:  77.15%  F1:  74.30%  #: 534\n",
            "org:website                          P:  85.86%  R:  98.84%  F1:  91.89%  #: 86\n",
            "per:age                              P:  86.75%  R:  88.89%  F1:  87.80%  #: 243\n",
            "per:alternate_names                  P:  63.16%  R:  31.58%  F1:  42.11%  #: 38\n",
            "per:cause_of_death                   P:  82.58%  R:  76.19%  F1:  79.26%  #: 168\n",
            "per:charges                          P:  70.25%  R:  80.95%  F1:  75.22%  #: 105\n",
            "per:children                         P:  69.39%  R:  68.69%  F1:  69.04%  #: 99\n",
            "per:cities_of_residence              P:  48.65%  R:  50.28%  F1:  49.45%  #: 179\n",
            "per:city_of_birth                    P:  74.29%  R:  78.79%  F1:  76.47%  #: 33\n",
            "per:city_of_death                    P:  72.82%  R:  63.56%  F1:  67.87%  #: 118\n",
            "per:countries_of_residence           P:  35.56%  R:  42.48%  F1:  38.71%  #: 226\n",
            "per:country_of_birth                 P:  40.00%  R:  30.00%  F1:  34.29%  #: 20\n",
            "per:country_of_death                 P:  60.00%  R:   6.52%  F1:  11.76%  #: 46\n",
            "per:date_of_birth                    P:  90.32%  R:  90.32%  F1:  90.32%  #: 31\n",
            "per:date_of_death                    P:  79.78%  R:  70.87%  F1:  75.06%  #: 206\n",
            "per:employee_of                      P:  59.48%  R:  61.07%  F1:  60.26%  #: 375\n",
            "per:origin                           P:  59.18%  R:  55.24%  F1:  57.14%  #: 210\n",
            "per:other_family                     P:  50.00%  R:  16.25%  F1:  24.53%  #: 80\n",
            "per:parents                          P:  70.27%  R:  46.43%  F1:  55.91%  #: 56\n",
            "per:religion                         P:  55.77%  R:  54.72%  F1:  55.24%  #: 53\n",
            "per:schools_attended                 P:  74.51%  R:  76.00%  F1:  75.25%  #: 50\n",
            "per:siblings                         P:  72.00%  R:  60.00%  F1:  65.45%  #: 30\n",
            "per:spouse                           P:  69.03%  R:  67.30%  F1:  68.15%  #: 159\n",
            "per:stateorprovince_of_birth         P:  71.43%  R:  57.69%  F1:  63.83%  #: 26\n",
            "per:stateorprovince_of_death         P:  62.50%  R:  60.98%  F1:  61.73%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  42.42%  R:  58.33%  F1:  49.12%  #: 72\n",
            "per:title                            P:  76.56%  R:  84.22%  F1:  80.21%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17363 Guess as no relation\n",
            "5268 guess\n",
            "3552 correct\n",
            "5436 gold\n",
            "Precision (micro): 67.426%\n",
            "   Recall (micro): 65.342%\n",
            "       F1 (micro): 66.368%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 17: train_loss = 0.023768, dev_loss = 1.091493, dev_f1 = 0.6637\n",
            "model saved to ./saved_models/00/checkpoint_epoch_17.pt\n",
            "\n",
            "2020-12-02 08:27:06.348150: step 57920/119245 (epoch 18/35), loss = 0.001044 (0.391 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:27:15.813344: step 57940/119245 (epoch 18/35), loss = 0.065257 (0.410 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:27:27.038624: step 57960/119245 (epoch 18/35), loss = 0.019002 (0.801 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:27:35.705361: step 57980/119245 (epoch 18/35), loss = 0.002841 (0.552 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:27:46.279411: step 58000/119245 (epoch 18/35), loss = 0.000179 (0.513 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:27:55.431893: step 58020/119245 (epoch 18/35), loss = 0.000116 (0.716 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:28:05.236218: step 58040/119245 (epoch 18/35), loss = 0.004159 (0.330 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:28:14.693483: step 58060/119245 (epoch 18/35), loss = 0.000085 (0.318 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:28:23.000770: step 58080/119245 (epoch 18/35), loss = 0.000535 (0.346 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:28:32.750663: step 58100/119245 (epoch 18/35), loss = 0.000314 (0.329 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:28:42.553627: step 58120/119245 (epoch 18/35), loss = 0.000088 (0.527 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:28:52.031994: step 58140/119245 (epoch 18/35), loss = 0.000708 (0.614 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:29:00.517137: step 58160/119245 (epoch 18/35), loss = 0.147293 (0.376 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:29:10.455331: step 58180/119245 (epoch 18/35), loss = 0.141237 (0.411 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:29:20.502607: step 58200/119245 (epoch 18/35), loss = 0.004498 (0.278 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:29:29.069646: step 58220/119245 (epoch 18/35), loss = 0.011158 (0.343 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:29:39.236082: step 58240/119245 (epoch 18/35), loss = 0.005062 (0.405 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:29:49.219786: step 58260/119245 (epoch 18/35), loss = 0.000231 (0.381 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:30:00.910373: step 58280/119245 (epoch 18/35), loss = 0.000058 (0.608 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:30:09.612625: step 58300/119245 (epoch 18/35), loss = 0.007874 (0.351 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:30:19.812281: step 58320/119245 (epoch 18/35), loss = 0.001326 (0.465 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:30:29.773038: step 58340/119245 (epoch 18/35), loss = 0.002152 (0.609 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:30:39.045940: step 58360/119245 (epoch 18/35), loss = 0.000031 (0.635 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:30:51.188099: step 58380/119245 (epoch 18/35), loss = 0.003815 (0.412 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:31:00.378229: step 58400/119245 (epoch 18/35), loss = 0.077508 (0.399 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:31:11.459027: step 58420/119245 (epoch 18/35), loss = 0.000049 (0.347 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:31:20.685695: step 58440/119245 (epoch 18/35), loss = 0.000113 (0.332 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:31:32.051025: step 58460/119245 (epoch 18/35), loss = 0.000480 (0.817 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:31:40.656509: step 58480/119245 (epoch 18/35), loss = 0.000107 (0.891 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:31:49.927632: step 58500/119245 (epoch 18/35), loss = 0.000216 (0.362 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:31:58.556444: step 58520/119245 (epoch 18/35), loss = 0.000034 (0.402 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:32:07.757647: step 58540/119245 (epoch 18/35), loss = 0.001196 (0.376 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:32:16.667294: step 58560/119245 (epoch 18/35), loss = 0.000254 (0.380 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:32:26.971793: step 58580/119245 (epoch 18/35), loss = 0.000143 (0.994 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:32:37.180154: step 58600/119245 (epoch 18/35), loss = 0.011171 (0.420 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:32:46.891438: step 58620/119245 (epoch 18/35), loss = 0.359135 (0.448 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:32:56.749402: step 58640/119245 (epoch 18/35), loss = 0.000025 (0.417 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:33:06.932774: step 58660/119245 (epoch 18/35), loss = 0.031226 (0.327 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:33:16.207958: step 58680/119245 (epoch 18/35), loss = 0.001035 (0.387 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:33:25.950677: step 58700/119245 (epoch 18/35), loss = 0.001975 (0.347 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:33:35.532811: step 58720/119245 (epoch 18/35), loss = 0.000043 (0.405 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:33:44.765467: step 58740/119245 (epoch 18/35), loss = 0.036695 (0.585 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:33:53.756232: step 58760/119245 (epoch 18/35), loss = 0.000355 (0.400 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:34:02.960623: step 58780/119245 (epoch 18/35), loss = 0.000083 (0.404 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:34:14.446800: step 58800/119245 (epoch 18/35), loss = 0.000106 (0.529 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:34:25.476201: step 58820/119245 (epoch 18/35), loss = 0.002321 (0.340 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:34:35.162650: step 58840/119245 (epoch 18/35), loss = 0.003758 (0.393 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:34:44.107437: step 58860/119245 (epoch 18/35), loss = 0.287158 (0.436 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:34:52.865982: step 58880/119245 (epoch 18/35), loss = 0.000745 (0.363 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:35:01.318231: step 58900/119245 (epoch 18/35), loss = 0.002833 (0.418 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:35:10.237889: step 58920/119245 (epoch 18/35), loss = 0.053391 (0.358 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:35:18.928353: step 58940/119245 (epoch 18/35), loss = 0.000037 (0.428 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:35:28.350197: step 58960/119245 (epoch 18/35), loss = 0.000059 (0.607 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:35:37.869751: step 58980/119245 (epoch 18/35), loss = 0.000131 (0.307 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:35:48.785284: step 59000/119245 (epoch 18/35), loss = 0.000138 (0.748 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:36:00.732852: step 59020/119245 (epoch 18/35), loss = 0.000106 (0.338 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:36:09.677033: step 59040/119245 (epoch 18/35), loss = 0.000120 (0.469 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:36:19.210313: step 59060/119245 (epoch 18/35), loss = 0.000778 (0.455 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:36:30.316164: step 59080/119245 (epoch 18/35), loss = 0.000539 (0.757 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:36:39.466023: step 59100/119245 (epoch 18/35), loss = 0.001836 (0.378 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:36:49.381599: step 59120/119245 (epoch 18/35), loss = 0.014999 (0.447 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:36:58.240133: step 59140/119245 (epoch 18/35), loss = 0.000060 (0.405 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:37:07.273551: step 59160/119245 (epoch 18/35), loss = 0.001109 (0.349 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:37:17.043380: step 59180/119245 (epoch 18/35), loss = 0.005815 (0.744 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:37:27.792909: step 59200/119245 (epoch 18/35), loss = 0.000201 (0.511 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:37:40.902570: step 59220/119245 (epoch 18/35), loss = 0.006285 (0.314 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:37:50.806411: step 59240/119245 (epoch 18/35), loss = 0.000181 (0.481 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:38:00.837640: step 59260/119245 (epoch 18/35), loss = 0.002018 (0.491 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:38:10.004169: step 59280/119245 (epoch 18/35), loss = 0.000521 (0.576 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:38:19.521411: step 59300/119245 (epoch 18/35), loss = 0.000734 (0.335 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:38:28.274433: step 59320/119245 (epoch 18/35), loss = 0.154282 (0.315 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:38:38.272312: step 59340/119245 (epoch 18/35), loss = 0.000132 (0.549 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:38:47.917388: step 59360/119245 (epoch 18/35), loss = 0.000087 (0.394 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:38:57.978071: step 59380/119245 (epoch 18/35), loss = 0.007461 (0.399 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:39:07.436859: step 59400/119245 (epoch 18/35), loss = 0.329188 (0.455 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:39:16.072788: step 59420/119245 (epoch 18/35), loss = 0.000702 (0.410 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:39:26.017589: step 59440/119245 (epoch 18/35), loss = 0.000120 (0.958 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:39:36.266128: step 59460/119245 (epoch 18/35), loss = 0.000052 (0.333 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:39:45.595253: step 59480/119245 (epoch 18/35), loss = 0.000095 (0.528 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:39:55.462093: step 59500/119245 (epoch 18/35), loss = 0.000079 (0.433 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:40:04.574663: step 59520/119245 (epoch 18/35), loss = 0.000504 (0.382 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:40:14.471228: step 59540/119245 (epoch 18/35), loss = 0.000487 (0.333 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:40:24.256827: step 59560/119245 (epoch 18/35), loss = 0.000018 (0.402 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:40:33.981850: step 59580/119245 (epoch 18/35), loss = 0.000093 (0.375 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:40:43.992057: step 59600/119245 (epoch 18/35), loss = 0.002580 (0.612 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:40:53.494398: step 59620/119245 (epoch 18/35), loss = 0.016462 (1.148 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:41:04.350179: step 59640/119245 (epoch 18/35), loss = 0.000519 (0.546 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:41:14.312617: step 59660/119245 (epoch 18/35), loss = 0.141454 (0.536 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:41:24.038878: step 59680/119245 (epoch 18/35), loss = 0.000113 (0.436 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:41:32.928954: step 59700/119245 (epoch 18/35), loss = 0.065851 (0.440 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:41:43.249692: step 59720/119245 (epoch 18/35), loss = 0.000045 (0.293 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:41:52.607604: step 59740/119245 (epoch 18/35), loss = 0.000352 (0.425 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:42:01.811028: step 59760/119245 (epoch 18/35), loss = 0.012755 (0.453 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:42:13.526287: step 59780/119245 (epoch 18/35), loss = 0.003644 (0.463 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:42:22.725197: step 59800/119245 (epoch 18/35), loss = 0.000609 (0.337 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:42:33.863182: step 59820/119245 (epoch 18/35), loss = 0.004687 (0.356 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:42:42.626601: step 59840/119245 (epoch 18/35), loss = 0.000129 (0.365 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:42:53.671004: step 59860/119245 (epoch 18/35), loss = 0.000382 (0.483 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:43:04.049891: step 59880/119245 (epoch 18/35), loss = 0.010494 (0.353 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:43:15.234820: step 59900/119245 (epoch 18/35), loss = 0.000128 (0.676 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:43:24.943238: step 59920/119245 (epoch 18/35), loss = 0.001152 (0.438 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:43:34.826499: step 59940/119245 (epoch 18/35), loss = 0.001085 (0.355 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:43:45.382393: step 59960/119245 (epoch 18/35), loss = 0.000126 (0.338 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:43:53.768096: step 59980/119245 (epoch 18/35), loss = 0.000697 (0.435 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:44:04.220136: step 60000/119245 (epoch 18/35), loss = 0.127605 (0.557 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:44:13.464289: step 60020/119245 (epoch 18/35), loss = 0.008640 (0.460 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:44:23.735815: step 60040/119245 (epoch 18/35), loss = 0.000042 (0.317 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:44:32.858288: step 60060/119245 (epoch 18/35), loss = 0.000102 (0.516 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:44:43.154431: step 60080/119245 (epoch 18/35), loss = 0.000383 (0.315 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:44:52.626986: step 60100/119245 (epoch 18/35), loss = 0.000040 (0.399 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:45:03.246992: step 60120/119245 (epoch 18/35), loss = 0.001068 (0.389 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:45:13.322491: step 60140/119245 (epoch 18/35), loss = 0.003159 (0.379 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:45:22.813631: step 60160/119245 (epoch 18/35), loss = 0.000150 (0.471 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:45:32.463242: step 60180/119245 (epoch 18/35), loss = 0.166879 (0.434 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:45:41.400818: step 60200/119245 (epoch 18/35), loss = 0.000185 (0.380 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:45:51.137227: step 60220/119245 (epoch 18/35), loss = 0.001612 (0.434 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:46:00.644032: step 60240/119245 (epoch 18/35), loss = 0.030221 (0.601 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:46:11.101875: step 60260/119245 (epoch 18/35), loss = 0.000698 (0.440 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:46:22.519031: step 60280/119245 (epoch 18/35), loss = 0.007621 (0.416 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:46:33.034785: step 60300/119245 (epoch 18/35), loss = 0.000641 (0.578 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:46:42.265350: step 60320/119245 (epoch 18/35), loss = 0.001969 (0.453 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:46:51.240185: step 60340/119245 (epoch 18/35), loss = 0.002690 (0.322 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:47:01.918044: step 60360/119245 (epoch 18/35), loss = 0.005937 (0.351 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:47:11.681246: step 60380/119245 (epoch 18/35), loss = 0.092902 (0.387 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:47:21.856831: step 60400/119245 (epoch 18/35), loss = 0.000913 (0.353 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:47:31.839327: step 60420/119245 (epoch 18/35), loss = 0.000909 (0.531 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:47:41.264797: step 60440/119245 (epoch 18/35), loss = 0.000112 (0.696 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:47:50.062222: step 60460/119245 (epoch 18/35), loss = 0.000067 (0.436 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:47:59.791459: step 60480/119245 (epoch 18/35), loss = 0.000269 (0.451 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:48:09.455944: step 60500/119245 (epoch 18/35), loss = 0.002573 (0.317 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:48:18.048457: step 60520/119245 (epoch 18/35), loss = 0.000219 (0.489 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:48:28.567348: step 60540/119245 (epoch 18/35), loss = 0.000959 (0.759 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:48:37.971577: step 60560/119245 (epoch 18/35), loss = 0.000384 (0.493 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:48:46.749969: step 60580/119245 (epoch 18/35), loss = 0.209039 (0.394 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:48:55.279010: step 60600/119245 (epoch 18/35), loss = 0.000545 (0.325 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:49:04.799612: step 60620/119245 (epoch 18/35), loss = 0.000102 (0.421 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:49:16.326044: step 60640/119245 (epoch 18/35), loss = 0.000065 (0.307 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:49:25.532614: step 60660/119245 (epoch 18/35), loss = 0.000142 (0.285 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:49:35.564690: step 60680/119245 (epoch 18/35), loss = 0.010204 (0.573 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:49:44.517805: step 60700/119245 (epoch 18/35), loss = 0.008374 (0.373 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:49:54.143983: step 60720/119245 (epoch 18/35), loss = 0.000375 (0.327 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:50:03.875272: step 60740/119245 (epoch 18/35), loss = 0.001311 (0.507 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:50:13.446865: step 60760/119245 (epoch 18/35), loss = 0.000067 (0.635 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:50:23.128258: step 60780/119245 (epoch 18/35), loss = 0.000156 (0.366 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:50:32.555075: step 60800/119245 (epoch 18/35), loss = 0.000020 (0.500 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:50:41.824483: step 60820/119245 (epoch 18/35), loss = 0.004979 (0.537 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:50:50.811987: step 60840/119245 (epoch 18/35), loss = 0.000047 (0.365 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:50:59.576446: step 60860/119245 (epoch 18/35), loss = 0.002339 (0.550 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:51:11.572204: step 60880/119245 (epoch 18/35), loss = 0.106519 (0.893 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:51:20.309438: step 60900/119245 (epoch 18/35), loss = 0.000677 (0.452 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:51:29.586060: step 60920/119245 (epoch 18/35), loss = 0.000297 (0.413 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:51:40.140499: step 60940/119245 (epoch 18/35), loss = 0.005548 (0.416 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:51:49.466634: step 60960/119245 (epoch 18/35), loss = 0.000066 (0.365 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:51:59.430210: step 60980/119245 (epoch 18/35), loss = 0.000336 (0.989 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:52:09.673644: step 61000/119245 (epoch 18/35), loss = 0.003581 (0.438 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:52:19.533967: step 61020/119245 (epoch 18/35), loss = 0.001442 (0.475 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:52:29.473831: step 61040/119245 (epoch 18/35), loss = 0.000148 (0.390 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:52:38.796247: step 61060/119245 (epoch 18/35), loss = 0.000154 (0.407 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:52:48.273996: step 61080/119245 (epoch 18/35), loss = 0.000137 (0.368 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:52:56.443166: step 61100/119245 (epoch 18/35), loss = 0.000429 (0.568 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:53:07.108521: step 61120/119245 (epoch 18/35), loss = 0.129831 (0.415 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:53:16.546947: step 61140/119245 (epoch 18/35), loss = 0.000330 (0.444 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:53:25.356046: step 61160/119245 (epoch 18/35), loss = 0.000009 (0.476 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:53:33.758775: step 61180/119245 (epoch 18/35), loss = 0.003530 (0.526 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:53:43.896290: step 61200/119245 (epoch 18/35), loss = 0.001518 (0.710 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:53:52.996526: step 61220/119245 (epoch 18/35), loss = 0.000091 (0.446 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:54:03.797856: step 61240/119245 (epoch 18/35), loss = 0.021488 (0.431 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:54:13.511473: step 61260/119245 (epoch 18/35), loss = 0.007088 (0.360 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:54:22.787125: step 61280/119245 (epoch 18/35), loss = 0.000152 (0.406 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:54:32.746266: step 61300/119245 (epoch 18/35), loss = 0.000002 (0.555 sec/batch), lr: 0.024300\n",
            "2020-12-02 08:54:43.671474: step 61320/119245 (epoch 18/35), loss = 0.000133 (0.335 sec/batch), lr: 0.024300\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  80.00%  R:  65.09%  F1:  71.78%  #: 338\n",
            "org:city_of_headquarters             P:  66.28%  R:  52.29%  F1:  58.46%  #: 109\n",
            "org:country_of_headquarters          P:  58.38%  R:  64.97%  F1:  61.50%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  84.38%  R:  71.05%  F1:  77.14%  #: 38\n",
            "org:founded_by                       P:  66.13%  R:  53.95%  F1:  59.42%  #: 76\n",
            "org:member_of                        P:  33.33%  R:   3.23%  F1:   5.88%  #: 31\n",
            "org:members                          P:  73.77%  R:  52.94%  F1:  61.64%  #: 85\n",
            "org:number_of_employees/members      P:  56.67%  R:  62.96%  F1:  59.65%  #: 27\n",
            "org:parents                          P:  32.80%  R:  42.71%  F1:  37.10%  #: 96\n",
            "org:political/religious_affiliation  P:  33.33%  R:  20.00%  F1:  25.00%  #: 10\n",
            "org:shareholders                     P:  36.21%  R:  38.18%  F1:  37.17%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  67.50%  R:  77.14%  F1:  72.00%  #: 70\n",
            "org:subsidiaries                     P:  41.76%  R:  33.63%  F1:  37.25%  #: 113\n",
            "org:top_members/employees            P:  77.67%  R:  73.60%  F1:  75.58%  #: 534\n",
            "org:website                          P:  91.11%  R:  95.35%  F1:  93.18%  #: 86\n",
            "per:age                              P:  87.45%  R:  88.89%  F1:  88.16%  #: 243\n",
            "per:alternate_names                  P:  57.89%  R:  28.95%  F1:  38.60%  #: 38\n",
            "per:cause_of_death                   P:  84.44%  R:  67.86%  F1:  75.25%  #: 168\n",
            "per:charges                          P:  73.79%  R:  72.38%  F1:  73.08%  #: 105\n",
            "per:children                         P:  74.70%  R:  62.63%  F1:  68.13%  #: 99\n",
            "per:cities_of_residence              P:  42.54%  R:  43.02%  F1:  42.78%  #: 179\n",
            "per:city_of_birth                    P:  73.53%  R:  75.76%  F1:  74.63%  #: 33\n",
            "per:city_of_death                    P:  70.41%  R:  58.47%  F1:  63.89%  #: 118\n",
            "per:countries_of_residence           P:  33.70%  R:  54.87%  F1:  41.75%  #: 226\n",
            "per:country_of_birth                 P:  55.56%  R:  25.00%  F1:  34.48%  #: 20\n",
            "per:country_of_death                 P: 100.00%  R:   2.17%  F1:   4.26%  #: 46\n",
            "per:date_of_birth                    P:  87.50%  R:  90.32%  F1:  88.89%  #: 31\n",
            "per:date_of_death                    P:  79.89%  R:  69.42%  F1:  74.29%  #: 206\n",
            "per:employee_of                      P:  53.12%  R:  70.40%  F1:  60.55%  #: 375\n",
            "per:origin                           P:  64.02%  R:  50.00%  F1:  56.15%  #: 210\n",
            "per:other_family                     P:  60.00%  R:  11.25%  F1:  18.95%  #: 80\n",
            "per:parents                          P:  72.50%  R:  51.79%  F1:  60.42%  #: 56\n",
            "per:religion                         P:  64.44%  R:  54.72%  F1:  59.18%  #: 53\n",
            "per:schools_attended                 P:  77.08%  R:  74.00%  F1:  75.51%  #: 50\n",
            "per:siblings                         P:  81.82%  R:  60.00%  F1:  69.23%  #: 30\n",
            "per:spouse                           P:  72.86%  R:  64.15%  F1:  68.23%  #: 159\n",
            "per:stateorprovince_of_birth         P:  80.00%  R:  46.15%  F1:  58.54%  #: 26\n",
            "per:stateorprovince_of_death         P:  75.76%  R:  60.98%  F1:  67.57%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  45.78%  R:  52.78%  F1:  49.03%  #: 72\n",
            "per:title                            P:  75.07%  R:  86.83%  F1:  80.52%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17262 Guess as no relation\n",
            "5369 guess\n",
            "3571 correct\n",
            "5436 gold\n",
            "Precision (micro): 66.511%\n",
            "   Recall (micro): 65.692%\n",
            "       F1 (micro): 66.099%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 18: train_loss = 0.021891, dev_loss = 1.170749, dev_f1 = 0.6610\n",
            "model saved to ./saved_models/00/checkpoint_epoch_18.pt\n",
            "\n",
            "2020-12-02 08:57:40.709865: step 61340/119245 (epoch 19/35), loss = 0.134940 (0.513 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:57:51.183440: step 61360/119245 (epoch 19/35), loss = 0.004008 (0.346 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:58:00.736407: step 61380/119245 (epoch 19/35), loss = 0.000080 (0.315 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:58:10.090773: step 61400/119245 (epoch 19/35), loss = 0.013658 (0.654 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:58:19.814514: step 61420/119245 (epoch 19/35), loss = 0.001983 (0.448 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:58:29.422711: step 61440/119245 (epoch 19/35), loss = 0.000699 (0.443 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:58:39.282558: step 61460/119245 (epoch 19/35), loss = 0.000102 (0.337 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:58:48.190365: step 61480/119245 (epoch 19/35), loss = 0.001345 (0.484 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:58:56.940393: step 61500/119245 (epoch 19/35), loss = 0.001111 (0.462 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:59:07.184198: step 61520/119245 (epoch 19/35), loss = 0.000016 (0.518 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:59:16.642778: step 61540/119245 (epoch 19/35), loss = 0.000226 (0.385 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:59:25.620064: step 61560/119245 (epoch 19/35), loss = 0.000069 (0.362 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:59:35.321822: step 61580/119245 (epoch 19/35), loss = 0.008521 (0.524 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:59:45.498249: step 61600/119245 (epoch 19/35), loss = 0.000182 (0.336 sec/batch), lr: 0.021870\n",
            "2020-12-02 08:59:53.649670: step 61620/119245 (epoch 19/35), loss = 0.000417 (0.452 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:00:03.325942: step 61640/119245 (epoch 19/35), loss = 0.000044 (0.780 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:00:13.856927: step 61660/119245 (epoch 19/35), loss = 0.000020 (0.530 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:00:23.745821: step 61680/119245 (epoch 19/35), loss = 0.002979 (0.523 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:00:34.240622: step 61700/119245 (epoch 19/35), loss = 0.002082 (0.323 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:00:43.216652: step 61720/119245 (epoch 19/35), loss = 0.074272 (0.367 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:00:53.954485: step 61740/119245 (epoch 19/35), loss = 0.000462 (0.326 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:01:03.207756: step 61760/119245 (epoch 19/35), loss = 0.000456 (0.525 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:01:15.727901: step 61780/119245 (epoch 19/35), loss = 0.000016 (0.436 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:01:24.878356: step 61800/119245 (epoch 19/35), loss = 0.000733 (0.440 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:01:34.808012: step 61820/119245 (epoch 19/35), loss = 0.000354 (0.341 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:01:45.261216: step 61840/119245 (epoch 19/35), loss = 0.048632 (0.342 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:01:55.166487: step 61860/119245 (epoch 19/35), loss = 0.000033 (1.174 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:02:04.957943: step 61880/119245 (epoch 19/35), loss = 0.009687 (0.396 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:02:14.216789: step 61900/119245 (epoch 19/35), loss = 0.001256 (0.429 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:02:23.040506: step 61920/119245 (epoch 19/35), loss = 0.000161 (0.441 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:02:31.631953: step 61940/119245 (epoch 19/35), loss = 0.059826 (0.400 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:02:41.290222: step 61960/119245 (epoch 19/35), loss = 0.000206 (0.673 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:02:50.835355: step 61980/119245 (epoch 19/35), loss = 0.000547 (0.402 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:03:01.567571: step 62000/119245 (epoch 19/35), loss = 0.000082 (0.309 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:03:11.559903: step 62020/119245 (epoch 19/35), loss = 0.000086 (0.383 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:03:21.383694: step 62040/119245 (epoch 19/35), loss = 0.003271 (0.523 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:03:31.374905: step 62060/119245 (epoch 19/35), loss = 0.001312 (0.472 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:03:40.697527: step 62080/119245 (epoch 19/35), loss = 0.000050 (0.521 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:03:49.810663: step 62100/119245 (epoch 19/35), loss = 0.006171 (0.585 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:03:59.229427: step 62120/119245 (epoch 19/35), loss = 0.000276 (0.612 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:04:09.032208: step 62140/119245 (epoch 19/35), loss = 0.003192 (0.473 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:04:18.238665: step 62160/119245 (epoch 19/35), loss = 0.000246 (0.584 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:04:27.312536: step 62180/119245 (epoch 19/35), loss = 0.001432 (0.525 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:04:38.337949: step 62200/119245 (epoch 19/35), loss = 0.196159 (0.409 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:04:49.014028: step 62220/119245 (epoch 19/35), loss = 0.000153 (0.406 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:04:59.625361: step 62240/119245 (epoch 19/35), loss = 0.000294 (0.601 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:05:08.208570: step 62260/119245 (epoch 19/35), loss = 0.046579 (0.410 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:05:17.224840: step 62280/119245 (epoch 19/35), loss = 0.000074 (0.434 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:05:25.714704: step 62300/119245 (epoch 19/35), loss = 0.172615 (0.375 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:05:34.100723: step 62320/119245 (epoch 19/35), loss = 0.002842 (0.385 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:05:42.973828: step 62340/119245 (epoch 19/35), loss = 0.000017 (0.727 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:05:52.078264: step 62360/119245 (epoch 19/35), loss = 0.077037 (0.637 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:06:02.410787: step 62380/119245 (epoch 19/35), loss = 0.000045 (0.538 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:06:12.360513: step 62400/119245 (epoch 19/35), loss = 0.000408 (0.419 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:06:24.585283: step 62420/119245 (epoch 19/35), loss = 0.032316 (0.422 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:06:33.617584: step 62440/119245 (epoch 19/35), loss = 0.000738 (0.473 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:06:42.828069: step 62460/119245 (epoch 19/35), loss = 0.000048 (0.571 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:06:53.459891: step 62480/119245 (epoch 19/35), loss = 0.001602 (0.375 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:07:03.365497: step 62500/119245 (epoch 19/35), loss = 0.000352 (0.383 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:07:12.536189: step 62520/119245 (epoch 19/35), loss = 0.000192 (0.416 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:07:22.228033: step 62540/119245 (epoch 19/35), loss = 0.000401 (0.339 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:07:31.342795: step 62560/119245 (epoch 19/35), loss = 0.000209 (0.444 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:07:40.358191: step 62580/119245 (epoch 19/35), loss = 0.000055 (0.540 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:07:50.773542: step 62600/119245 (epoch 19/35), loss = 0.007397 (0.708 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:08:03.981848: step 62620/119245 (epoch 19/35), loss = 0.001097 (0.421 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:08:14.179439: step 62640/119245 (epoch 19/35), loss = 0.000100 (0.618 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:08:24.430099: step 62660/119245 (epoch 19/35), loss = 0.000013 (0.441 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:08:33.921007: step 62680/119245 (epoch 19/35), loss = 0.001330 (0.413 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:08:43.134830: step 62700/119245 (epoch 19/35), loss = 0.000387 (0.457 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:08:52.094154: step 62720/119245 (epoch 19/35), loss = 0.000068 (0.389 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:09:01.891145: step 62740/119245 (epoch 19/35), loss = 0.000140 (0.434 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:09:11.582232: step 62760/119245 (epoch 19/35), loss = 0.000461 (0.404 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:09:20.908402: step 62780/119245 (epoch 19/35), loss = 0.000406 (0.448 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:09:30.765833: step 62800/119245 (epoch 19/35), loss = 0.001212 (0.376 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:09:39.894146: step 62820/119245 (epoch 19/35), loss = 0.127201 (0.531 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:09:49.135544: step 62840/119245 (epoch 19/35), loss = 0.000144 (0.309 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:09:59.349000: step 62860/119245 (epoch 19/35), loss = 0.000709 (0.379 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:10:08.819641: step 62880/119245 (epoch 19/35), loss = 0.000137 (0.388 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:10:18.858807: step 62900/119245 (epoch 19/35), loss = 0.007203 (0.571 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:10:28.361655: step 62920/119245 (epoch 19/35), loss = 0.005193 (0.508 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:10:38.252233: step 62940/119245 (epoch 19/35), loss = 0.000075 (0.646 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:10:48.071927: step 62960/119245 (epoch 19/35), loss = 0.000011 (0.368 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:10:57.942277: step 62980/119245 (epoch 19/35), loss = 0.000184 (0.671 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:11:07.516781: step 63000/119245 (epoch 19/35), loss = 0.000918 (0.436 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:11:16.247827: step 63020/119245 (epoch 19/35), loss = 0.000638 (0.339 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:11:26.043162: step 63040/119245 (epoch 19/35), loss = 0.000089 (0.360 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:11:37.307352: step 63060/119245 (epoch 19/35), loss = 0.002942 (0.665 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:11:46.960479: step 63080/119245 (epoch 19/35), loss = 0.009103 (0.432 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:11:56.319621: step 63100/119245 (epoch 19/35), loss = 0.000455 (0.389 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:12:06.447255: step 63120/119245 (epoch 19/35), loss = 0.000005 (0.391 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:12:15.831523: step 63140/119245 (epoch 19/35), loss = 0.000339 (0.957 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:12:25.051029: step 63160/119245 (epoch 19/35), loss = 0.003281 (0.456 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:12:36.642847: step 63180/119245 (epoch 19/35), loss = 0.000356 (1.527 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:12:46.117391: step 63200/119245 (epoch 19/35), loss = 0.008166 (0.346 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:12:57.148676: step 63220/119245 (epoch 19/35), loss = 0.006326 (0.391 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:13:06.490079: step 63240/119245 (epoch 19/35), loss = 0.003537 (0.610 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:13:16.066576: step 63260/119245 (epoch 19/35), loss = 0.013461 (0.342 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:13:27.053437: step 63280/119245 (epoch 19/35), loss = 0.000057 (0.606 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:13:37.851323: step 63300/119245 (epoch 19/35), loss = 0.000058 (0.374 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:13:47.750970: step 63320/119245 (epoch 19/35), loss = 0.003037 (0.649 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:13:57.758445: step 63340/119245 (epoch 19/35), loss = 0.007539 (0.438 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:14:06.884514: step 63360/119245 (epoch 19/35), loss = 0.135317 (0.314 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:14:17.209064: step 63380/119245 (epoch 19/35), loss = 0.031960 (0.386 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:14:27.328255: step 63400/119245 (epoch 19/35), loss = 0.000005 (0.510 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:14:36.746491: step 63420/119245 (epoch 19/35), loss = 0.000062 (0.450 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:14:46.715158: step 63440/119245 (epoch 19/35), loss = 0.016606 (0.527 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:14:56.165855: step 63460/119245 (epoch 19/35), loss = 0.000235 (0.999 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:15:06.073101: step 63480/119245 (epoch 19/35), loss = 0.067809 (0.397 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:15:15.976603: step 63500/119245 (epoch 19/35), loss = 0.001485 (0.365 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:15:25.384648: step 63520/119245 (epoch 19/35), loss = 0.000157 (0.412 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:15:35.917785: step 63540/119245 (epoch 19/35), loss = 0.000055 (0.571 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:15:45.976078: step 63560/119245 (epoch 19/35), loss = 0.005855 (0.340 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:15:54.832196: step 63580/119245 (epoch 19/35), loss = 0.328571 (0.444 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:16:04.533555: step 63600/119245 (epoch 19/35), loss = 0.000939 (0.311 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:16:14.235386: step 63620/119245 (epoch 19/35), loss = 0.000526 (0.866 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:16:23.564346: step 63640/119245 (epoch 19/35), loss = 0.028992 (0.447 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:16:34.155736: step 63660/119245 (epoch 19/35), loss = 0.000098 (0.428 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:16:44.919803: step 63680/119245 (epoch 19/35), loss = 0.000429 (0.410 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:16:55.371312: step 63700/119245 (epoch 19/35), loss = 0.000467 (0.497 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:17:05.059341: step 63720/119245 (epoch 19/35), loss = 0.000045 (0.457 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:17:14.340068: step 63740/119245 (epoch 19/35), loss = 0.000357 (0.608 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:17:23.830252: step 63760/119245 (epoch 19/35), loss = 0.184007 (0.347 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:17:34.691481: step 63780/119245 (epoch 19/35), loss = 0.000171 (0.385 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:17:44.240150: step 63800/119245 (epoch 19/35), loss = 0.000597 (0.382 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:17:53.905999: step 63820/119245 (epoch 19/35), loss = 0.000029 (0.675 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:18:03.863220: step 63840/119245 (epoch 19/35), loss = 0.002852 (0.480 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:18:12.767701: step 63860/119245 (epoch 19/35), loss = 0.000075 (0.325 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:18:21.843558: step 63880/119245 (epoch 19/35), loss = 0.000004 (0.309 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:18:32.246966: step 63900/119245 (epoch 19/35), loss = 0.000041 (0.282 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:18:40.755148: step 63920/119245 (epoch 19/35), loss = 0.114082 (0.422 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:18:50.186521: step 63940/119245 (epoch 19/35), loss = 0.003985 (0.351 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:19:00.478946: step 63960/119245 (epoch 19/35), loss = 0.000004 (0.376 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:19:09.108784: step 63980/119245 (epoch 19/35), loss = 0.000638 (0.617 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:19:17.954128: step 64000/119245 (epoch 19/35), loss = 0.000083 (0.317 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:19:27.433391: step 64020/119245 (epoch 19/35), loss = 0.029296 (0.399 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:19:37.644368: step 64040/119245 (epoch 19/35), loss = 0.002574 (0.388 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:19:47.721813: step 64060/119245 (epoch 19/35), loss = 0.164777 (0.391 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:19:58.258567: step 64080/119245 (epoch 19/35), loss = 0.005688 (0.525 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:20:06.982025: step 64100/119245 (epoch 19/35), loss = 0.005719 (0.544 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:20:16.494935: step 64120/119245 (epoch 19/35), loss = 0.000657 (0.512 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:20:26.057077: step 64140/119245 (epoch 19/35), loss = 0.000998 (0.507 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:20:35.423125: step 64160/119245 (epoch 19/35), loss = 0.002828 (0.394 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:20:45.802015: step 64180/119245 (epoch 19/35), loss = 0.000077 (0.442 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:20:55.241968: step 64200/119245 (epoch 19/35), loss = 0.000205 (0.410 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:21:04.224015: step 64220/119245 (epoch 19/35), loss = 0.002556 (0.500 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:21:13.096467: step 64240/119245 (epoch 19/35), loss = 0.000049 (0.668 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:21:22.038989: step 64260/119245 (epoch 19/35), loss = 0.000028 (0.367 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:21:32.830791: step 64280/119245 (epoch 19/35), loss = 0.001006 (0.599 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:21:42.496007: step 64300/119245 (epoch 19/35), loss = 0.000062 (0.376 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:21:51.805051: step 64320/119245 (epoch 19/35), loss = 0.000110 (0.350 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:22:02.629041: step 64340/119245 (epoch 19/35), loss = 0.000062 (0.510 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:22:12.020055: step 64360/119245 (epoch 19/35), loss = 0.023361 (0.414 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:22:21.274236: step 64380/119245 (epoch 19/35), loss = 0.000393 (0.331 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:22:32.073574: step 64400/119245 (epoch 19/35), loss = 0.000308 (0.408 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:22:41.162635: step 64420/119245 (epoch 19/35), loss = 0.028678 (0.438 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:22:51.631473: step 64440/119245 (epoch 19/35), loss = 0.003274 (0.347 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:23:01.329946: step 64460/119245 (epoch 19/35), loss = 0.002390 (0.473 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:23:10.829049: step 64480/119245 (epoch 19/35), loss = 0.004897 (0.472 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:23:19.092737: step 64500/119245 (epoch 19/35), loss = 0.001840 (0.352 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:23:28.184481: step 64520/119245 (epoch 19/35), loss = 0.238038 (0.416 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:23:38.839106: step 64540/119245 (epoch 19/35), loss = 0.000087 (0.469 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:23:47.541073: step 64560/119245 (epoch 19/35), loss = 0.000027 (0.478 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:23:56.142164: step 64580/119245 (epoch 19/35), loss = 0.001873 (0.410 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:24:05.125694: step 64600/119245 (epoch 19/35), loss = 0.000243 (0.449 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:24:15.397242: step 64620/119245 (epoch 19/35), loss = 0.000092 (0.411 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:24:25.576966: step 64640/119245 (epoch 19/35), loss = 0.000471 (0.520 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:24:35.206633: step 64660/119245 (epoch 19/35), loss = 0.000052 (0.477 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:24:45.038765: step 64680/119245 (epoch 19/35), loss = 0.000053 (0.955 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:24:54.134281: step 64700/119245 (epoch 19/35), loss = 0.023661 (0.371 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:25:05.936480: step 64720/119245 (epoch 19/35), loss = 0.000296 (0.442 sec/batch), lr: 0.021870\n",
            "Evaluating on dev set...\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  79.51%  R:  67.75%  F1:  73.16%  #: 338\n",
            "org:city_of_headquarters             P:  67.90%  R:  50.46%  F1:  57.89%  #: 109\n",
            "org:country_of_headquarters          P:  58.93%  R:  55.93%  F1:  57.39%  #: 177\n",
            "org:dissolved                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 8\n",
            "org:founded                          P:  81.82%  R:  71.05%  F1:  76.06%  #: 38\n",
            "org:founded_by                       P:  66.18%  R:  59.21%  F1:  62.50%  #: 76\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 31\n",
            "org:members                          P:  75.00%  R:  52.94%  F1:  62.07%  #: 85\n",
            "org:number_of_employees/members      P:  73.68%  R:  51.85%  F1:  60.87%  #: 27\n",
            "org:parents                          P:  35.19%  R:  39.58%  F1:  37.25%  #: 96\n",
            "org:political/religious_affiliation  P:   0.00%  R:   0.00%  F1:   0.00%  #: 10\n",
            "org:shareholders                     P:  27.45%  R:  25.45%  F1:  26.42%  #: 55\n",
            "org:stateorprovince_of_headquarters  P:  66.67%  R:  68.57%  F1:  67.61%  #: 70\n",
            "org:subsidiaries                     P:  45.76%  R:  23.89%  F1:  31.40%  #: 113\n",
            "org:top_members/employees            P:  76.23%  R:  72.66%  F1:  74.40%  #: 534\n",
            "org:website                          P:  92.22%  R:  96.51%  F1:  94.32%  #: 86\n",
            "per:age                              P:  85.44%  R:  91.77%  F1:  88.49%  #: 243\n",
            "per:alternate_names                  P:  55.00%  R:  28.95%  F1:  37.93%  #: 38\n",
            "per:cause_of_death                   P:  81.29%  R:  82.74%  F1:  82.01%  #: 168\n",
            "per:charges                          P:  71.93%  R:  78.10%  F1:  74.89%  #: 105\n",
            "per:children                         P:  69.00%  R:  69.70%  F1:  69.35%  #: 99\n",
            "per:cities_of_residence              P:  52.17%  R:  40.22%  F1:  45.43%  #: 179\n",
            "per:city_of_birth                    P:  73.68%  R:  84.85%  F1:  78.87%  #: 33\n",
            "per:city_of_death                    P:  66.15%  R:  72.88%  F1:  69.35%  #: 118\n",
            "per:countries_of_residence           P:  37.58%  R:  49.56%  F1:  42.75%  #: 226\n",
            "per:country_of_birth                 P:  37.50%  R:  30.00%  F1:  33.33%  #: 20\n",
            "per:country_of_death                 P:  66.67%  R:   4.35%  F1:   8.16%  #: 46\n",
            "per:date_of_birth                    P:  96.55%  R:  90.32%  F1:  93.33%  #: 31\n",
            "per:date_of_death                    P:  77.60%  R:  72.33%  F1:  74.87%  #: 206\n",
            "per:employee_of                      P:  62.15%  R:  60.00%  F1:  61.06%  #: 375\n",
            "per:origin                           P:  60.87%  R:  53.33%  F1:  56.85%  #: 210\n",
            "per:other_family                     P:  42.11%  R:  10.00%  F1:  16.16%  #: 80\n",
            "per:parents                          P:  74.29%  R:  46.43%  F1:  57.14%  #: 56\n",
            "per:religion                         P:  66.67%  R:  56.60%  F1:  61.22%  #: 53\n",
            "per:schools_attended                 P:  86.84%  R:  66.00%  F1:  75.00%  #: 50\n",
            "per:siblings                         P:  62.07%  R:  60.00%  F1:  61.02%  #: 30\n",
            "per:spouse                           P:  71.74%  R:  62.26%  F1:  66.67%  #: 159\n",
            "per:stateorprovince_of_birth         P:  72.73%  R:  61.54%  F1:  66.67%  #: 26\n",
            "per:stateorprovince_of_death         P:  68.89%  R:  75.61%  F1:  72.09%  #: 41\n",
            "per:stateorprovinces_of_residence    P:  48.24%  R:  56.94%  F1:  52.23%  #: 72\n",
            "per:title                            P:  79.47%  R:  82.15%  F1:  80.79%  #: 919\n",
            "\n",
            "Final Score:\n",
            "17547 Guess as no relation\n",
            "5084 guess\n",
            "3513 correct\n",
            "5436 gold\n",
            "Precision (micro): 69.099%\n",
            "   Recall (micro): 64.625%\n",
            "       F1 (micro): 66.787%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "epoch 19: train_loss = 0.018623, dev_loss = 1.169725, dev_f1 = 0.6679\n",
            "model saved to ./saved_models/00/checkpoint_epoch_19.pt\n",
            "\n",
            "2020-12-02 09:28:03.113024: step 64740/119245 (epoch 20/35), loss = 0.000023 (0.446 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:28:13.133769: step 64760/119245 (epoch 20/35), loss = 0.000145 (0.424 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:28:23.691151: step 64780/119245 (epoch 20/35), loss = 0.000258 (0.473 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:28:32.587675: step 64800/119245 (epoch 20/35), loss = 0.011345 (0.451 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:28:42.583980: step 64820/119245 (epoch 20/35), loss = 0.000036 (0.352 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:28:51.820654: step 64840/119245 (epoch 20/35), loss = 0.036602 (0.419 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:29:02.136349: step 64860/119245 (epoch 20/35), loss = 0.000176 (1.016 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:29:10.724527: step 64880/119245 (epoch 20/35), loss = 0.001048 (0.329 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:29:19.807329: step 64900/119245 (epoch 20/35), loss = 0.227555 (0.846 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:29:29.639028: step 64920/119245 (epoch 20/35), loss = 0.000649 (0.531 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:29:39.471183: step 64940/119245 (epoch 20/35), loss = 0.000834 (0.521 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:29:48.430603: step 64960/119245 (epoch 20/35), loss = 0.002364 (0.398 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:29:57.883099: step 64980/119245 (epoch 20/35), loss = 0.021777 (0.517 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:30:08.504576: step 65000/119245 (epoch 20/35), loss = 0.001718 (0.391 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:30:16.586845: step 65020/119245 (epoch 20/35), loss = 0.000028 (0.362 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:30:25.717894: step 65040/119245 (epoch 20/35), loss = 0.000056 (0.592 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:30:36.097907: step 65060/119245 (epoch 20/35), loss = 0.002050 (0.439 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:30:45.749477: step 65080/119245 (epoch 20/35), loss = 0.195151 (0.504 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:30:56.778582: step 65100/119245 (epoch 20/35), loss = 0.000111 (0.437 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:31:06.171616: step 65120/119245 (epoch 20/35), loss = 0.000007 (0.403 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:31:16.657071: step 65140/119245 (epoch 20/35), loss = 0.000141 (0.481 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:31:25.456483: step 65160/119245 (epoch 20/35), loss = 0.002401 (0.364 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:31:37.301602: step 65180/119245 (epoch 20/35), loss = 0.092067 (0.367 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:31:47.211416: step 65200/119245 (epoch 20/35), loss = 0.000068 (0.671 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:31:56.986194: step 65220/119245 (epoch 20/35), loss = 0.000203 (0.453 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:32:07.729108: step 65240/119245 (epoch 20/35), loss = 0.000068 (0.532 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:32:17.051100: step 65260/119245 (epoch 20/35), loss = 0.000978 (0.716 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:32:27.850392: step 65280/119245 (epoch 20/35), loss = 0.000935 (0.501 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:32:36.758134: step 65300/119245 (epoch 20/35), loss = 0.001488 (0.709 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:32:45.775125: step 65320/119245 (epoch 20/35), loss = 0.000069 (0.576 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:32:54.505508: step 65340/119245 (epoch 20/35), loss = 0.000161 (0.505 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:33:03.794224: step 65360/119245 (epoch 20/35), loss = 0.000538 (0.743 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:33:12.634064: step 65380/119245 (epoch 20/35), loss = 0.000295 (0.329 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:33:23.111206: step 65400/119245 (epoch 20/35), loss = 0.000036 (0.611 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:33:34.238808: step 65420/119245 (epoch 20/35), loss = 0.000233 (0.600 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:33:42.877753: step 65440/119245 (epoch 20/35), loss = 0.211966 (0.550 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:33:53.146869: step 65460/119245 (epoch 20/35), loss = 0.000368 (0.547 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:34:02.755852: step 65480/119245 (epoch 20/35), loss = 0.000839 (0.481 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:34:12.217695: step 65500/119245 (epoch 20/35), loss = 0.000051 (0.357 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:34:21.686977: step 65520/119245 (epoch 20/35), loss = 0.000034 (0.354 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:34:31.520580: step 65540/119245 (epoch 20/35), loss = 0.010020 (0.413 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:34:40.542870: step 65560/119245 (epoch 20/35), loss = 0.000440 (0.383 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:34:49.872369: step 65580/119245 (epoch 20/35), loss = 0.000086 (0.432 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:35:00.110455: step 65600/119245 (epoch 20/35), loss = 0.189663 (0.412 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:35:10.284676: step 65620/119245 (epoch 20/35), loss = 0.000526 (0.620 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:35:21.572221: step 65640/119245 (epoch 20/35), loss = 0.000254 (0.408 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:35:30.999321: step 65660/119245 (epoch 20/35), loss = 0.000119 (0.471 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:35:39.689514: step 65680/119245 (epoch 20/35), loss = 0.000426 (0.310 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:35:48.373971: step 65700/119245 (epoch 20/35), loss = 0.000485 (0.450 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:35:56.857396: step 65720/119245 (epoch 20/35), loss = 0.004913 (0.343 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:36:05.517222: step 65740/119245 (epoch 20/35), loss = 0.000092 (0.359 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:36:14.259213: step 65760/119245 (epoch 20/35), loss = 0.002205 (0.382 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:36:24.655688: step 65780/119245 (epoch 20/35), loss = 0.000011 (0.364 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:36:33.737647: step 65800/119245 (epoch 20/35), loss = 0.000539 (0.478 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:36:44.787650: step 65820/119245 (epoch 20/35), loss = 0.000029 (0.417 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:36:56.211018: step 65840/119245 (epoch 20/35), loss = 0.000040 (0.522 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:37:04.872847: step 65860/119245 (epoch 20/35), loss = 0.000151 (0.354 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:37:15.235855: step 65880/119245 (epoch 20/35), loss = 0.000023 (0.470 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:37:25.554450: step 65900/119245 (epoch 20/35), loss = 0.000008 (0.442 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:37:34.535175: step 65920/119245 (epoch 20/35), loss = 0.000684 (0.483 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:37:44.692673: step 65940/119245 (epoch 20/35), loss = 0.000026 (0.555 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:37:53.904378: step 65960/119245 (epoch 20/35), loss = 0.000015 (0.533 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:38:02.928935: step 65980/119245 (epoch 20/35), loss = 0.000091 (0.412 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:38:12.959560: step 66000/119245 (epoch 20/35), loss = 0.000569 (0.992 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:38:24.464441: step 66020/119245 (epoch 20/35), loss = 0.000229 (0.337 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:38:36.339548: step 66040/119245 (epoch 20/35), loss = 0.010817 (0.605 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:38:46.534953: step 66060/119245 (epoch 20/35), loss = 0.057226 (0.643 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:38:56.052038: step 66080/119245 (epoch 20/35), loss = 0.000459 (0.412 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:39:05.818417: step 66100/119245 (epoch 20/35), loss = 0.000968 (0.649 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:39:14.692658: step 66120/119245 (epoch 20/35), loss = 0.005424 (0.469 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:39:24.061378: step 66140/119245 (epoch 20/35), loss = 0.000253 (0.337 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:39:33.682618: step 66160/119245 (epoch 20/35), loss = 0.000103 (0.501 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:39:43.067943: step 66180/119245 (epoch 20/35), loss = 0.000195 (0.447 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:39:53.345470: step 66200/119245 (epoch 20/35), loss = 0.000338 (0.538 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:40:02.406411: step 66220/119245 (epoch 20/35), loss = 0.000788 (0.337 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:40:11.539870: step 66240/119245 (epoch 20/35), loss = 0.000057 (0.481 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:40:21.242903: step 66260/119245 (epoch 20/35), loss = 0.006908 (0.519 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:40:31.257230: step 66280/119245 (epoch 20/35), loss = 0.000770 (0.526 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:40:41.076876: step 66300/119245 (epoch 20/35), loss = 0.002611 (0.736 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:40:50.757221: step 66320/119245 (epoch 20/35), loss = 0.000308 (0.495 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:41:00.704831: step 66340/119245 (epoch 20/35), loss = 0.006036 (0.495 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:41:09.730720: step 66360/119245 (epoch 20/35), loss = 0.002223 (0.479 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:41:20.358841: step 66380/119245 (epoch 20/35), loss = 0.000730 (0.520 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:41:29.383931: step 66400/119245 (epoch 20/35), loss = 0.000077 (0.380 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:41:39.072803: step 66420/119245 (epoch 20/35), loss = 0.000113 (0.498 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:41:48.229839: step 66440/119245 (epoch 20/35), loss = 0.000017 (0.535 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:41:59.198132: step 66460/119245 (epoch 20/35), loss = 0.000190 (0.422 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:42:09.342755: step 66480/119245 (epoch 20/35), loss = 0.000305 (0.766 sec/batch), lr: 0.021870\n",
            "2020-12-02 09:42:18.746124: step 66500/119245 (epoch 20/35), loss = 0.000483 (0.374 sec/batch), lr: 0.021870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCLywasTnGN-"
      },
      "source": [
        "## Evaluation on the model with the best dev score\n",
        "Note that after the training, several trained models should have been saved. Make sure to download the ones you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_CHJof5fAzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08158b4c-06e3-4b03-9b1f-b4f6d44e75b1"
      },
      "source": [
        "!python eval.py saved_models/00 --dataset test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 09:47:41.886381: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Loading model from saved_models/00/best_model.pt\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 300kB/s]\n",
            "Downloading: 100% 433/433 [00:00<00:00, 370kB/s]\n",
            "Downloading: 100% 436M/436M [00:07<00:00, 57.3MB/s]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Finetune all embeddings.\n",
            "Loading data from dataset/tacred/test.json with batch size 20...\n",
            "100% 15509/15509 [00:00<00:00, 64081.83it/s]\n",
            "776 batches created for dataset/tacred/test.json\n",
            "batch count 776\n",
            "\n",
            "Running with the following configs:\n",
            "\tdata_dir : dataset/tacred\n",
            "\tvocab_dir : dataset/vocab\n",
            "\ttrain_file : train.json\n",
            "\tdev_file : dev.json\n",
            "\temb_dim : 768\n",
            "\tner_dim : 0\n",
            "\tpos_dim : 0\n",
            "\thidden_dim : 768\n",
            "\tmlp_dim : 300\n",
            "\tnum_layers : 1\n",
            "\tdropout : 0.5\n",
            "\tword_dropout : 0.04\n",
            "\ttopn : 10000000000.0\n",
            "\tlower : False\n",
            "\tattn : False\n",
            "\tattn_dim : 200\n",
            "\tpe_dim : 20\n",
            "\tlr : 0.03\n",
            "\tlr_decay : 0.9\n",
            "\toptim : sgd\n",
            "\tnum_epoch : 35\n",
            "\tbatch_size : 20\n",
            "\tmax_grad_norm : 5.0\n",
            "\tlog_step : 20\n",
            "\tlog : logs.txt\n",
            "\tsave_epoch : 10\n",
            "\tsave_dir : ./saved_models\n",
            "\tid : 00\n",
            "\tinfo : \n",
            "\tseed : 1234\n",
            "\tcuda : True\n",
            "\tcpu : False\n",
            "\tbert : True\n",
            "\tlife : False\n",
            "\tspecial_token : True\n",
            "\tnum_class : 42\n",
            "\tvocab_size : 28996\n",
            "\tmodel_save_dir : ./saved_models/00\n",
            "\n",
            "\n",
            "!!! (tensor([[  101,  1124,  1144,  ...,     0,     0,     0],\n",
            "        [  101,  2508, 21543,  ...,     0,     0,     0],\n",
            "        [  101, 29018,   117,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  3823,   118,  ...,     0,     0,     0],\n",
            "        [  101,   118,   118,  ...,  7234,  1116,   102],\n",
            "        [  101, 15463, 20080,  ...,     0,     0,     0]]), tensor([[False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        ...,\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ...,  True,  True,  True]]), None, None, None, tensor([[ -43,  -42,  -41,  ...,    0,    0,    0],\n",
            "        [ -11,  -10,   -9,  ...,    0,    0,    0],\n",
            "        [  -1,    0,    1,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ -31,  -30,  -29,  ...,    0,    0,    0],\n",
            "        [-101, -100,  -99,  ...,   48,   49,   50],\n",
            "        [ -64,  -63,  -62,  ...,    0,    0,    0]]), tensor([[-50, -49, -48,  ...,   0,   0,   0],\n",
            "        [-14, -13, -12,  ...,   0,   0,   0],\n",
            "        [-18, -17, -16,  ...,   0,   0,   0],\n",
            "        ...,\n",
            "        [-19, -18, -17,  ...,   0,   0,   0],\n",
            "        [-53, -52, -51,  ...,  96,  97,  98],\n",
            "        [-57, -56, -55,  ...,   0,   0,   0]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0]), None)\n",
            "Per-relation statistics:\n",
            "org:alternate_names                  P:  72.32%  R:  76.06%  F1:  74.14%  #: 213\n",
            "org:city_of_headquarters             P:  75.00%  R:  62.20%  F1:  68.00%  #: 82\n",
            "org:country_of_headquarters          P:  63.54%  R:  56.48%  F1:  59.80%  #: 108\n",
            "org:dissolved                        P:  50.00%  R:  50.00%  F1:  50.00%  #: 2\n",
            "org:founded                          P:  91.18%  R:  83.78%  F1:  87.32%  #: 37\n",
            "org:founded_by                       P:  75.00%  R:  48.53%  F1:  58.93%  #: 68\n",
            "org:member_of                        P:   0.00%  R:   0.00%  F1:   0.00%  #: 18\n",
            "org:members                          P:  55.56%  R:  16.13%  F1:  25.00%  #: 31\n",
            "org:number_of_employees/members      P:  81.82%  R:  47.37%  F1:  60.00%  #: 19\n",
            "org:parents                          P:  46.88%  R:  24.19%  F1:  31.91%  #: 62\n",
            "org:political/religious_affiliation  P:  37.50%  R:  60.00%  F1:  46.15%  #: 10\n",
            "org:shareholders                     P:   0.00%  R:   0.00%  F1:   0.00%  #: 13\n",
            "org:stateorprovince_of_headquarters  P:  67.92%  R:  70.59%  F1:  69.23%  #: 51\n",
            "org:subsidiaries                     P:  41.18%  R:  47.73%  F1:  44.21%  #: 44\n",
            "org:top_members/employees            P:  70.32%  R:  81.50%  F1:  75.50%  #: 346\n",
            "org:website                          P:  70.00%  R:  80.77%  F1:  75.00%  #: 26\n",
            "per:age                              P:  85.85%  R:  91.00%  F1:  88.35%  #: 200\n",
            "per:alternate_names                  P:   0.00%  R:   0.00%  F1:   0.00%  #: 11\n",
            "per:cause_of_death                   P:  78.57%  R:  21.15%  F1:  33.33%  #: 52\n",
            "per:charges                          P:  67.97%  R:  84.47%  F1:  75.32%  #: 103\n",
            "per:children                         P:  64.10%  R:  67.57%  F1:  65.79%  #: 37\n",
            "per:cities_of_residence              P:  62.67%  R:  49.74%  F1:  55.46%  #: 189\n",
            "per:city_of_birth                    P:  60.00%  R:  60.00%  F1:  60.00%  #: 5\n",
            "per:city_of_death                    P:  80.00%  R:  28.57%  F1:  42.11%  #: 28\n",
            "per:countries_of_residence           P:  43.90%  R:  48.65%  F1:  46.15%  #: 148\n",
            "per:country_of_birth                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 5\n",
            "per:country_of_death                 P: 100.00%  R:  11.11%  F1:  20.00%  #: 9\n",
            "per:date_of_birth                    P:  80.00%  R:  88.89%  F1:  84.21%  #: 9\n",
            "per:date_of_death                    P:  73.08%  R:  35.19%  F1:  47.50%  #: 54\n",
            "per:employee_of                      P:  66.18%  R:  68.18%  F1:  67.16%  #: 264\n",
            "per:origin                           P:  50.96%  R:  60.61%  F1:  55.36%  #: 132\n",
            "per:other_family                     P:  56.25%  R:  45.00%  F1:  50.00%  #: 60\n",
            "per:parents                          P:  70.00%  R:  71.59%  F1:  70.79%  #: 88\n",
            "per:religion                         P:  60.98%  R:  53.19%  F1:  56.82%  #: 47\n",
            "per:schools_attended                 P:  73.91%  R:  56.67%  F1:  64.15%  #: 30\n",
            "per:siblings                         P:  66.67%  R:  72.73%  F1:  69.57%  #: 55\n",
            "per:spouse                           P:  82.61%  R:  57.58%  F1:  67.86%  #: 66\n",
            "per:stateorprovince_of_birth         P:  50.00%  R:  50.00%  F1:  50.00%  #: 8\n",
            "per:stateorprovince_of_death         P:  26.67%  R:  28.57%  F1:  27.59%  #: 14\n",
            "per:stateorprovinces_of_residence    P:  56.38%  R:  65.43%  F1:  60.57%  #: 81\n",
            "per:title                            P:  78.98%  R:  89.40%  F1:  83.86%  #: 500\n",
            "\n",
            "Final Score:\n",
            "12254 Guess as no relation\n",
            "3255 guess\n",
            "2222 correct\n",
            "3325 gold\n",
            "Precision (micro): 68.264%\n",
            "   Recall (micro): 66.827%\n",
            "       F1 (micro): 67.538%\n",
            "If they are all the same, meaning\n",
            "0 for Gold = No, Guess = Re\n",
            "0 for Gold = Re, Guess = No\n",
            "or the two has same number\n",
            "precision = correct number/guessed number\n",
            "recall = correct number/gold number\n",
            "Evaluation ended.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}